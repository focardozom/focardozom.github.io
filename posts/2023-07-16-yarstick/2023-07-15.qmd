---
title: "Computing Sensitivity and Specificity using tidymodels"
author: 
  - "Francisco Cardozo"
  - "Kyle Grealis"
date: 2023-07-16
categories: ["Sensitivity", "Tidymodels", "Specificity"]
tags: ["Sensitivity", "Tidymodels", "Specificity"]
bibliography: ref.bib
---

Sensitivity and specificity are two crucial metrics used to evaluate the performance of binary classification models. In this article, we delve into the process of computing these metrics using the 'tidymodels' package in R.

We begin by simulating a dataset. Assuming we have access to the true value of the phenomenon under consideration, as well as our model's predictions, we can proceed to compute the sensitivity and specificity of our model.

```{r simulate-data, warning=FALSE, message=FALSE}
library(tidyverse)  # Load tidyverse package
library(gt)         # Load gt package
library(gtsummary)  # Load gtsummary package


# Create a tibble with the confusion matrix data
confusion_matrix_long <- tibble(
  'True Class' = c('True 1', 'True 0', 'True 1', 'True 0'),
  'Predicted Class' = c('1', '1', '0', '0'),
  'Count' = c(40, 15, 10, 35)
)

confusion_matrix_long  |> gt()

```

Function <a href='https://www.danieldsjoberg.com/gtsummary/reference/tbl_cross.html' target='_blank'>`tbl_cross()`</a> offers a handy feature for obtaining percentages, which is useful when computing sensitivity and specificity.

```{r tbl_cross, warning=FALSE, message=FALSE}

# Duplicate each row of the tibble based on the value in the Count column
confusion_matrix_long |>
  uncount(Count) |>
  tbl_cross(percent = "col")  

```


So, we know the values of specificity and sensitivity, which are essentially the results of these two equations:

* Sensitivity = TP / (TP + FN) = 40 / (40 + 10) = 0.80 (80%)

* Specificity = TN / (TN + FP) = 35 / (35 + 15) = 0.70 (70%)


::: {.callout-tip collapse="true"}

> Refresh your memory on the meaning of these terms by looking at the following table:

|        | True 1 (Positive) | True 0 (Negative) |
|--------|:-----------------:|:-----------------:|
| Predict 1 (Positive) |       TP = 40     |       FP = 15     |
| Predict 0 (Negative) |       FN = 10     |       TN = 35     |

This table above is the confusion matrix, featuring the following terms:

- TP (True Positive): Instances where the model correctly predicted the positive class.
- TN (True Negative): Instances where the model correctly predicted the negative class.
- FP (False Positive): Instances where the model incorrectly predicted the positive class.
- FN (False Negative): Instances where the model incorrectly predicted the negative class.

:::

However, the burning question remains: how do we calculate these in R? Let's explore this while evaluating the performance of our models using the 'tidymodels' package.

### Modeling with tidymodels

Let's now assume that we have a dataset comprising Y and X values, and we aim to create a model that predicts Y based on X. We'll use the same dataset we created earlier.

```{r simulate-data-2, warning=FALSE, message=FALSE} 
df <- confusion_matrix_long |> 
  uncount(Count)  |> 
  rename(y = `True Class`, x = `Predicted Class`) # Rename the columns to shorter names
```

First, we'll load the tidymodels library and specify our model.


```{r tidymodels, message=FALSE}

# Load the tidymodels package
library(tidymodels)

# Define a logistic regression model specification
model_spec <- logistic_reg() |>
  set_engine("glm") |> 
  set_mode("classification")

# Define a recipe for preprocessing the data
recipe_glm <- 
  recipe(y ~ x, data = df) |> 
  # Convert all nominal variables to dummy variables
  step_dummy(all_nominal(), -all_outcomes()) 

```

Let's examine the dataset we've created.

```{r examine-data, message=FALSE}
# Preprocess the data using the recipe
# This includes converting nominal variables to dummy variables
recipe_glm |>
  prep() |>
  # Apply the recipe to new data
  bake(new_data = df) |>
  # View the first few rows of the preprocessed data
  skimr::skim()

```

Remeber that our outcome is a binary variable with values `True 0` and `True 1`.

Next, we can create a workflow that combines our recipe and the model specification, and fit the model.

```{r fit-model, message=FALSE}

# Define a workflow for fitting the logistic regression model
wr_glm <- workflow() |> 
  add_recipe(recipe_glm) |> 
  add_model(model_spec)  

# Fit the logistic regression model to the preprocessed data
model <- wr_glm |> 
  fit(data = df) 

# Extract the model coefficients and create a summary table
model |> 
  extract_fit_parsnip() |> 
  tbl_regression()

```

So, we've established that the model predicts the Y values based on the X values. *But how do we calculate the sensitivity and specificity using the model results?*

First, we need to generate predictions. The <a href='https://broom.tidymodels.org/reference/augment.lm.html' target='_blank'>`augment()`</a> function from the <a href='https://dplyr.tidyverse.org/' target='_blank'>`dplyr`</a> package is an excellent choice for this task.

```{r}
# Load the reactable package
library(reactable)

# Use the augment() function to add predicted values to the data frame
the_predictions <- model |>
  augment(df)  |> 
  # Rename the "y" column to "Observed"
  rename("Observed" = "y")


# Create an interactive table using the reactable::reactable() function
the_predictions |>
  reactable::reactable(
    groupBy = "Observed",
    columns = list(
      x = colDef(aggregate = "unique"),
      .pred_class = colDef(aggregate = "unique"),
      `.pred_True 0` = colDef(aggregate = "mean", format = colFormat(digits = 2)),
      `.pred_True 1` = colDef(aggregate = "mean", format = colFormat(digits = 2))
    )
  )
```


At this point, we have both the predicted class and the observed class. We could use the same method we employed in the earlier demonstration (<a href='https://www.danieldsjoberg.com/gtsummary/reference/tbl_cross.html' target='_blank'>`tbl_cross()`</a>), or we can make use of the <a href='https://yardstick.tidymodels.org/reference/sens.html' target='_blank'>`sensitivity()`</a> and <a href='https://yardstick.tidymodels.org/reference/spec.html' target='_blank'>`specificity()`</a> functions from the <a href='https://yardstick.tidymodels.org/' target='_blank'>`yardstick`</a> package. Let's look at how this works.

### Computing sensitivity and specificity using a cross table
```{r}  
the_predictions  |> 
    tbl_cross(.pred_class, Observed, percent = "col")
```

This yields the same results as before.

### Computing sensitivity and specificity using the yardstick package

Finally! we can use the <a href='https://yardstick.tidymodels.org/reference/sens.html' target='_blank'>`sensitivity()`</a> and <a href='https://yardstick.tidymodels.org/reference/spec.html' target='_blank'>`specificity()`</a> functions from the <a href='https://yardstick.tidymodels.org/' target='_blank'>`yardstick`</a> package to calculate the sensitivity and specificity of our model.

```{r}
sens <- the_predictions  |> 
  mutate(Observed = as.factor(Observed))  |>
  sensitivity(Observed, .pred_class)

spec <- the_predictions  |> 
  mutate(Observed = as.factor(Observed))  |>
  specificity(Observed, .pred_class)

bind_rows(sens, spec)  |> 
  gt()
```

Oops! That doesn't look right. Why is that?

In the context of logistic regression, R's default behavior is to take the first level of a factor as the reference category when using the <a href='https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm' target='_blank'>`glm`</a> function. This behavior becomes particularly important when we're dealing with binary outcomes, often coded as 0 (absence of the event) and 1 (presence of the event). By default, R will take 0 as the reference level and compare it against 1, due to 0 coming before 1 in numerical order.

However, when we're working with factors, the order in which they're arranged is somewhat arbitrary and it doesn't necessarily make sense to always treat 0 as the absence of an event and 1 as the presence of an event [@KuhnS2022]. 

::: {.callout-note appearance="simple"}
This is from [here](https://www.tmwr.org/performance.html) or page 116 in the tidymodels book.

![tidymodels. pag 116](tidymodels.png)
:::


**The `yardstick` package interprets this setup a bit differently**. It views the first factor as the most important, leading it to switch the factor levels in the process. This change in order may affect our sensitivity and specificity measures.

To maintain consistency with our earlier computations, it's necessary to explicitly set the reference level in the <a href='https://yardstick.tidymodels.org/reference/sens.html' target='_blank'>`sensitivity()`</a> and <a href='https://yardstick.tidymodels.org/reference/spec.html' target='_blank'>`specificity()`</a> functions from the <a href='https://yardstick.tidymodels.org/' target='_blank'>`yardstick`</a> functions, using the `event_level` = "second" argument. This ensures that the factor levels are interpreted in a way that aligns with our initial demonstration.

```{r}
sens_y  <- the_predictions  |>
  mutate(Observed = as.factor(Observed))  |>
  sensitivity(Observed, .pred_class, event_level = "second")

spec_y  <- the_predictions  |>
  mutate(Observed = as.factor(Observed))  |>
  specificity(Observed, .pred_class, event_level = "second")

bind_rows(sens_y, spec_y)  |> 
  gt()
```

Now we've restored our original results. ðŸ˜Ž

::: {.callout-caution}

## Caution
This 'switching' behavior in yardstick is also apparent in the <a href='https://tune.tidymodels.org/reference/collect_predictions.html' target='_blank'>`collect_metrics()`</a> function, making it essential to check the event level. Failure to do so may result in inadvertently switching the event level.
:::

# Summary 

In this blog post, we've walked through the process of computing sensitivity and specificity using the tidymodels package in R, demonstrating it with a simulated dataset. These metrics are indispensable for evaluating the performance of binary classification models. Don't forget: for the most accurate and realistic results, always evaluate your models using separate test data.
