[
  {
    "objectID": "posts/2023-12-03-most/most.html",
    "href": "posts/2023-12-03-most/most.html",
    "title": "Balancing Efficiency and Flexibility: The Challenges of Over-Optimization in the Multiphase Optimization Strategy (MOST)",
    "section": "",
    "text": "The Multiphase Optimization Strategy (MOST) is an approach to intervention development in prevention science. MOST is structured around three phases: the Screening Phase, where intervention components are initially selected or eliminated based on efficacy; the Refining Phase, which is dedicated to ‘calibrating’ these elements to determine their optimal levels and combinations; and the Confirming Phase, during which the refined intervention is subjected to rigorous evaluation via a conventional randomized controlled trial. Although this strategy is designed to optimize intervention development, it faces a significant challenge: the possibility that excessive optimization could be counterproductive.\nAlthough it may seem counterintuitive, pursuing efficiency can sometimes lead to inferior outcomes. This paradox is named Goodhart’s Law, which suggests that once a measure becomes a target, it can end up distorting the very outcome it was intended to assess. This happens when individuals or groups start to ‘game the system’ to meet these targets, thereby neglecting the real improvements these measures are designed to track.\nRegarding MOST, the issue does not always stem from intentional misbehavior or manipulation; rather, it arises from an overemphasis on efficiency that can result in an excessive focus on optimization, which may neglect components that benefit achieving the outcomes of the intervention. To illustrate how an excessive focus on optimization can lead to worse outcomes, consider students who are overly prepared to excel at standardized tests, which may cause them to neglect to develop a wider range of skills that are crucial for overall life success. Similarly, overly incentivizing researchers with bonuses can encourage fraudulent activities and undermine the integrity of scientific research.\nTherefore, within MOST, over-selecting components may lead to overfitting in interventions, which is counterproductive given the need for interventions suitable for diverse populations. Moreover, there is a risk that prioritizing intervention components based solely on their individual ‘effectiveness’ can overlook the principle that the total impact may not always equal the sum of its parts. These two aspects represent fundamental challenges inherent in the MOST methodology.\nTo address this challenge, intervention developers must take into consideration an appropriate balance between the efficiency of components and the variability of their effects under diverse conditions. In addressing this, it is crucial to acknowledge that samples are less diverse than populations, that other factors such as implementation components also contribute to program effectiveness, and, overall, that interventions may not be universally beneficial across all population groups.\nIn conclusion, while MOST is a promising framework for intervention development, it is vital to recognize that it is not perfect. The applicability of the MOST methodology is not universal across all interventions, and the advantages of efficiency must be carefully weighed against the risks of overfitting.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html",
    "href": "posts/2024-04-06-getref/getref.html",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "",
    "text": "Dealing with references can feel like a real headache. However, recently I came across a fantastic solution by Richard Mclearn. This blog post is all about making your life easier with a simple tutorial for Mac users. I’ll show you how to set up a getref function that quickly grabs bibliographic information in BibTeX format from a DOI or a DOI URL."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-1-prepare-your-workspace",
    "href": "posts/2024-04-06-getref/getref.html#step-1-prepare-your-workspace",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 1: Prepare Your Workspace",
    "text": "Step 1: Prepare Your Workspace\nFirst, we need to create a dedicated folder to store your getref function. This not only helps in organizing your scripts but also in managing your PATH environment efficiently.\nYou can create the folder following this steps:\n1.1. Open the terminal (cmd+space then type terminal).\n1.2. Create a new folder named bin in your home directory:\ntype\n\nmkdir ~/bin\n\nand press return in your keyboard.\n\n\n\n\n\n\nNote\n\n\n\nI created the bin folder in my home directory. It will make it easier to access the script later on."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-2-move-to-the-bin-folder",
    "href": "posts/2024-04-06-getref/getref.html#step-2-move-to-the-bin-folder",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 2: Move to the bin Folder",
    "text": "Step 2: Move to the bin Folder\nNavigate to the newly created folder by typing\n\ncd ~/bin."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-3-create-the-script-file",
    "href": "posts/2024-04-06-getref/getref.html#step-3-create-the-script-file",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 3: Create the Script File",
    "text": "Step 3: Create the Script File\nCreate a new file named getref by typing\n\ntouch getref\n\nThis file will be our script."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-4-add-the-script-code",
    "href": "posts/2024-04-06-getref/getref.html#step-4-add-the-script-code",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 4: Add the Script Code",
    "text": "Step 4: Add the Script Code\nOpen the getref file in your favorite text editor. For simplicity, you can use Nano.\n\nnano getref\n\nIn the inside of the file, copy and paste the following code snippet. This script checks if a DOI or DOI URL is provided as an argument and then fetches the bibliographic information in BibTeX format.\n\n#!/bin/bash\n# Check if an argument is provided\nif [ \"$#\" -ne 1 ]; then\n    echo \"Usage: $0 &lt;DOI or DOI URL&gt;\"\n    exit 1\nfi\n\n# Determine if the input is a DOI URL or just a DOI number\nif [[ \"$1\" =~ ^https:// ]]; then\n    # Extract the DOI number from the URL\n    DOI=$(echo \"$1\" | sed 's|https://doi.org/||')\nelse\n    # Assume the input is just a DOI number\n    DOI=\"$1\"\nfi\n\n# Extract and print the reference\ncurl -LH \"Accept: application/x-bibtex\" \"https://doi.org/$DOI\""
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-5-make-the-script-executable",
    "href": "posts/2024-04-06-getref/getref.html#step-5-make-the-script-executable",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 5: Make the Script Executable",
    "text": "Step 5: Make the Script Executable\nEnsure the script can be executed by running in the terminal\n\nchmod +x ~/bin/getref\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that I used the path to my file, which is located in the bin folder in my home directory."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-6-update-your-shell-configuration",
    "href": "posts/2024-04-06-getref/getref.html#step-6-update-your-shell-configuration",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 6: Update Your Shell Configuration",
    "text": "Step 6: Update Your Shell Configuration\nNow, Open your shell configuration file (.zshrc for Zsh or .bashrc for Bash) using a text editor, e.g.,\n\nnano ~/.zshrc.\n\nYou will need to add ~/bin to your PATH. To do this, add the following line at the end of the file:\n\nexport PATH=\"$HOME/bin:$PATH\".\n\nSave and close the file."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-7-apply-the-changes",
    "href": "posts/2024-04-06-getref/getref.html#step-7-apply-the-changes",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 7: Apply the Changes",
    "text": "Step 7: Apply the Changes\nApply the changes to your current session by sourcing your configuration file:\n\nsource ~/.zshrc\n\nfor Zsh\nor\n\nsource ~/.bashrc\n\nfor Bash."
  },
  {
    "objectID": "posts/2024-01-27-explain/explain.html",
    "href": "posts/2024-01-27-explain/explain.html",
    "title": "To Explain or to Predict",
    "section": "",
    "text": "There is a current debate regarding the appropriate circumstances to employ machine learning (ML) over traditional statistics for statistical modeling. Some traditionalists assert that ML often represents nothing more than a rebrand version of existing methods, occasionally rendering it unnecessary. Their primary criticism is that while ML can be advantageous in some circumstances, it frequently operates as a “black box”, offering limited transparency and scant insights into the underlying phenomena. On the other hand, ML advocates argue that traditional statistics often fail to grasp the intricate complexities of real-world data, positioning ML as the superior option. Both arguments, however, might be missing the essence of the issue. This debate is not merely about the characteristics of the tools used but more about the foundational objectives of scientific research.\nScientific research predominantly deals with questions to respond ‘why’ and ‘when’ something occurs. When scientists seek explanations, they are interested in the primary causes or influencers of observed events. Predictive endeavors, conversely, are inherently forward-looking, valuing future forecasts over causal understanding. While no single statistical method can directly answer causal questions, prediction can be approached using both traditional regression techniques and ML models. Thus, the real dilemma emerges in choosing methods for predictive situations, as tools for causal explanations are beyond mere statistical techniques.\nWhen predictive modeling is chosen, machine learning often proves more advantageous than traditional statistical models. These advantages stem from distinct characteristics inherent in the construction of ML models. Primarily, this is because the construction of machine learning models is guided by out-of-sample metrics, tailored to maximize the model’s ability to generalize to unseen data. In essence, machine learning models seek a balance between variance and bias. Conversely, traditional statistical methods focus on hypothesis testing to draw conclusions from models, often overlooking concerns such as overfitting. This oversight includes practices like using the same dataset for both training and testing, and a lack of emphasis on metrics such as the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), or adjusted R², which are crucial for evaluating model complexity. It should be mentioned that this traditional approach is not inherently wrong, but it is not the most appropriate for predictive modeling. Instead, these methods are excellent tools for descriptive analysis and most research in public health is descriptive (unfortunately?).\nWhile machine learning models offer advantages, they also present several challenges. They frequently require extensive computational resources and, in numerous cases, cannot offer insights about underlying phenomena. Such shortcomings can be significant limitations in various scientific contexts. In these scenarios, traditional statistics often emerge as the preferred approach, due to their ability to summarize data relationships more straightforwardly.\nIn prevention science, differentiating between prediction and explanation is crucial. However, this distinction is often overlooked, resulting in innovative missteps. In the absence of clear guidelines, practitioners may opt for inappropriate tools, leading to confusion between predictive outcomes and causal relationships. For example, when assessing interventions, programs are typically evaluated with a focus on their theoretical foundations and the program’s underlying mechanisms, necessitating explanatory modeling. Yet, in practice, the models often tend to be predictive (or descriptive), thereby overlooking the methods essential for explanatory analysis.\nIn summary, the distinction between explanation and prediction is not merely about the methods or algorithms employed but centers on the overarching purpose of the research. While both are fundamental to scientific research, their practical applications and tools, especially in prevention science, can differ significantly.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-05-10-recipes/recipes.html",
    "href": "posts/2023-05-10-recipes/recipes.html",
    "title": "An Introduction to the Recipes Package for Data Preprocessing",
    "section": "",
    "text": "I liked this presentation done by Max Kuhn.\n\n\nHere are some notes:\nThe recipes package provides a framework for preprocessing data prior to modeling or visualization. With its pipeable sequence of steps and syntax similar to dplyr, the package simplifies a wide range of preprocessing tasks, from data normalization and missing data imputation, to categorical variable encoding and data transformation.\nIt’s important to remember when using the recipes package, the type of model you’re fitting can determine the necessary preprocessing steps for your data.\nIn addition to model-driven preprocessing steps, the recipes package also provides functions for feature engineering. This involves representing your data in ways most effective for your particular problem. For instance, you might create interaction terms, polynomial terms, or spline terms to capture non-linear relationships between predictors and the outcome.\nHere are some useful preprocessing steps:\n\nData normalization: The step_normalize() function normalizes your data by centering and scaling the variables. This is useful when working with models that require predictors to be on the same scale, such as k-nearest neighbors or neural networks.\nMissing data imputation: The step_impute_*() functions impute missing data using various methods, like mean imputation, median imputation, or k-nearest neighbors imputation.\nCategorical variable encoding: The step_dummy() function creates dummy variables for categorical predictors. This is handy when working with models that can’t handle categorical predictors directly, like linear regression or logistic regression.\nData transformation: The step_*() functions transform your data in various ways, such as applying the logarithm, square root, or Box-Cox transformation to a variable. This is useful when working with data that isn’t normally distributed or when trying to improve the linearity of the relationship between predictors and the outcome.\nFeature engineering: The step_*() functions are also used for feature engineering, such as creating interaction terms, polynomial terms, or spline terms. This is beneficial when trying to capture non-linear relationships between predictors and the outcome.\n\nLink to the package documentation\nThings to think:\nA point of confusion might be whether preprocessing is considered part of data cleaning or data transformation for modeling. It appears that there’s an overlap between data cleaning and data transformation, and it can sometimes be difficult to distinguish between these stages. It would be helpful to clarify the difference between these concepts and data preprocessing.\nWhen I try to imagine where recipes fits into these models, it’s not completely clear to me.\n\n\n\nThe data science process. From R for Data Science\n\n\n\n\n\nModeling Process. From Tidymodels book\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html",
    "href": "posts/2023-01-10-posit-education/Posit.html",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "",
    "text": "I recently watched a presentation by Garrett Grolemund, the Director of Learning at Rstudio, on the POSIT Enterprise Community Meetup on YouTube. I wanted to share with you some of the interesting thoughts about teaching R that he mentioned."
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html#context",
    "href": "posts/2023-01-10-posit-education/Posit.html#context",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "Context",
    "text": "Context\n\nGarret believes that the success of the training ultimately depends on how it’s delivered.\nTeachers out there really aren’t that good and online courses are not so great.\nPeople who become experts in R by years of practice. However, they may not have had the opportunity to devote those years to being a good teacher.\nThere is theories in psychology(such as cognitive load theory and multimedia learning theory) that can be applied to make training more successful.\nResearch shows that students only retain about 56% of the content from a lecture\nThe retention rate decreases over time.\nStudies on procedural instruction show similar results, with only 60% of students able to reproduce the procedure immediately after training.\nSix months later, the number drops to 40%, and a year later it drops to 30%\nTraining outcomes are not always great and this is a “dirty secret” of the training industry.\nTrainers are not always trained to be trainers, which is a challenge in the field.\nEducation is not simply about transferring information from an educator to a student, and the student then becoming an expert."
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html#insights",
    "href": "posts/2023-01-10-posit-education/Posit.html#insights",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "Insights",
    "text": "Insights\n\nPractice\n\nThe more the task is practiced, the more the brain will conserve the neural networks and retain the ability to perform the task. This is not likely to occur in a workshop that lasts only half a day or two days.\nTo build a robust neural network, it is important to sleep.\nThe ability to perform a new skill in six months from now dependent on the amount of practice they receive after initial instruction, rather than the instruction itself.\n\n\n“Well, the revelation we had at our studio is that data science is a skill, and if you want to learn to do good data science with code or otherwise, you have to practice it and learn it as a skill.”\n\n\n\nMentors, Mates and Accountability\n\nAs a student, it is possible to practice a skill incorrectly without even being aware of it. To practice effectively and efficiently, it is crucial to receive feedback and guidance to make sure the proper techniques are being utilized.\nIndividuals do not acquire knowledge in a vacuum, they are driven by social influences and the identity they construct by participating in a community that is studying data science.\nThe lacking factor in online courses is motivation, which can be obtained through interaction with a mentor or peers. Talking to them can provide the necessary inspiration.\nHaving a mentor or being part of a group provides accountability, as one is expected to show up with something to demonstrate to them.\n\n\n“So, as you go through the course, not only do you have an expert who has your back who’s coaching you, you also have fellow travelers who you could discuss things with. You could work through problems together and, you might not even realize it, but you can hold each other accountable and motivated as you go through the process.”\n\n\n\nRecomendations\n\nHow Learning Happens by Paul Kirschner and Carl. link to amazon. But also consult with your librarian friend\nVisit Posit Academy. You can learn in detail about the POSIT Academy model in minute 24."
  },
  {
    "objectID": "posts/2023-08-01-while/while.html",
    "href": "posts/2023-08-01-while/while.html",
    "title": "The Chessboard and the Wise Courtier",
    "section": "",
    "text": "The Story\nOnce upon a time in ancient India, a wise courtier presented a challenge to his king using a chessboard and some grains of rice. He placed a single grain of rice on the first square of the chessboard, then asked the king to double the number of grains on each subsequent square until all 64 squares were filled. Initially, the king thought that the result would be a meaningless amount of rice, but he soon realized the astronomical sum this would amount to.\n\n\n\n\n\n“Generated with Midjourney”\n\n\n\n\n\nThis story illustrates the power of exponential growth, a mathematical concept that can describe certain patterns in nature and society. Exponential growth occurs when the rate of growth is proportional to the current value, leading to increasingly rapid growth over time. This type of growth can be found in many real-world phenomena, including the spread of disease, and the adoption of new technologies.\n\n\n\n\n\n\n\n\nNote\n\n\n\nPandemics: In the early stages of a pandemic, such as COVID-19, the number of infections may double within a fixed time period, leading to an exponential increase.\nSocial Media and Viral Content: Posts or videos that “go viral” on social media platforms can exhibit exponential growth. A piece of content might be shared by a few people initially, but if each person who sees it shares it with others, the total number of views can grow exponentially.\nEnvironmental Concerns: Exponential growth can also have negative consequences, such as in the spread of invasive species or the consumption of non-renewable resources, leading to potential environmental damage.\n\n\nI will use the story of chess to demonstrate how quickly the number of grains grows when it doubles over the course of only 64 steps. I will also provide Python code to calculate the total number of grains on the chessboard.\n\n\nPython Code to Compute the Total Number of Grains\nImagine you have a bag of grains, and you want to fill a chessboard with them. You start with one grain on the first square, then double the number of grains on each subsequent square until you reach the 64th square. You can think of this process like a snowball rolling down a hill, getting bigger and bigger as it goes.\nIn Python, we can use something called a while loop to replicate this process. It’s like having a robot that puts the grains on the squares for you, following your exact instructions: “Start with one grain, then double the number, and keep going until you fill all 64 squares.”\n\n\n\n\n\n\nWhile Loop\n\n\n\nA while loop continues to execute the block of code as long as a specified condition is true. It checks the condition before each iteration, and if the condition is false, it exits the loop.\n\n\nNow, let’s think about a different way to approach the task using what’s called a for loop. Imagine instead that you have 64 small containers, each representing a square on the chessboard, and you know exactly how many grains should go in each one. Starting with the first container, you fill it with one grain, then move to the next one and fill it with double the grains of the previous container, and so on, until you reach the 64th container.\n\n\n\n\n\n\nFor Loop\n\n\n\nA for loop works like this staircase-building process. You tell the computer exactly how many times to repeat something, and it follows your instructions step by step. Unlike the snowball rolling down the hill, which might take an unpredictable path, building the staircase (or using a “for loop”) is more controlled and precise.\n\n\n\n\nWhile Loop in python\nLets to compute that number of grains using the while loop.\n\n# Initialize variables\nsquare_number = 1  # The number of the current square\ngrains_on_square = 1  # The number of grains on the current square\ntotal_grains = 0  # The total number of grains so far\n\n# Loop through each square on the chessboard\nwhile square_number &lt;= 64:\n    # Add the number of grains on the current square to the total\n    total_grains += grains_on_square\n    # Double the number of grains on the current square for the next square\n    grains_on_square *= 2\n    # Move to the next square\n    square_number += 1\n\nprint(f\"The total number of grains on the chessboard is: \\n {total_grains:,}\")\n\nThe total number of grains on the chessboard is: \n 18,446,744,073,709,551,615\n\n\nSo, it’s a big number. Here are some interesting facts about this number:\n\n\n\n\n\n\nNote\n\n\n\n\nThis number in words is eighteen quintillion, four hundred forty-six quadrillion, seven hundred forty-four trillion, seventy-three billion, seven hundred nine million, five hundred fifty-one thousand, six hundred fifteen, or approximately 18 quintillion, or 18 billion billion.\nIn seconds, it is equal to 293,274,701,009 years, 3 weeks, 3 days, 15 hours, 30 minutes, 7 seconds.\nIf each grain of rice were 0.05 grams in weight, then the total weight of rice would be 922337203685.48 tons.\n\n\n\n\n# Total number of grains as computed earlier\ntotal_grains = 18446744073709551615\n\n# Weight of a single grain in grams\nweight_per_grain = 0.05\n\n# Compute the total weight in grams\ntotal_weight_grams = total_grains * weight_per_grain\n\n# Convert to kilograms\ntotal_weight_kilograms = total_weight_grams / 1000\n\n# Convert to metric tons\ntotal_weight_metric_tons = total_weight_kilograms / 1000\n\nprint(f\"The total weight of the rice in grams is {total_weight_grams:.2f} grams\")\nprint(f\"The total weight of the rice in kilograms is {total_weight_kilograms:.2f} kilograms\")\nprint(f\"The total weight of the rice in metric tons is {total_weight_metric_tons:.2f} metric tons\")\n\nThe total weight of the rice in grams is 922337203685477632.00 grams\nThe total weight of the rice in kilograms is 922337203685477.62 kilograms\nThe total weight of the rice in metric tons is 922337203685.48 metric tons\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nFind more facts here:\n“18446744073709551615 - Facts”\n\n\nLet’s finish this post by comparing the while loop with the for loop.\n\n\n\n\n\n\n\n\nAspect\nWhile Loop\nFor Loop\n\n\n\n\nCondition vs Sequence\nUses a condition that can be any logical expression, continues iterating as long as the condition is true.\nIterates over a sequence of values, executing the code block once for each value.\n\n\nControl Over Iteration\nYou have full control over how many times the loop iterates by manipulating the condition.\nThe number of iterations is defined by the length of the sequence.\n\n\nPotential for Infinite Loop\nThere’s a risk of creating an infinite loop if the condition never becomes false.\nGenerally not at risk for infinite loops, as it iterates over a finite sequence.\n\n\nUse Cases\nOften used when you don’t know how many times you’ll need to iterate.\nUsed when you want to iterate a known number of times or over a specific sequence.\n\n\n\n\nConclusion\nThe chessboard problem illustrates how quickly numbers can grow when they double with each step. By using a while loop in Python, we’ve computed this enormous figure, providing insight into the power of exponential growth.\nThis example shows how Python can be used to solve problems that would be otherwise challenging to compute manually. Feel free to explore the code further, perhaps by changing the number of squares or the initial number of grains. What other intriguing patterns might you discover?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2022-11-27-trustworthy/trustworthy.html",
    "href": "posts/2022-11-27-trustworthy/trustworthy.html",
    "title": "Trustworthy",
    "section": "",
    "text": "Francisco Cardozo, Pablo Montero-Zamora\nConfidence in students’ survey responses is commonly questioned when performing drug use research. For example, one’s can wonder how do you know students are telling the truth about their alcohol consumption? We recognize this as a legitimate question that can be even more complex when considering two types of students: 1) those who say they have not used alcohol but used it (i.e., deniers), and 2) those who say they have used it but never use it (i.e., braggers). It is crucial to understand how this reporting bias affects the validity of our measurements and research findings. Therefore, we propose the following app to know how the proportions of deniers, braggers, and drug use prevalence can influence confidence levels in self-reported measures collected in adolescents.\n\nTrue prevalence: number of students using a drug divided by the population.\n\nDeniers: students who say they have not used a drug but used it.\nBraggers: students who say they have used a drug but never used it.\n\nGiven the information above, we can estimate the probability of a student’s drug use behavior if they respond Yes or No to a drug use question.\nTo model this probability, we can use the Bayes theorem:\n\nP(A|B) = Drug use given they say yes in the questionnaire.\nP(B|A) = Say yes given that they have used drugs.\nP(A) = Drug use prevalence.\nP(B) = Say yes in the questionnaire."
  },
  {
    "objectID": "posts/2022-12-01-ChessOlympiad22/index.html",
    "href": "posts/2022-12-01-ChessOlympiad22/index.html",
    "title": "Can ELO Predict the Outcome of Chess Games?",
    "section": "",
    "text": "Code\nlibrary(ChessOlympiad22)\nlibrary(rpart.plot)\nlibrary(tidymodels)\n\n\nWarning: package 'scales' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'recipes' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'yardstick' was built under R version 4.2.3\n\n\nThe ELO rating system is a method used to calculate the relative skill levels of chess players, based on their game results. A higher ELO rating indicates a higher perceived skill level.\nIn this document, the ELO difference between two players is used to evaluate the probability of one player winning the match.\nI will use the The ChessOlympiad package. It contains two datasets: players and results. The players dataset contains the ELO ratings of all players participating in the 44th Chess Olympiad, which took place in Chennai, India, in 2022. The results dataset contains the results of all matches played in the tournament.\n\nDistribution of ELO\n\n\nCode\ndata(\"players\")\n\n\nFirst, let’s visualize the ELO distribution of the players.\nIt is important to filter out players with an ELO rating of zero from the analysis, as these players have not yet been rated and do not have a known skill level.\n\n\nCode\nplayers |&gt; \n  filter(rtg!=0) |&gt; \n  ggplot(aes(rtg)) + \n  geom_histogram(bins=90,fill=\"gray90\", alpha=0.85, color=\"gray95\") +\n  scale_x_continuous(breaks = seq(1000,2900,150))+\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        panel.grid = element_blank()) +\n  labs(x=\"Elo Rating System (Higher values indicate greater player strength)\",\n       y=\"\",\n       title = \"Elo Rating System of players in Chess Olympiad, 2022. Chennai, India \",\n       caption = \"Magnus Carlsen has the highest Elo rating in the tournament: 2864\") +\n  geom_text(data = players %&gt;% \n    filter(rtg &gt; 2800),\n            aes(x = 2550, \n            y = 4.5, \n            label = \"Magnus Carlsen\"),\n            size = 3,\n            color = \"black\") +\n  geom_curve(data = players %&gt;% filter(rtg &gt; 2800),\n               aes(x = 2700, \n               y = 4.4, xend = 2864.8, \n               yend = 1),\n               arrow = arrow(length = unit(0.3, \"cm\")),\n               color = \"red\",\n               linewidth = 0.5,\n               curvature = -0.5)\n\n\n\n\n\nIt may be interesting to plot the ELO ratings of players by federation, as the Chess Olympiad is played by national teams. By examining the ELO ratings of players within each federation, we can get a sense of the overall strength of the teams participating in the event. This analysis could potentially provide insight into the results of the Chess Olympiad and help predict the outcomes of matches.\n\n\nCode\nplayers |&gt; \n  filter(rtg&gt;2600) |&gt; \n  ggplot(aes(reorder(fed, rtg),rtg)) + \n  theme_minimal() +\n  geom_boxplot() +\n  coord_flip() +\n  theme(panel.grid = element_blank()) +\n  labs(x=\"\", y=\"ELO\")\n\n\n\n\n\nAccording to ELO ratings, the United States fielded the strongest team in the tournament.\n\n\nCode\ndata(\"results\")\n\n\n\n\nDifferences in ELO by round\nThe Chess Olympiad followed a Swiss-style tournament, meaning that players are paired with opponents with similar scores in each round. Specifically, in the first round, the highest-ranked player is matched against the median-ranked player, followed by the second-highest ranked player against the next below median, and so forth.\nA visual representation of the differences in ELO by round are presented in the following graph.\n\n\nCode\nresults |&gt; \n  filter(elo_difference&gt;=-1000, \n         elo_difference&lt;=1000, \n         !is.na(elo_white), !is.na(elo_black), elo_white!=0,elo_black!=0) |&gt; \n  ggplot(aes(as.numeric(elo_white),elo_difference, \n             fill=factor(result_white), \n             color=factor(result_white))) + \n  geom_point(shape = 21, alpha=0.85,\n             size = 3, stroke = 0.5) +\n  theme_minimal() +\n  scale_fill_manual(values=c(\"Lost\"=\"black\", \"Draw\"=\"gray50\",\"Won\"=\"white\")) +\n  scale_color_manual(values=c(\"Lost\"=\"black\", \"Draw\"=\"black\",\"Won\"=\"black\")) +\n  labs(fill=\"Result\", color=\"Result\", \n       x=\"Player Elo\", \n       y=\"Elo difference\",\n       caption = \"Difference greater than zero indicates stronger player\n       44th Chess Olympiad. Chennai, 2022 Open\") +\n  facet_wrap(~ round)\n\n\n\n\n\n\n\nModel the winning chances for players with the white pieces based on ELO difference\nNow, let’s try to identify the optimal divisions in ELO rating differences that could potentially classify the outcomes of chess games. For this, I will be using the tidymodels package to estimate a Classification and Regression Trees (CART) model.\n\n\nCode\ncart_spec &lt;-\n   decision_tree() |&gt; \n   set_engine(\"rpart\") |&gt;\n   set_mode(\"classification\")\n\n\nI will add two steps in the recipe. One to filter the data set by round, and the other to convert results in a factor variable. I also will limit my analysis to players with more than 1600 in ELO.\n\n\nCode\nresults &lt;- results |&gt; \n  filter(as.numeric(elo_white)&gt;1600) |&gt; \n  filter(as.numeric(elo_black)&gt;1600) \n\nrecipe &lt;- recipe(\n  result_white ~ elo_difference + round_number, data = results) |&gt; \n    step_filter(round_number==round)\n\n\n\n\nCode\nwrkfl &lt;- workflow() |&gt; \n  add_model(cart_spec) |&gt; \n  add_recipe(recipe)\n\n\nLet’s estimate the model for the round 1.\n\n\nCode\nround &lt;- 1\n\ncart_fit &lt;- wrkfl |&gt; \n  fit(data=results) |&gt; \n  extract_fit_parsnip()\n\n\n\nDraw an tree to understand the results\nFinally, I will create a tree showing the splits\n\n\nCode\ncart_fit &lt;- repair_call(cart_fit, data = round)\n\ncart_tree_fit &lt;- cart_fit$fit\n\nrpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)\n\n\n\n\n\nAccording to the model, a difference of 12 in the ELO is sufficient to accurately predict the winner in 95% of cases and the loser in 91% of cases.\n\n\n\nModel the last round\nNow, let’s apply the model to the final round, which featured matches between the most formidable opponents.\nI will add a the tree_depth parameter to my model. The depth of the tree refers to the number of levels the tree has.\n\n\nCode\ncart_spec &lt;-\n   decision_tree(tree_depth = 4) |&gt; \n   set_engine(\"rpart\") |&gt;\n   set_mode(\"classification\")\n\nwrkfl &lt;- workflow() |&gt; \n  add_model(cart_spec) |&gt; \n  add_recipe(recipe)\n\n\n\n\nCode\nround &lt;- 11\n\ncart_fit &lt;- wrkfl |&gt; \n  fit(data=results)\n\ncart_fit &lt;- wrkfl |&gt; \n  fit(data=results) |&gt; \n  extract_fit_parsnip()\n\ncart_fit &lt;- repair_call(cart_fit, data = round)\n\ncart_tree_fit &lt;- cart_fit$fit\n\nrpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)\n\n\n\n\n\n\n\nCode\nrpart.rules(cart_tree_fit, cover = TRUE)\n\n\n  ..y  Dra Los Won                                       cover\n Draw [.45 .30 .25] when elo_difference is -176 to  85     51%\n Draw [.55 .09 .36] when elo_difference &gt;=         333      4%\n Draw [.69 .15 .15] when elo_difference is   95 to 115      4%\n Lost [.29 .61 .10] when elo_difference &lt;  -176            20%\n  Won [.10 .30 .60] when elo_difference is   85 to  95      3%\n  Won [.17 .20 .63] when elo_difference is  115 to 333     18%\n\n\nThe color-coding of the decision tree leaves suggests that differences in ELO ratings remain a critical factor, even in the final round of the chess tournament. It seems that the model is particularly adept at predicting outcomes when the ELO differences are substantial. For example, if you have more than 333 points in ELO, the model predicts 63% wining chances for you. However, if the ELO difference is less than 333 (but more than 115), the model predicts 55% of draw.\nWhen the ELO difference is less than 115, the model’s predictions become more interesting. If a player has 176 points less than their opponent, the model is more likely to classify them as a loser (61%). However, if the ELO difference is less than 85 points, the player still has a good chance of winning the game. This could be seen as an indicator of the performance of some players with lower ELO ratings who are having a strong tournament. On the other hand, if the ELO difference is greater than 85 points, most of the chances are for a draw.\nThis serves as a quick demonstration of how the ChessOlympiad package can be utilized in predictive modeling.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-03-12-imbalance/imbalance_ci.html",
    "href": "posts/2024-03-12-imbalance/imbalance_ci.html",
    "title": "Balancing Data in Machine Learning: When Good Intentions Meet Slippery Slopes",
    "section": "",
    "text": "Have you ever attended a party where the host tries so hard to make everyone happy that they end up achieving the exact opposite? That’s somewhat like what happens when we overenthusiastically balance data in machine learning. The intention is noble, but the outcome? Not always a hit.\nIn the debate over whether to balance data or not (Elor & Averbuch-Elor, 2022; Goorbergh et al., 2022), simulations often act as referees, showcasing results that typically advise against manipulating data to achieve balance. But let’s be honest, can we trust everything that comes out of a simulation? Shouldn’t we explore different angles to grasp why data balancing might not always be the wisest course of action?\n\nRubin to the Rescue… Or Not?\nLet’s turn the spotlight onto a different perspective. Picture yourself in an examination hall, faced with a challenging test. In a moment of desperation, you decide to copy answers from the student next to you. However, you have no evidence they’re the top student. So, you start sizing them up. Glasses? Check. Philosophical expressions? Check. Or maybe they’re a math wiz because, obviously, that’s the universal sign of genius, right?\nAs your anxiety mounts, something extraordinary happens: a fairy godmother appears, not with a magic wand, but with important message: she whispers your neighbor’s grade. Bingo! With newfound confidence, you embark on your note-transcribing quest.\nThis little adventure parallels adjusting your data based on observed outcomes. Before the fairy godmother’s revelation, you’re in the dark, much like when we lack insight into the observed outcomes. You might make assumptions (glasses mean brains, right?), but that’s shaky ground and we might just be copying the wrong answers, mistaking luck for wisdom.\nAnd that’s where Rubin’s thinking comes in. It’s not like the fairy godmother, it is much like venturing into an enchanted forest where paths diverge and outcomes remain uncertain.\nThe Rubin causal model, also known as the potential outcomes framework, is a way to think about cause and effect in a systematic manner. Imagine every time you make a decision, there are two parallel universes: one where you made one choice and another where you made a different one. The Rubin model compares what happens in these two universes to understand the true impact of that choice. It’s like having a twin who makes different decisions, and by looking at both your lives, you can see the effects of those decisions. In practice, since we can’t see parallel universes, this model uses statistics to estimate what would have happened in the alternate scenario, helping to isolate the cause of an outcome from all the noise. In this context, balancing data is contrary to observing divergence outcomes across parallel universes; it artificially aligns one universe to resemble the other, potentially obscuring causal relationships rather than revealing them.\nBut wait, as in fairy tales, there’s a twist! Machine learning loves predictions like cats love cardboard boxes. And in this arena, confounders could be seen as those annoying but sometimes helpful pieces of furniture. Sure, they make your model look cluttered, but they might just help it predict better, and we have the tools to deal with the ‘cluttering’: ’In the force of data, too much bias, the clarity it clouds; too much variance, the truth it obscures.\nIn essence, while machine learning and causal reasoning might seem like strange bedfellows, they both aim to make sense of our chaotic, data-driven world. Perhaps it’s time we invite them both to the party, have them shake hands, and work together. After all, the best parties are those where everyone gets along, right?\n\n\n\n\n\n Back to topReferences\n\nElor, Y., & Averbuch-Elor, H. (2022). To SMOTE, or not to SMOTE? CoRR, abs/2201.08528. https://arxiv.org/abs/2201.08528\n\n\nGoorbergh, R. van den, Smeden, M. van, Timmerman, D., & Van Calster, B. (2022). The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression. Journal of the American Medical Informatics Association, 29(9), 1525–1534. https://doi.org/10.1093/jamia/ocac093"
  },
  {
    "objectID": "posts/2023-07-22-collect_metrics/collect_metrics.html",
    "href": "posts/2023-07-22-collect_metrics/collect_metrics.html",
    "title": "Computing Sensitivity and Specificity Using collect_metrics",
    "section": "",
    "text": "(in progress)\nIn the last post, we discussed computing sensitivity and specificity using the sensitivity() and specificity() functions from the yardstick package. In this post, we’ll demonstrate how to compute these metrics using the collect_metrics function from the tidymodels package."
  },
  {
    "objectID": "posts/2023-07-22-collect_metrics/collect_metrics.html#summary",
    "href": "posts/2023-07-22-collect_metrics/collect_metrics.html#summary",
    "title": "Computing Sensitivity and Specificity Using collect_metrics",
    "section": "Summary",
    "text": "Summary\nIn this blog post, we discussed the use of the tidymodels package to compute the sensitivity and specificity of a model - two important measures for evaluating model performance. We started with the recreation of a confusion matrix, then used the tbl_cross() function to perform some data wrangling.\nNext, we leveraged fit_resamples() from the tidymodels package to estimate our model multiple times, using cross-validation with 10 repeated folds to create 100 datasets in total.\nWe set up a logistic regression model, fit it to our data, and estimated the model multiple times using the created folds. The saved predictions allowed us to compute sensitivity and specificity for each fold.\nWe defined a custom set of metrics - roc_auc, sensitivity, and specificity - using the metric_set() function from the yardstick package. Finally, we used collect_metrics() to compute the average of these metrics for all the folds, and visualized the distribution of the sensitivity and specificity values.\nFuture posts will delve into calculating confidence intervals and evaluating the variability of these values across resamples."
  },
  {
    "objectID": "posts/2023-07-15-yarstick/2023-07-15.html",
    "href": "posts/2023-07-15-yarstick/2023-07-15.html",
    "title": "Computing Sensitivity and Specificity using tidymodels",
    "section": "",
    "text": "Sensitivity and specificity are two crucial metrics used to evaluate the performance of binary classification models. In this article, we delve into the process of computing these metrics using the ‘tidymodels’ package in R.\nWe begin by simulating a dataset. Assuming we have access to the true value of the phenomenon under consideration, as well as our model’s predictions, we can proceed to compute the sensitivity and specificity of our model.\n\nlibrary(tidyverse)  # Load tidyverse package\nlibrary(gt)         # Load gt package\nlibrary(gtsummary)  # Load gtsummary package\n\n\n# Create a tibble with the confusion matrix data\nconfusion_matrix_long &lt;- tibble(\n  'Predicted Class' = c('1', '1', '0', '0'),\n  'True Class' = c('True 1', 'True 0', 'True 1', 'True 0'),\n  'Count' = c(40, 15, 10, 35)\n)\n\nconfusion_matrix_long  |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      Predicted Class\n      True Class\n      Count\n    \n  \n  \n    1\nTrue 1\n40\n    1\nTrue 0\n15\n    0\nTrue 1\n10\n    0\nTrue 0\n35\n  \n  \n  \n\n\n\n\nFunction tbl_cross() offers a handy feature for obtaining percentages, which is useful when computing sensitivity and specificity.\n\n# Duplicate each row of the tibble based on the value in the Count column\nconfusion_matrix_long |&gt;\n  uncount(Count) |&gt;\n  tbl_cross(percent = \"col\")  \n\n\n\n\n\n  \n    \n    \n      \n      \n        True Class\n      \n      Total\n    \n    \n      True 0\n      True 1\n    \n  \n  \n    Predicted Class\n\n\n\n        0\n35 (70%)\n10 (20%)\n45 (45%)\n        1\n15 (30%)\n40 (80%)\n55 (55%)\n    Total\n50 (100%)\n50 (100%)\n100 (100%)\n  \n  \n  \n\n\n\n\nSo, we know the values of specificity and sensitivity, which are essentially the results of these two equations:\n\nSensitivity = TP / (TP + FN) = 40 / (40 + 10) = 0.80 (80%)\nSpecificity = TN / (TN + FP) = 35 / (35 + 15) = 0.70 (70%)\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nRefresh your memory on the meaning of these terms by looking at the following table:\n\n\n\n\n\nTrue 1 (Positive)\nTrue 0 (Negative)\n\n\n\n\nPredict 1 (Positive)\nTP = 40\nFP = 15\n\n\nPredict 0 (Negative)\nFN = 10\nTN = 35\n\n\n\nThis table above is the confusion matrix, featuring the following terms:\n\nTP (True Positive): Instances where the model correctly predicted the positive class.\nTN (True Negative): Instances where the model correctly predicted the negative class.\nFP (False Positive): Instances where the model incorrectly predicted the positive class.\nFN (False Negative): Instances where the model incorrectly predicted the negative class.\n\n\n\n\nHowever, the burning question remains: how do we calculate these in R? Let’s explore this while evaluating the performance of our models using the ‘tidymodels’ package.\n\nModeling with tidymodels\nLet’s now assume that we have a dataset comprising Y and X values, and we aim to create a model that predicts Y based on X. We’ll use the same dataset we created earlier.\n\ndf &lt;- confusion_matrix_long |&gt; \n  uncount(Count)  |&gt; \n  rename(y = `True Class`, x = `Predicted Class`) # Rename the columns to shorter names\n\nFirst, we’ll load the tidymodels library and specify our model.\n\n# Load the tidymodels package\nlibrary(tidymodels)\n\nWarning: package 'scales' was built under R version 4.2.3\n\n\nWarning: package 'recipes' was built under R version 4.2.3\n\n\nWarning: package 'yardstick' was built under R version 4.2.3\n\n# Define a logistic regression model specification\nmodel_spec &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt; \n  set_mode(\"classification\")\n\n# Define a recipe for preprocessing the data\nrecipe_glm &lt;- \n  recipe(y ~ x, data = df) |&gt; \n  # Convert all nominal variables to dummy variables\n  step_dummy(all_nominal(), -all_outcomes()) \n\nLet’s examine the dataset we’ve created.\n\n# Preprocess the data using the recipe\n# This includes converting nominal variables to dummy variables\nrecipe_glm |&gt;\n  prep() |&gt;\n  # Apply the recipe to new data\n  bake(new_data = df) |&gt;\n  # View the first few rows of the preprocessed data\n  skimr::skim()\n\n\nData summary\n\n\nName\nbake(prep(recipe_glm), ne…\n\n\nNumber of rows\n100\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ny\n0\n1\nFALSE\n2\nTru: 50, Tru: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nx_X1\n0\n1\n0.55\n0.5\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\n\n\n\n\nRemeber that our outcome is a binary variable with values True 0 and True 1.\nNext, we can create a workflow that combines our recipe and the model specification, and fit the model.\n\n# Define a workflow for fitting the logistic regression model\nwr_glm &lt;- workflow() |&gt; \n  add_recipe(recipe_glm) |&gt; \n  add_model(model_spec)  \n\n# Fit the logistic regression model to the preprocessed data\nmodel &lt;- wr_glm |&gt; \n  fit(data = df) \n\n# Extract the model coefficients and create a summary table\nmodel |&gt; \n  extract_fit_parsnip() |&gt; \n  tbl_regression()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    x_X1\n2.2\n1.3, 3.2\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nSo, we’ve established that the model predicts the Y values based on the X values. But how do we calculate the sensitivity and specificity using the model results?\nFirst, we need to generate predictions. The augment() function from the dplyr package is an excellent choice for this task.\n\n# Load the reactable package\nlibrary(reactable)\nlibrary(knitr)\n\n# Use the augment() function to add predicted values to the data frame\nthe_predictions &lt;- model |&gt;\n  augment(df)  |&gt; \n  # Rename the \"y\" column to \"Observed\"\n  rename(\"Observed\" = \"y\")\n\n\n# Create an interactive table using the reactable::reactable() function\nthe_predictions |&gt;\n  reactable(\n    groupBy = \"Observed\",\n    columns = list(\n      x = colDef(aggregate = \"unique\"),\n      .pred_class = colDef(aggregate = \"unique\"),\n      `.pred_True 0` = colDef(aggregate = \"mean\", format = colFormat(digits = 2)),\n      `.pred_True 1` = colDef(aggregate = \"mean\", format = colFormat(digits = 2))\n    )\n  )\n\n\n\n\n\n\nAt this point, we have both the predicted class and the observed class. We could use the same method we employed in the earlier demonstration (tbl_cross()), or we can make use of the sensitivity() and specificity() functions from the yardstick package. Let’s look at how this works.\n\n\nComputing sensitivity and specificity using a cross table\n\nthe_predictions  |&gt; \n    tbl_cross(.pred_class, Observed, percent = \"col\")\n\n\n\n\n\n  \n    \n    \n      \n      \n        Observed\n      \n      Total\n    \n    \n      True 0\n      True 1\n    \n  \n  \n    .pred_class\n\n\n\n        True 0\n35 (70%)\n10 (20%)\n45 (45%)\n        True 1\n15 (30%)\n40 (80%)\n55 (55%)\n    Total\n50 (100%)\n50 (100%)\n100 (100%)\n  \n  \n  \n\n\n\n\nThis yields the same results as before.\n\n\nComputing sensitivity and specificity using the yardstick package\nFinally! we can use the sensitivity() and specificity() functions from the yardstick package to calculate the sensitivity and specificity of our model.\n\nsens &lt;- the_predictions  |&gt; \n  mutate(Observed = as.factor(Observed))  |&gt;\n  sensitivity(Observed, .pred_class)\n\nspec &lt;- the_predictions  |&gt; \n  mutate(Observed = as.factor(Observed))  |&gt;\n  specificity(Observed, .pred_class)\n\nbind_rows(sens, spec)  |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    sensitivity\nbinary\n0.7\n    specificity\nbinary\n0.8\n  \n  \n  \n\n\n\n\nOops! That doesn’t look right. Why is that?\nIn the context of logistic regression, R’s default behavior is to take the first level of a factor as the reference category when using the glm function. This behavior becomes particularly important when we’re dealing with binary outcomes, often coded as 0 (absence of the event) and 1 (presence of the event). By default, R will take 0 as the reference level and compare it against 1, due to 0 coming before 1 in numerical order.\nHowever, when we’re working with factors, the order in which they’re arranged is somewhat arbitrary and it doesn’t necessarily make sense to always treat 0 as the absence of an event and 1 as the presence of an event (Kuhn and Silge 2022).\n\n\n\n\n\n\nThis is from here or page 116 in the tidymodels book.\n\n\n\ntidymodels. pag 116\n\n\n\n\n\nThe yardstick package interprets this setup a bit differently. It views the first factor as the most important, leading it to switch the factor levels in the process. This change in order may affect our sensitivity and specificity measures.\nTo maintain consistency with our earlier computations, it’s necessary to explicitly set the reference level in the sensitivity() and specificity() functions from the yardstick functions, using the event_level = “second” argument. This ensures that the factor levels are interpreted in a way that aligns with our initial demonstration.\n\nsens_y  &lt;- the_predictions  |&gt;\n  mutate(Observed = as.factor(Observed))  |&gt;\n  sensitivity(Observed, .pred_class, event_level = \"second\")\n\nspec_y  &lt;- the_predictions  |&gt;\n  mutate(Observed = as.factor(Observed))  |&gt;\n  specificity(Observed, .pred_class, event_level = \"second\")\n\nbind_rows(sens_y, spec_y)  |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    sensitivity\nbinary\n0.8\n    specificity\nbinary\n0.7\n  \n  \n  \n\n\n\n\nNow we’ve restored our original results. 😎\n\n\n\n\n\n\nCaution\n\n\n\nThis ‘switching’ behavior in yardstick is also apparent in the collect_metrics() function, making it essential to check the event level. Failure to do so may result in inadvertently switching the event level.\n\n\n\n\nSummary\nIn this blog post, we’ve walked through the process of computing sensitivity and specificity using the tidymodels package in R, demonstrating it with a simulated dataset. These metrics are indispensable for evaluating the performance of binary classification models. Don’t forget: for the most accurate and realistic results, always evaluate your models using separate test data.\n\n\n\n\n\n Back to topReferences\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. O’Reilly Media, Inc."
  },
  {
    "objectID": "posts/2024-02-03-health_literacy/health_literacy.html",
    "href": "posts/2024-02-03-health_literacy/health_literacy.html",
    "title": "Health Literacy: A Double-Edged Sword?",
    "section": "",
    "text": "In prevention science, information is often considered the cornerstone of any public health initiative. For example, the Health Belief Model, one of the most commonly employed frameworks in preventive interventions, posits that well-informed individuals are more likely to engage in preventive health behaviors (Rosenstock, 2000). This model suggests that people’s perceptions of susceptibility, severity, benefits, and barriers related to a specific health issue can predict their behavior. Consequently, interventions that provide information—such as highlighting the benefits of a particular action—tend to increase people’s engagement in expected healthy behaviors. During the pandemic, governments and institutions made significant efforts to raise public awareness about various important concepts related to public health. This widespread dissemination of information led to a general increase in health literacy, something previously unseen in past years. As a result, conversations within families began to include public health concepts like R0, infection rates, and social distancing, among others.\nWhile this mass education was intended to be beneficial, it also presented challenges. For instance, some people developed an overconfidence in their own expertise, leading to misunderstandings among the general population. The abundance of information gave rise to a wave of ‘influencers’ who negatively impacted pandemic response efforts. One example was the promotion of the medication ‘ivermectin’ by social media influencers as a preventive measure against COVID-19 complications, despite a lack of evidence supporting its efficacy (Bryant et al., 2021). Furthermore, governments imposed non-medical interventions based on a flawed understanding of the crisis. Measures such as cleaning the soles of shoes and sanitizing streets and commercial spaces were enforced as precautions. However, these actions were later found to be ineffective, leading the general population to question the assumptions underlying interventions aimed at ‘flattening the curve.’ Politicians capitalized on these misunderstandings to connect with a broad audience, often promoting agendas not aligned with public health goals. This politicization of the pandemic subsequently affected compliance with other measures, such as mask-wearing and vaccination. The issue became so politicized that people could even identify others’ political affiliations based on their mask usage or vaccination status, a phenomenon unprecedented in modern history.\nThis presents a ‘double-edged sword’ dilemma, where the dissemination of information can be both beneficial and detrimental. On the positive side, both theory and practice suggest that well-informed populations are more likely to adopt healthier behaviors. Conversely, public health concepts can be weaponized to advance specific political agendas that may conflict with general health goals. As a result, prevention scientists face the challenge of finding the right balance between promoting health literacy and mitigating its potential misuse. These reflections lead me to question the idea that ‘more information is always better.’ At times, an abundance of information can have unintended consequences, rendering individuals more susceptible to the agendas of specific groups.\nIn my experience in the field, I’ve noticed that people in communities interpret risk assessments about adolescent behavior in one of two ways. Some see the data as validation of their existing views, often negative, about young people. For example, if the data shows high rates of alcohol use among teens, these individuals may push for punitive measures like curfews. Conversely, others use the same data to advocate for more preventive programs in families and schools, aiming to better engage and support adolescents. This divergence prompts a critical question: What is our responsibility as disseminators of this information? Or, how can we strike the “right balance” in promoting health literacy to ensure it benefits the community?\nAs a prevention scientist, I find hard to ensure the responsible use of data and insights generated from scientific research. Our field faces a significant gap in effective communication tools, leading many of us to navigate the complexities of conveying scientific concepts through trial and error. The COVID-19 pandemic showed us the detrimental impact of information misuse, particularly in the flawed implementation of safety measures that could have saved lives. Meanwhile, data collection systems have evolved to become increasingly omnipresent, obligatory, and comprehensive. Information about our health, activities, and personal history is now routinely gathered during medical consultations and intervention programs. Though stored with the assurance of benefiting us, there are no guarantees.\nI’d like to leave some questions open for future discussion. Does expanding a population’s health vocabulary not only empower them but also make them more vulnerable to misinformation? Could the introduction of scientific terms into public discourse, such as ‘flatten the curve,’ potentially give institutions more control over individuals? If so, what strategies should we employ in communicating and disseminating these concepts to prevent institutional abuse?\n\n\n\n\n\n\n\n Back to topReferences\n\nBryant, A., Lawrie, T., Dowswell, T., Fordham, E., Mitchell, S., Hill, S., & Tham, T. (2021). Ivermectin for prevention and treatment of COVID-19 infection: A systematic review and meta-analysis. https://doi.org/10.21203/rs.3.rs-317485/v1\n\n\nRosenstock, I. M. (2000). Health Belief Model (pp. 78–80). Oxford University Press. https://doi.org/10.1037/10519-035"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Welcome to my webpage. My main goal is to develop evidence-based interventions for enhancing well-being. Using scientific methods, I generate evidence to assess these interventions’ effectiveness. I translate this data into actionable advice for schools, families, and policymakers. I’m committed to making a real impact on people’s lives through research."
  },
  {
    "objectID": "index.html#workshops",
    "href": "index.html#workshops",
    "title": "About me",
    "section": "Workshops",
    "text": "Workshops\n\nWorkshops website"
  },
  {
    "objectID": "index.html#web-projects",
    "href": "index.html#web-projects",
    "title": "About me",
    "section": "Web Projects",
    "text": "Web Projects\n\nBusiness That Care Toolkit\nThrustworthy"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "About me",
    "section": "R Packages",
    "text": "R Packages\n\nBreakNBuild: Designed to evaluate model performance with progressively sampled data.\nDocumentData: Designed to facilitate dataset documentation during package creation.\nwandR: Designed to transform R learning into an entertaining journey for all, by weaving the magic of the wizarding world into R programming.\nChessOlympiad: This package contains three datasets to analyze the performance of 937 players from 188 countries during the Chess Olympiad in Chennai, India in 2022."
  },
  {
    "objectID": "index.html#communities",
    "href": "index.html#communities",
    "title": "About me",
    "section": "Communities",
    "text": "Communities\n\nSoftware Carpentry\nrOpenSci\nSociety for Prevention Research\nInternational Network for Social Network Analysis"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "About me",
    "section": "Contact",
    "text": "Contact\n\n📧 foc9@miami.edu\nPersonal card"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "How to Create a Bash Script to Extract Bibliographic References from DOIs\n\n\n\nDOI\n\n\nBash\n\n\nReferences\n\n\n\n\nApr, 24\n\n\n\n\n\n\n\n\n\n\n\nBalancing Data in Machine Learning: When Good Intentions Meet Slippery Slopes\n\n\n\nMachine Learning\n\n\nCausal Inference\n\n\nCausality\n\n\n\n\nMar, 24\n\n\n\n\n\n\n\n\n\n\n\nHealth Literacy: A Double-Edged Sword?\n\n\n\nHealth\n\n\nPrevention\n\n\nPandemic\n\n\n\n\nFeb, 24\n\n\n\n\n\n\n\n\n\n\n\nTo Explain or to Predict\n\n\n\nExplain\n\n\nPrevention\n\n\nPredict\n\n\n\n\nJan, 24\n\n\n\n\n\n\n\n\n\n\n\nBalancing Efficiency and Flexibility: The Challenges of Over-Optimization in the Multiphase Optimization Strategy (MOST)\n\n\n\nMOST\n\n\nPrevention\n\n\noptimization\n\n\n\n\nDec, 23\n\n\n\n\n\n\n\n\n\n\n\nThe Chessboard and the Wise Courtier\n\n\n\nChess\n\n\nPython\n\n\nloops\n\n\nexponential growth\n\n\n\n\nAug, 23\n\n\n\n\n\n\n\n\n\n\n\nComputing Sensitivity and Specificity Using collect_metrics\n\n\n\nSensitivity\n\n\nTidymodels\n\n\nSpecificity\n\n\n\n\nJul, 23\n\n\n\n\n\n\n\n\n\n\n\nComputing Sensitivity and Specificity using tidymodels\n\n\n\nSensitivity\n\n\nTidymodels\n\n\nSpecificity\n\n\n\n\nJul, 23\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to the Recipes Package for Data Preprocessing\n\n\n\nTidymodels\n\n\n\n\nMay, 23\n\n\n\n\n\n\n\n\n\n\n\nMaximizing the Impact of Your R Teaching: Insights from Garrett Grolemund\n\n\n\nTeaching\n\n\n\n\nFeb, 23\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]