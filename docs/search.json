[
  {
    "objectID": "posts/2023-05-10-recipes/recipes.html",
    "href": "posts/2023-05-10-recipes/recipes.html",
    "title": "An Introduction to the Recipes Package for Data Preprocessing",
    "section": "",
    "text": "I liked this presentation done by Max Kuhn.\n\n\nHere are some notes:\nThe recipes package provides a framework for preprocessing data prior to modeling or visualization. With its pipeable sequence of steps and syntax similar to dplyr, the package simplifies a wide range of preprocessing tasks, from data normalization and missing data imputation, to categorical variable encoding and data transformation.\nIt’s important to remember when using the recipes package, the type of model you’re fitting can determine the necessary preprocessing steps for your data.\nIn addition to model-driven preprocessing steps, the recipes package also provides functions for feature engineering. This involves representing your data in ways most effective for your particular problem. For instance, you might create interaction terms, polynomial terms, or spline terms to capture non-linear relationships between predictors and the outcome.\nHere are some useful preprocessing steps:\n\nData normalization: The step_normalize() function normalizes your data by centering and scaling the variables. This is useful when working with models that require predictors to be on the same scale, such as k-nearest neighbors or neural networks.\nMissing data imputation: The step_impute_*() functions impute missing data using various methods, like mean imputation, median imputation, or k-nearest neighbors imputation.\nCategorical variable encoding: The step_dummy() function creates dummy variables for categorical predictors. This is handy when working with models that can’t handle categorical predictors directly, like linear regression or logistic regression.\nData transformation: The step_*() functions transform your data in various ways, such as applying the logarithm, square root, or Box-Cox transformation to a variable. This is useful when working with data that isn’t normally distributed or when trying to improve the linearity of the relationship between predictors and the outcome.\nFeature engineering: The step_*() functions are also used for feature engineering, such as creating interaction terms, polynomial terms, or spline terms. This is beneficial when trying to capture non-linear relationships between predictors and the outcome.\n\nLink to the package documentation\nThings to think:\nA point of confusion might be whether preprocessing is considered part of data cleaning or data transformation for modeling. It appears that there’s an overlap between data cleaning and data transformation, and it can sometimes be difficult to distinguish between these stages. It would be helpful to clarify the difference between these concepts and data preprocessing.\nWhen I try to imagine where recipes fits into these models, it’s not completely clear to me.\n\n\n\nThe data science process. From R for Data Science\n\n\n\n\n\nModeling Process. From Tidymodels book\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html",
    "href": "posts/2023-01-10-posit-education/Posit.html",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "",
    "text": "I recently watched a presentation by Garrett Grolemund, the Director of Learning at Rstudio, on the POSIT Enterprise Community Meetup on YouTube. I wanted to share with you some of the interesting thoughts about teaching R that he mentioned."
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html#context",
    "href": "posts/2023-01-10-posit-education/Posit.html#context",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "Context",
    "text": "Context\n\nGarret believes that the success of the training ultimately depends on how it’s delivered.\nTeachers out there really aren’t that good and online courses are not so great.\nPeople who become experts in R by years of practice. However, they may not have had the opportunity to devote those years to being a good teacher.\nThere is theories in psychology(such as cognitive load theory and multimedia learning theory) that can be applied to make training more successful.\nResearch shows that students only retain about 56% of the content from a lecture\nThe retention rate decreases over time.\nStudies on procedural instruction show similar results, with only 60% of students able to reproduce the procedure immediately after training.\nSix months later, the number drops to 40%, and a year later it drops to 30%\nTraining outcomes are not always great and this is a “dirty secret” of the training industry.\nTrainers are not always trained to be trainers, which is a challenge in the field.\nEducation is not simply about transferring information from an educator to a student, and the student then becoming an expert."
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html#insights",
    "href": "posts/2023-01-10-posit-education/Posit.html#insights",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "Insights",
    "text": "Insights\n\nPractice\n\nThe more the task is practiced, the more the brain will conserve the neural networks and retain the ability to perform the task. This is not likely to occur in a workshop that lasts only half a day or two days.\nTo build a robust neural network, it is important to sleep.\nThe ability to perform a new skill in six months from now dependent on the amount of practice they receive after initial instruction, rather than the instruction itself.\n\n\n“Well, the revelation we had at our studio is that data science is a skill, and if you want to learn to do good data science with code or otherwise, you have to practice it and learn it as a skill.”\n\n\n\nMentors, Mates and Accountability\n\nAs a student, it is possible to practice a skill incorrectly without even being aware of it. To practice effectively and efficiently, it is crucial to receive feedback and guidance to make sure the proper techniques are being utilized.\nIndividuals do not acquire knowledge in a vacuum, they are driven by social influences and the identity they construct by participating in a community that is studying data science.\nThe lacking factor in online courses is motivation, which can be obtained through interaction with a mentor or peers. Talking to them can provide the necessary inspiration.\nHaving a mentor or being part of a group provides accountability, as one is expected to show up with something to demonstrate to them.\n\n\n“So, as you go through the course, not only do you have an expert who has your back who’s coaching you, you also have fellow travelers who you could discuss things with. You could work through problems together and, you might not even realize it, but you can hold each other accountable and motivated as you go through the process.”\n\n\n\nRecomendations\n\nHow Learning Happens by Paul Kirschner and Carl. link to amazon. But also consult with your librarian friend\nVisit Posit Academy. You can learn in detail about the POSIT Academy model in minute 24."
  },
  {
    "objectID": "posts/2023-07-16-yarstick/2023-07-15.html",
    "href": "posts/2023-07-16-yarstick/2023-07-15.html",
    "title": "Computing Sensitivity and Specificity using tidymodels",
    "section": "",
    "text": "Sensitivity and specificity are two crucial metrics used to evaluate the performance of binary classification models. In this article, we delve into the process of computing these metrics using the ‘tidymodels’ package in R.\nWe begin by simulating a dataset. Assuming we have access to the true value of the phenomenon under consideration, as well as our model’s predictions, we can proceed to compute the sensitivity and specificity of our model.\n\nlibrary(tidyverse)  # Load tidyverse package\nlibrary(gt)         # Load gt package\nlibrary(gtsummary)  # Load gtsummary package\n\n\n# Create a tibble with the confusion matrix data\nconfusion_matrix_long &lt;- tibble(\n  'Predicted Class' = c('1', '1', '0', '0'),\n  'True Class' = c('True 1', 'True 0', 'True 1', 'True 0'),\n  'Count' = c(40, 15, 10, 35)\n)\n\nconfusion_matrix_long  |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      Predicted Class\n      True Class\n      Count\n    \n  \n  \n    1\nTrue 1\n40\n    1\nTrue 0\n15\n    0\nTrue 1\n10\n    0\nTrue 0\n35\n  \n  \n  \n\n\n\n\nFunction tbl_cross() offers a handy feature for obtaining percentages, which is useful when computing sensitivity and specificity.\n\n# Duplicate each row of the tibble based on the value in the Count column\nconfusion_matrix_long |&gt;\n  uncount(Count) |&gt;\n  tbl_cross(percent = \"col\")  \n\n\n\n\n\n  \n    \n    \n      \n      \n        True Class\n      \n      Total\n    \n    \n      True 0\n      True 1\n    \n  \n  \n    Predicted Class\n\n\n\n        0\n35 (70%)\n10 (20%)\n45 (45%)\n        1\n15 (30%)\n40 (80%)\n55 (55%)\n    Total\n50 (100%)\n50 (100%)\n100 (100%)\n  \n  \n  \n\n\n\n\nSo, we know the values of specificity and sensitivity, which are essentially the results of these two equations:\n\nSensitivity = TP / (TP + FN) = 40 / (40 + 10) = 0.80 (80%)\nSpecificity = TN / (TN + FP) = 35 / (35 + 15) = 0.70 (70%)\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nRefresh your memory on the meaning of these terms by looking at the following table:\n\n\n\n\n\nTrue 1 (Positive)\nTrue 0 (Negative)\n\n\n\n\nPredict 1 (Positive)\nTP = 40\nFP = 15\n\n\nPredict 0 (Negative)\nFN = 10\nTN = 35\n\n\n\nThis table above is the confusion matrix, featuring the following terms:\n\nTP (True Positive): Instances where the model correctly predicted the positive class.\nTN (True Negative): Instances where the model correctly predicted the negative class.\nFP (False Positive): Instances where the model incorrectly predicted the positive class.\nFN (False Negative): Instances where the model incorrectly predicted the negative class.\n\n\n\n\nHowever, the burning question remains: how do we calculate these in R? Let’s explore this while evaluating the performance of our models using the ‘tidymodels’ package.\n\nModeling with tidymodels\nLet’s now assume that we have a dataset comprising Y and X values, and we aim to create a model that predicts Y based on X. We’ll use the same dataset we created earlier.\n\ndf &lt;- confusion_matrix_long |&gt; \n  uncount(Count)  |&gt; \n  rename(y = `True Class`, x = `Predicted Class`) # Rename the columns to shorter names\n\nFirst, we’ll load the tidymodels library and specify our model.\n\n# Load the tidymodels package\nlibrary(tidymodels)\n\n# Define a logistic regression model specification\nmodel_spec &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt; \n  set_mode(\"classification\")\n\n# Define a recipe for preprocessing the data\nrecipe_glm &lt;- \n  recipe(y ~ x, data = df) |&gt; \n  # Convert all nominal variables to dummy variables\n  step_dummy(all_nominal(), -all_outcomes()) \n\nLet’s examine the dataset we’ve created.\n\n# Preprocess the data using the recipe\n# This includes converting nominal variables to dummy variables\nrecipe_glm |&gt;\n  prep() |&gt;\n  # Apply the recipe to new data\n  bake(new_data = df) |&gt;\n  # View the first few rows of the preprocessed data\n  skimr::skim()\n\n\nData summary\n\n\nName\nbake(prep(recipe_glm), ne…\n\n\nNumber of rows\n100\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ny\n0\n1\nFALSE\n2\nTru: 50, Tru: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nx_X1\n0\n1\n0.55\n0.5\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\n\n\n\n\nRemeber that our outcome is a binary variable with values True 0 and True 1.\nNext, we can create a workflow that combines our recipe and the model specification, and fit the model.\n\n# Define a workflow for fitting the logistic regression model\nwr_glm &lt;- workflow() |&gt; \n  add_recipe(recipe_glm) |&gt; \n  add_model(model_spec)  \n\n# Fit the logistic regression model to the preprocessed data\nmodel &lt;- wr_glm |&gt; \n  fit(data = df) \n\n# Extract the model coefficients and create a summary table\nmodel |&gt; \n  extract_fit_parsnip() |&gt; \n  tbl_regression()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    x_X1\n2.2\n1.3, 3.2\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nSo, we’ve established that the model predicts the Y values based on the X values. But how do we calculate the sensitivity and specificity using the model results?\nFirst, we need to generate predictions. The augment() function from the dplyr package is an excellent choice for this task.\n\n# Load the reactable package\nlibrary(reactable)\n\n# Use the augment() function to add predicted values to the data frame\nthe_predictions &lt;- model |&gt;\n  augment(df)  |&gt; \n  # Rename the \"y\" column to \"Observed\"\n  rename(\"Observed\" = \"y\")\n\n\n# Create an interactive table using the reactable::reactable() function\nthe_predictions |&gt;\n  reactable::reactable(\n    groupBy = \"Observed\",\n    columns = list(\n      x = colDef(aggregate = \"unique\"),\n      .pred_class = colDef(aggregate = \"unique\"),\n      `.pred_True 0` = colDef(aggregate = \"mean\", format = colFormat(digits = 2)),\n      `.pred_True 1` = colDef(aggregate = \"mean\", format = colFormat(digits = 2))\n    )\n  )\n\n\n\n\n\n\nAt this point, we have both the predicted class and the observed class. We could use the same method we employed in the earlier demonstration (tbl_cross()), or we can make use of the sensitivity() and specificity() functions from the yardstick package. Let’s look at how this works.\n\n\nComputing sensitivity and specificity using a cross table\n\nthe_predictions  |&gt; \n    tbl_cross(.pred_class, Observed, percent = \"col\")\n\n\n\n\n\n  \n    \n    \n      \n      \n        Observed\n      \n      Total\n    \n    \n      True 0\n      True 1\n    \n  \n  \n    .pred_class\n\n\n\n        True 0\n35 (70%)\n10 (20%)\n45 (45%)\n        True 1\n15 (30%)\n40 (80%)\n55 (55%)\n    Total\n50 (100%)\n50 (100%)\n100 (100%)\n  \n  \n  \n\n\n\n\nThis yields the same results as before.\n\n\nComputing sensitivity and specificity using the yardstick package\nFinally! we can use the sensitivity() and specificity() functions from the yardstick package to calculate the sensitivity and specificity of our model.\n\nsens &lt;- the_predictions  |&gt; \n  mutate(Observed = as.factor(Observed))  |&gt;\n  sensitivity(Observed, .pred_class)\n\nspec &lt;- the_predictions  |&gt; \n  mutate(Observed = as.factor(Observed))  |&gt;\n  specificity(Observed, .pred_class)\n\nbind_rows(sens, spec)  |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    sensitivity\nbinary\n0.7\n    specificity\nbinary\n0.8\n  \n  \n  \n\n\n\n\nOops! That doesn’t look right. Why is that?\nIn the context of logistic regression, R’s default behavior is to take the first level of a factor as the reference category when using the glm function. This behavior becomes particularly important when we’re dealing with binary outcomes, often coded as 0 (absence of the event) and 1 (presence of the event). By default, R will take 0 as the reference level and compare it against 1, due to 0 coming before 1 in numerical order.\nHowever, when we’re working with factors, the order in which they’re arranged is somewhat arbitrary and it doesn’t necessarily make sense to always treat 0 as the absence of an event and 1 as the presence of an event (Kuhn and Silge 2022).\n\n\n\n\n\n\nThis is from here or page 116 in the tidymodels book.\n\n\n\ntidymodels. pag 116\n\n\n\n\n\nThe yardstick package interprets this setup a bit differently. It views the first factor as the most important, leading it to switch the factor levels in the process. This change in order may affect our sensitivity and specificity measures.\nTo maintain consistency with our earlier computations, it’s necessary to explicitly set the reference level in the sensitivity() and specificity() functions from the yardstick functions, using the event_level = “second” argument. This ensures that the factor levels are interpreted in a way that aligns with our initial demonstration.\n\nsens_y  &lt;- the_predictions  |&gt;\n  mutate(Observed = as.factor(Observed))  |&gt;\n  sensitivity(Observed, .pred_class, event_level = \"second\")\n\nspec_y  &lt;- the_predictions  |&gt;\n  mutate(Observed = as.factor(Observed))  |&gt;\n  specificity(Observed, .pred_class, event_level = \"second\")\n\nbind_rows(sens_y, spec_y)  |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    sensitivity\nbinary\n0.8\n    specificity\nbinary\n0.7\n  \n  \n  \n\n\n\n\nNow we’ve restored our original results. 😎\n\n\n\n\n\n\nCaution\n\n\n\nThis ‘switching’ behavior in yardstick is also apparent in the collect_metrics() function, making it essential to check the event level. Failure to do so may result in inadvertently switching the event level.\n\n\n\n\nSummary\nIn this blog post, we’ve walked through the process of computing sensitivity and specificity using the tidymodels package in R, demonstrating it with a simulated dataset. These metrics are indispensable for evaluating the performance of binary classification models. Don’t forget: for the most accurate and realistic results, always evaluate your models using separate test data.\n\n\n\n\n\n Back to topReferences\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. O’Reilly Media, Inc."
  },
  {
    "objectID": "posts/2022-12-01-ChessOlympiad22/index.html",
    "href": "posts/2022-12-01-ChessOlympiad22/index.html",
    "title": "Chess Olympiad 2022",
    "section": "",
    "text": "library(ChessOlympiad22)\n\n\nDoes ELO predict the wining move?\nThe ELO rating system is a method for calculating the relative skill levels of chess players. The rating is based on the results of games played, and is used to compare the strength of one player to another. The higher a player’s ELO rating, the more skilled they are considered to be.\nIn this document, the ELO difference between two players is used to evaluate the probability of one player winning the match.\n\nLibraries\n\n\nlibrary(rpart.plot)\nlibrary(tidymodels)\n\n\n\nLoad the players data\n\ndata(\"players\")\n\n\n\nDistribution of ELO\nFirst, let’s visualize the ELO distribution of the players.\nIt is important to filter out players with an ELO rating of zero from the analysis, as these players have not yet been rated and do not have a known skill level.\n\nplayers %&gt;% \n  filter(rtg!=0) %&gt;% \n  ggplot(aes(rtg)) + \n  geom_histogram(bins = 50, fill=\"gray20\") +\n  scale_x_continuous(breaks = seq(1000,2900,100))+\n  theme_minimal() +\n  coord_flip() +\n  theme(axis.text = element_text(\"\")) +\n  labs(x=\"Elo Rating System (player strength)\",\n       y=\"Number of players\",\n       title = \"Elo Rating System of players in Chess Olympiad, 2022. \n       Chennai, India \",\n       caption = \"Magnus Carlsen has the highest Elo rating in the tournoment:2864\")\n\n\n\n\nIt may be interesting to plot the ELO ratings of players by federation, as the Chess Olympiad is played by national teams. By examining the ELO ratings of players within each federation, we can get a sense of the overall strength of the teams participating in the event. This analysis could potentially provide insight into the results of the Chess Olympiad and help predict the outcomes of matches.\n\nplayers %&gt;% \n  filter(rtg&gt;2600) %&gt;% \n  ggplot(aes(reorder(fed, rtg),rtg)) + \n  theme_minimal() +\n  geom_boxplot() +\n  coord_flip() +\n  labs(x=\"\", y=\"ELO\")\n\n\n\n\nThe United States have the strongest team in the tournament.\n\n\nLoad results data\n\ndata(\"results\")\n\n\n\nDifferences in ELO by round\nThe Chess Olympiad was a Swiss-style tournament, which means that players are paired against opponents with similar scores in each round. In the first round, the highest-ranked player is matched against the player in the middle of the ELO rating distribution. The second highest-ranked player is then matched against the player just below the player in the middle of the ELO rating distribution, and so on. This implies that ELO differences are maximized in the first round. Then, the pairings for each subsequent round are determined based on the results of the previous rounds, with the goal of ensuring that players face opponents with similar scores.\nA visual representation of the differences in ELO by round are presented in the following graph.\n\n\nresults %&gt;% \n  filter(elo_difference&gt;=-1000, \n         elo_difference&lt;=1000, \n         !is.na(elo_white), !is.na(elo_black), elo_white!=0,elo_black!=0) %&gt;% \n  ggplot(aes(as.numeric(elo_white),elo_difference, \n             fill=factor(result_white), \n             color=factor(result_white))) + \n  geom_point(shape = 21, alpha=0.85,\n             size = 3, stroke = 0.5) +\n  theme_minimal() +\n  scale_fill_manual(values=c(\"Lost\"=\"black\", \"Draw\"=\"gray50\",\"Won\"=\"white\")) +\n  scale_color_manual(values=c(\"Lost\"=\"black\", \"Draw\"=\"black\",\"Won\"=\"black\")) +\n  labs(fill=\"Result\", color=\"Result\", \n       x=\"Player Elo\", \n       y=\"Elo difference\",\n       caption = \"Difference greater than zero indicates stronger player\n       44th Chess Olympiad. Chennai, 2022 Open\") +\n  facet_wrap(~ round)\n\n\n\n\n\n\n\nModel the winning chances for players with the white pieces based on ELO difference.\nNow, we can identify the best split of the differences in ELO ratings to classify the results of chess games. I will use tidymodels to estimate a CART model.\n\nSet the engine\n\ncart_spec &lt;-\n   decision_tree() %&gt;% \n   set_engine(\"rpart\") %&gt;%\n   set_mode(\"classification\")\n\n\n\nCreate a recipie\nI will add two steps in the recipe. One to filter the data set by round, and the other to convert results in a factor variable. I also will limit my analysis to players with more than 1600 in ELO.\n\nresults &lt;- results %&gt;% \n  filter(as.numeric(elo_white)&gt;1600) %&gt;% \n  filter(as.numeric(elo_black)&gt;1600) \n\nrecipe &lt;- recipe(\n  result_white ~ elo_difference + round_number, data = results) %&gt;% \n    step_filter(round_number==round)\n  \n\n\n\nCreate a workflow\n\nwrkfl &lt;- workflow() %&gt;% \n  add_model(cart_spec) %&gt;% \n  add_recipe(recipe)\n\n\n\nEstimate the model\nI will estimate the model for the round 1.\n\n\nround &lt;- 1\n\ncart_fit &lt;- wrkfl %&gt;% \n  fit(data=results) %&gt;% \n  extract_fit_parsnip()\n\n\n\nDraw an tree to understand the results\nFinally, I will create a tree showing the splits\n\n\ncart_fit &lt;- repair_call(cart_fit, data = round)\n\ncart_tree_fit &lt;- cart_fit$fit\n\n\nrpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)\n\n\n\nrpart.rules(cart_tree_fit, cover = TRUE)\n#&gt; Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\n#&gt; To silence this warning:\n#&gt;     Call rpart.rules with roundint=FALSE,\n#&gt;     or rebuild the rpart model with model=TRUE.\n#&gt;   ..y  Dra Los Won                               cover\n#&gt;  Lost [.09 .91 .00] when elo_difference &lt;  -12     51%\n#&gt;   Won [.04 .01 .95] when elo_difference &gt;= -12     49%\n\nAccording to the model, a difference of 12 in the ELO is sufficient to accurately predict the winner in 95% of cases and the loser in 91% of cases.\n\n\n\nModel the last round\nLet’s test the model for the final round, where the matches happened between the strongest opponents.\nI will add a the tree_depth parameter to my model. The depth of the tree refers to the number of levels the tree has.\n\ncart_spec &lt;-\n   decision_tree(tree_depth = 4) %&gt;% \n   set_engine(\"rpart\") %&gt;%\n   set_mode(\"classification\")\n\nwrkfl &lt;- workflow() %&gt;% \n  add_model(cart_spec) %&gt;% \n  add_recipe(recipe)\n\n\nround &lt;- 11\n\ncart_fit &lt;- wrkfl %&gt;% \n  fit(data=results)\n\ncart_fit &lt;- wrkfl %&gt;% \n  fit(data=results) %&gt;% \n  extract_fit_parsnip()\n\ncart_fit &lt;- repair_call(cart_fit, data = round)\n\ncart_tree_fit &lt;- cart_fit$fit\n\nrpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)\n\n\n\nrpart.rules(cart_tree_fit, cover = TRUE)\n#&gt; Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\n#&gt; To silence this warning:\n#&gt;     Call rpart.rules with roundint=FALSE,\n#&gt;     or rebuild the rpart model with model=TRUE.\n#&gt;   ..y  Dra Los Won                                       cover\n#&gt;  Draw [.45 .30 .25] when elo_difference is -176 to  85     51%\n#&gt;  Draw [.55 .09 .36] when elo_difference &gt;=         333      4%\n#&gt;  Draw [.69 .15 .15] when elo_difference is   95 to 115      4%\n#&gt;  Lost [.29 .61 .10] when elo_difference &lt;  -176            20%\n#&gt;   Won [.10 .30 .60] when elo_difference is   85 to  95      3%\n#&gt;   Won [.17 .20 .63] when elo_difference is  115 to 333     18%\n\nThe colors of the leaves in the decision tree suggest that ELO differences continue to be an important factor in the final round of the chess tournament. The model appears to be more accurate at predicting the outcomes of games with large ELO differences. For example, if you have more than 333 points in ELO, the model predicts 63% wining chances for you. However, if the ELO difference is less than 333 (but more than 115), the model predicts 55% of draw.\nWhen the ELO difference is less than 115, the model’s predictions become more interesting. If a player has 176 points less than their opponent, the model is more likely to classify them as a loser (61%). However, if the ELO difference is less than 85 points, the player still has a good chance of winning the game. This could be seen as an indicator of the performance of some players with lower ELO ratings who are having a strong tournament. On the other hand, if the ELO difference is greater than 85 points, most of the chances are for a draw.\nThis was a quick example of how to use ChessOlympiad package.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/thrusworthy.html",
    "href": "posts/thrusworthy.html",
    "title": "Trustworthy",
    "section": "",
    "text": "Francisco Cardozo, Pablo Montero-Zamora\nConfidence in students’ survey responses is commonly questioned when performing drug use research. For example, one’s can wonder how do you know students are telling the truth about their alcohol consumption? We recognize this as a legitimate question that can be even more complex when considering two types of students: 1) those who say they have not used alcohol but used it (i.e., deniers), and 2) those who say they have used it but never use it (i.e., braggers). It is crucial to understand how this reporting bias affects the validity of our measurements and research findings. Therefore, we propose the following app to know how the proportions of deniers, braggers, and drug use prevalence can influence confidence levels in self-reported measures collected in adolescents.\n\nTrue prevalence: number of students using a drug divided by the population.\n\nDeniers: students who say they have not used a drug but used it.\nBraggers: students who say they have used a drug but never used it.\n\nGiven the information above, we can estimate the probability of a student’s drug use behavior if they respond Yes or No to a drug use question.\nTo model this probability, we can use the Bayes theorem:\n\nP(A|B) = Drug use given they say yes in the questionnaire.\nP(B|A) = Say yes given that they have used drugs.\nP(A) = Drug use prevalence.\nP(B) = Say yes in the questionnaire."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francisco Cardozo",
    "section": "",
    "text": "Thank you for visiting my webpage. I hope you find it informative. If you have any further questions or would like to learn more about my work, please do not hesitate to contact me."
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Francisco Cardozo",
    "section": "",
    "text": "Thank you for visiting my webpage. I hope you find it informative. If you have any further questions or would like to learn more about my work, please do not hesitate to contact me."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Francisco Cardozo",
    "section": "About me",
    "text": "About me\nMy primary goal is to evaluate the real-life impact of interventions. I strive to combine my expertise in measurement and research design to obtain rigorous and meaningful results. I am committed to translating complex research findings into useful insights that can inform decision-making. I believe in the power of research to effect change and I am dedicated to making that change tangible and understandable for everyone.\n\n📰 CV\n🧑 Teaching\n📝 Publications\n GitHub"
  },
  {
    "objectID": "index.html#web-projects",
    "href": "index.html#web-projects",
    "title": "Francisco Cardozo",
    "section": "Web Projects",
    "text": "Web Projects\n\nBussiness that Care Toolkit\nThrusworthy"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "Francisco Cardozo",
    "section": "R Packages",
    "text": "R Packages\n\nDocumentData\nChessOlympiad"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\nReading Time\n\n\n\n\n\n\nJul 16, 2023\n\n\nComputing Sensitivity and Specificity using tidymodels\n\n\nSensitivity,Tidymodels,Specificity\n\n\n7 min\n\n\n\n\nMay 10, 2023\n\n\nAn Introduction to the Recipes Package for Data Preprocessing\n\n\nTidymodels\n\n\n3 min\n\n\n\n\nFeb 10, 2023\n\n\nMaximizing the Impact of Your R Teaching: Insights from Garrett Grolemund\n\n\nTeaching\n\n\n3 min\n\n\n\n\nDec 1, 2022\n\n\nChess Olympiad 2022\n\n\nR\n\n\n6 min\n\n\n\n\nNov 27, 2022\n\n\nTrustworthy\n\n\nAlcohol\n\n\n2 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]