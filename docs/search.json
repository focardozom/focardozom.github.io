[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to prevention science meets sci-fi.\n\n\n\nPrevention\n\n\nPrograms\n\n\nEfficacy\n\n\nImplementation\n\n\n\n\nMar, 25\n\n\n\n\n\n\n\n\n\n\n\nThree Components of a Successful Prevention Program\n\n\n\nPrevention\n\n\nPrograms\n\n\nEvaluation\n\n\nImplementation\n\n\n\n\nDec, 24\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Bash Script for Easy File Conversion Using DuckDB\n\n\n\nBash\n\n\nDuckDB\n\n\nData\n\n\n\n\nNov, 24\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Bash Script to Automatically Organize Your Files\n\n\n\nBash\n\n\nOrganization\n\n\nFiles\n\n\n\n\nNov, 24\n\n\n\n\n\n\n\n\n\n\n\nHow much do we trust a student’s answer about drug use?\n\n\n\nShiny\n\n\nDrug Use\n\n\nBayes\n\n\n\n\nJul, 24\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Bash Script to Extract Bibliographic References from DOIs\n\n\n\nDOI\n\n\nBash\n\n\nReferences\n\n\n\n\nApr, 24\n\n\n\n\n\n\n\n\n\n\n\nBalancing Data in Machine Learning: When Good Intentions Meet Slippery Slopes\n\n\n\nMachine Learning\n\n\nCausal Inference\n\n\nCausality\n\n\n\n\nMar, 24\n\n\n\n\n\n\n\n\n\n\n\nHealth Literacy: A Double-Edged Sword?\n\n\n\nHealth\n\n\nPrevention\n\n\nPandemic\n\n\n\n\nFeb, 24\n\n\n\n\n\n\n\n\n\n\n\nTo Explain or to Predict\n\n\n\nExplain\n\n\nPrevention\n\n\nPredict\n\n\n\n\nJan, 24\n\n\n\n\n\n\n\n\n\n\n\nBalancing Efficiency and Flexibility: The Challenges of Over-Optimization in the Multiphase Optimization Strategy (MOST)\n\n\n\nMOST\n\n\nPrevention\n\n\noptimization\n\n\n\n\nDec, 23\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/2024-02-03-health_literacy/health_literacy.html",
    "href": "posts/2024-02-03-health_literacy/health_literacy.html",
    "title": "Health Literacy: A Double-Edged Sword?",
    "section": "",
    "text": "In prevention science, information is often considered the cornerstone of any public health initiative. For example, the Health Belief Model, one of the most commonly employed frameworks in preventive interventions, posits that well-informed individuals are more likely to engage in preventive health behaviors (Rosenstock, 2000). This model suggests that people’s perceptions of susceptibility, severity, benefits, and barriers related to a specific health issue can predict their behavior. Consequently, interventions that provide information—such as highlighting the benefits of a particular action—tend to increase people’s engagement in expected healthy behaviors. During the pandemic, governments and institutions made significant efforts to raise public awareness about various important concepts related to public health. This widespread dissemination of information led to a general increase in health literacy, something previously unseen in past years. As a result, conversations within families began to include public health concepts like R0, infection rates, and social distancing, among others.\nWhile this mass education was intended to be beneficial, it also presented challenges. For instance, some people developed an overconfidence in their own expertise, leading to misunderstandings among the general population. The abundance of information gave rise to a wave of ‘influencers’ who negatively impacted pandemic response efforts. One example was the promotion of the medication ‘ivermectin’ by social media influencers as a preventive measure against COVID-19 complications, despite a lack of evidence supporting its efficacy (Bryant et al., 2021). Furthermore, governments imposed non-medical interventions based on a flawed understanding of the crisis. Measures such as cleaning the soles of shoes and sanitizing streets and commercial spaces were enforced as precautions. However, these actions were later found to be ineffective, leading the general population to question the assumptions underlying interventions aimed at ‘flattening the curve.’ Politicians capitalized on these misunderstandings to connect with a broad audience, often promoting agendas not aligned with public health goals. This politicization of the pandemic subsequently affected compliance with other measures, such as mask-wearing and vaccination. The issue became so politicized that people could even identify others’ political affiliations based on their mask usage or vaccination status, a phenomenon unprecedented in modern history.\nThis presents a ‘double-edged sword’ dilemma, where the dissemination of information can be both beneficial and detrimental. On the positive side, both theory and practice suggest that well-informed populations are more likely to adopt healthier behaviors. Conversely, public health concepts can be weaponized to advance specific political agendas that may conflict with general health goals. As a result, prevention scientists face the challenge of finding the right balance between promoting health literacy and mitigating its potential misuse. These reflections lead me to question the idea that ‘more information is always better.’ At times, an abundance of information can have unintended consequences, rendering individuals more susceptible to the agendas of specific groups.\nIn my experience in the field, I’ve noticed that people in communities interpret risk assessments about adolescent behavior in one of two ways. Some see the data as validation of their existing views, often negative, about young people. For example, if the data shows high rates of alcohol use among teens, these individuals may push for punitive measures like curfews. Conversely, others use the same data to advocate for more preventive programs in families and schools, aiming to better engage and support adolescents. This divergence prompts a critical question: What is our responsibility as disseminators of this information? Or, how can we strike the “right balance” in promoting health literacy to ensure it benefits the community?\nAs a prevention scientist, I find hard to ensure the responsible use of data and insights generated from scientific research. Our field faces a significant gap in effective communication tools, leading many of us to navigate the complexities of conveying scientific concepts through trial and error. The COVID-19 pandemic showed us the detrimental impact of information misuse, particularly in the flawed implementation of safety measures that could have saved lives. Meanwhile, data collection systems have evolved to become increasingly omnipresent, obligatory, and comprehensive. Information about our health, activities, and personal history is now routinely gathered during medical consultations and intervention programs. Though stored with the assurance of benefiting us, there are no guarantees.\nI’d like to leave some questions open for future discussion. Does expanding a population’s health vocabulary not only empower them but also make them more vulnerable to misinformation? Could the introduction of scientific terms into public discourse, such as ‘flatten the curve,’ potentially give institutions more control over individuals? If so, what strategies should we employ in communicating and disseminating these concepts to prevent institutional abuse?\n\n\n\n\n\n\n\n Back to topReferences\n\nBryant, A., Lawrie, T., Dowswell, T., Fordham, E., Mitchell, S., Hill, S., & Tham, T. (2021). Ivermectin for prevention and treatment of COVID-19 infection: A systematic review and meta-analysis. https://doi.org/10.21203/rs.3.rs-317485/v1\n\n\nRosenstock, I. M. (2000). Health Belief Model (pp. 78–80). Oxford University Press. https://doi.org/10.1037/10519-035"
  },
  {
    "objectID": "posts/2023-07-15-yarstick/2023-07-15.html",
    "href": "posts/2023-07-15-yarstick/2023-07-15.html",
    "title": "Computing Sensitivity and Specificity using tidymodels",
    "section": "",
    "text": "Sensitivity and specificity are two crucial metrics used to evaluate the performance of binary classification models. In this article, we delve into the process of computing these metrics using the ‘tidymodels’ package in R.\nWe begin by simulating a dataset. Assuming we have access to the true value of the phenomenon under consideration, as well as our model’s predictions, we can proceed to compute the sensitivity and specificity of our model.\n\nlibrary(tidyverse)  # Load tidyverse package\nlibrary(gt)         # Load gt package\nlibrary(gtsummary)  # Load gtsummary package\n\n\n# Create a tibble with the confusion matrix data\nconfusion_matrix_long &lt;- tibble(\n  'Predicted Class' = c('1', '1', '0', '0'),\n  'True Class' = c('True 1', 'True 0', 'True 1', 'True 0'),\n  'Count' = c(40, 15, 10, 35)\n)\n\nconfusion_matrix_long  |&gt; gt()\n\n\n\n\n\n\n\nPredicted Class\nTrue Class\nCount\n\n\n\n\n1\nTrue 1\n40\n\n\n1\nTrue 0\n15\n\n\n0\nTrue 1\n10\n\n\n0\nTrue 0\n35\n\n\n\n\n\n\n\nFunction tbl_cross() offers a handy feature for obtaining percentages, which is useful when computing sensitivity and specificity.\n\n# Duplicate each row of the tibble based on the value in the Count column\nconfusion_matrix_long |&gt;\n  uncount(Count) |&gt;\n  tbl_cross(percent = \"col\")  \n\n\n\n\n\n\n\n\nTrue Class\nTotal\n\n\nTrue 0\nTrue 1\n\n\n\n\nPredicted Class\n\n\n\n\n\n    0\n35 (70%)\n10 (20%)\n45 (45%)\n\n\n    1\n15 (30%)\n40 (80%)\n55 (55%)\n\n\nTotal\n50 (100%)\n50 (100%)\n100 (100%)\n\n\n\n\n\n\n\nSo, we know the values of specificity and sensitivity, which are essentially the results of these two equations:\n\nSensitivity = TP / (TP + FN) = 40 / (40 + 10) = 0.80 (80%)\nSpecificity = TN / (TN + FP) = 35 / (35 + 15) = 0.70 (70%)\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nRefresh your memory on the meaning of these terms by looking at the following table:\n\n\n\n\n\nTrue 1 (Positive)\nTrue 0 (Negative)\n\n\n\n\nPredict 1 (Positive)\nTP = 40\nFP = 15\n\n\nPredict 0 (Negative)\nFN = 10\nTN = 35\n\n\n\nThis table above is the confusion matrix, featuring the following terms:\n\nTP (True Positive): Instances where the model correctly predicted the positive class.\nTN (True Negative): Instances where the model correctly predicted the negative class.\nFP (False Positive): Instances where the model incorrectly predicted the positive class.\nFN (False Negative): Instances where the model incorrectly predicted the negative class.\n\n\n\n\nHowever, the burning question remains: how do we calculate these in R? Let’s explore this while evaluating the performance of our models using the ‘tidymodels’ package.\n\nModeling with tidymodels\nLet’s now assume that we have a dataset comprising Y and X values, and we aim to create a model that predicts Y based on X. We’ll use the same dataset we created earlier.\n\ndf &lt;- confusion_matrix_long |&gt; \n  uncount(Count)  |&gt; \n  rename(y = `True Class`, x = `Predicted Class`) # Rename the columns to shorter names\n\nFirst, we’ll load the tidymodels library and specify our model.\n\n# Load the tidymodels package\nlibrary(tidymodels)\n\nWarning: package 'scales' was built under R version 4.2.3\n\n\nWarning: package 'recipes' was built under R version 4.2.3\n\n\nWarning: package 'yardstick' was built under R version 4.2.3\n\n# Define a logistic regression model specification\nmodel_spec &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt; \n  set_mode(\"classification\")\n\n# Define a recipe for preprocessing the data\nrecipe_glm &lt;- \n  recipe(y ~ x, data = df) |&gt; \n  # Convert all nominal variables to dummy variables\n  step_dummy(all_nominal(), -all_outcomes()) \n\nLet’s examine the dataset we’ve created.\n\n# Preprocess the data using the recipe\n# This includes converting nominal variables to dummy variables\nrecipe_glm |&gt;\n  prep() |&gt;\n  # Apply the recipe to new data\n  bake(new_data = df) |&gt;\n  # View the first few rows of the preprocessed data\n  skimr::skim()\n\n\nData summary\n\n\nName\nbake(prep(recipe_glm), ne…\n\n\nNumber of rows\n100\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ny\n0\n1\nFALSE\n2\nTru: 50, Tru: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nx_X1\n0\n1\n0.55\n0.5\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\n\n\n\n\nRemeber that our outcome is a binary variable with values True 0 and True 1.\nNext, we can create a workflow that combines our recipe and the model specification, and fit the model.\n\n# Define a workflow for fitting the logistic regression model\nwr_glm &lt;- workflow() |&gt; \n  add_recipe(recipe_glm) |&gt; \n  add_model(model_spec)  \n\n# Fit the logistic regression model to the preprocessed data\nmodel &lt;- wr_glm |&gt; \n  fit(data = df) \n\n# Extract the model coefficients and create a summary table\nmodel |&gt; \n  extract_fit_parsnip() |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nx_X1\n2.2\n1.3, 3.2\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nSo, we’ve established that the model predicts the Y values based on the X values. But how do we calculate the sensitivity and specificity using the model results?\nFirst, we need to generate predictions. The augment() function from the dplyr package is an excellent choice for this task.\n\n# Load the reactable package\nlibrary(reactable)\nlibrary(knitr)\n\n# Use the augment() function to add predicted values to the data frame\nthe_predictions &lt;- model |&gt;\n  augment(df)  |&gt; \n  # Rename the \"y\" column to \"Observed\"\n  rename(\"Observed\" = \"y\")\n\n\n# Create an interactive table using the reactable::reactable() function\nthe_predictions |&gt;\n  reactable(\n    groupBy = \"Observed\",\n    columns = list(\n      x = colDef(aggregate = \"unique\"),\n      .pred_class = colDef(aggregate = \"unique\"),\n      `.pred_True 0` = colDef(aggregate = \"mean\", format = colFormat(digits = 2)),\n      `.pred_True 1` = colDef(aggregate = \"mean\", format = colFormat(digits = 2))\n    )\n  )\n\n\n\n\n\nAt this point, we have both the predicted class and the observed class. We could use the same method we employed in the earlier demonstration (tbl_cross()), or we can make use of the sensitivity() and specificity() functions from the yardstick package. Let’s look at how this works.\n\n\nComputing sensitivity and specificity using a cross table\n\nthe_predictions  |&gt; \n    tbl_cross(.pred_class, Observed, percent = \"col\")\n\n\n\n\n\n\n\n\nObserved\nTotal\n\n\nTrue 0\nTrue 1\n\n\n\n\n.pred_class\n\n\n\n\n\n    True 0\n35 (70%)\n10 (20%)\n45 (45%)\n\n\n    True 1\n15 (30%)\n40 (80%)\n55 (55%)\n\n\nTotal\n50 (100%)\n50 (100%)\n100 (100%)\n\n\n\n\n\n\n\nThis yields the same results as before.\n\n\nComputing sensitivity and specificity using the yardstick package\nFinally! we can use the sensitivity() and specificity() functions from the yardstick package to calculate the sensitivity and specificity of our model.\n\nsens &lt;- the_predictions  |&gt; \n  mutate(Observed = as.factor(Observed))  |&gt;\n  sensitivity(Observed, .pred_class)\n\nspec &lt;- the_predictions  |&gt; \n  mutate(Observed = as.factor(Observed))  |&gt;\n  specificity(Observed, .pred_class)\n\nbind_rows(sens, spec)  |&gt; \n  gt()\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nsensitivity\nbinary\n0.7\n\n\nspecificity\nbinary\n0.8\n\n\n\n\n\n\n\nOops! That doesn’t look right. Why is that?\nIn the context of logistic regression, R’s default behavior is to take the first level of a factor as the reference category when using the glm function. This behavior becomes particularly important when we’re dealing with binary outcomes, often coded as 0 (absence of the event) and 1 (presence of the event). By default, R will take 0 as the reference level and compare it against 1, due to 0 coming before 1 in numerical order.\nHowever, when we’re working with factors, the order in which they’re arranged is somewhat arbitrary and it doesn’t necessarily make sense to always treat 0 as the absence of an event and 1 as the presence of an event (Kuhn and Silge 2022).\n\n\n\n\n\n\nThis is from here or page 116 in the tidymodels book.\n\n\n\ntidymodels. pag 116\n\n\n\n\n\nThe yardstick package interprets this setup a bit differently. It views the first factor as the most important, leading it to switch the factor levels in the process. This change in order may affect our sensitivity and specificity measures.\nTo maintain consistency with our earlier computations, it’s necessary to explicitly set the reference level in the sensitivity() and specificity() functions from the yardstick functions, using the event_level = “second” argument. This ensures that the factor levels are interpreted in a way that aligns with our initial demonstration.\n\nsens_y  &lt;- the_predictions  |&gt;\n  mutate(Observed = as.factor(Observed))  |&gt;\n  sensitivity(Observed, .pred_class, event_level = \"second\")\n\nspec_y  &lt;- the_predictions  |&gt;\n  mutate(Observed = as.factor(Observed))  |&gt;\n  specificity(Observed, .pred_class, event_level = \"second\")\n\nbind_rows(sens_y, spec_y)  |&gt; \n  gt()\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nsensitivity\nbinary\n0.8\n\n\nspecificity\nbinary\n0.7\n\n\n\n\n\n\n\nNow we’ve restored our original results. 😎\n\n\n\n\n\n\nCaution\n\n\n\nThis ‘switching’ behavior in yardstick is also apparent in the collect_metrics() function, making it essential to check the event level. Failure to do so may result in inadvertently switching the event level.\n\n\n\n\nSummary\nIn this blog post, we’ve walked through the process of computing sensitivity and specificity using the tidymodels package in R, demonstrating it with a simulated dataset. These metrics are indispensable for evaluating the performance of binary classification models. Don’t forget: for the most accurate and realistic results, always evaluate your models using separate test data.\n\n\n\n\n\n Back to topReferences\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. O’Reilly Media, Inc."
  },
  {
    "objectID": "posts/2024-12-28-three-components/three-components.html",
    "href": "posts/2024-12-28-three-components/three-components.html",
    "title": "Three Components of a Successful Prevention Program",
    "section": "",
    "text": "One of the core challenges in prevention science and public health is establishing whether a program causes the outcomes we observe in the field. Are the gains we see (i.e., reduced substance use, improved mental well-being, or lowered dropout rates) directly attributable to the intervention we designed and implemented? Or might they result from unrelated social trends, fluctuating resources, or measurement artifacts?\nTo approach these questions, Randomized Controlled Trials (RCTs) have become the gold standard. In an RCT, we randomly assign participants or sites to treatment and control (or comparison) groups, then compare outcomes. If the treatment group shows a significantly better result than the control group, we often conclude that the program “worked.” But such a conclusion can be deceptively straightforward when programs are, in fact, systems, they typically involve an interplay of Theory of Change (T), Implementation (I), and Evaluation (E)."
  },
  {
    "objectID": "posts/2024-12-28-three-components/three-components.html#programs-as-systems",
    "href": "posts/2024-12-28-three-components/three-components.html#programs-as-systems",
    "title": "Three Components of a Successful Prevention Program",
    "section": "Programs as Systems",
    "text": "Programs as Systems\nConsider a program meant to decrease adolescent vaping. The Theory of Change sets out the blueprint of why specific activities (peer mentoring, parental engagement, etc.) are expected to make a difference. Implementation is the process of putting those activities into action (i.e., hiring and training staff, ensuring session fidelity, adapting schedules to participants’ needs, and more). Finally, Evaluation measures outcomes, seeking evidence that the program actually achieved its stated goals.\nWhen a prevention program is conceptualized as this interconnected system, a typical RCT that simply asks, “Does the program work?” can obscure which part of the system is actually influencing the observed outcomes. This can lead to confusing or contradictory findings, positive results might mask a faulty theory (if staff happened to do something else that helped), and negative or null results might disguise an essentially correct theory that was never implemented well."
  },
  {
    "objectID": "posts/2024-12-28-three-components/three-components.html#why-typical-rct-frameworks-can-be-limiting",
    "href": "posts/2024-12-28-three-components/three-components.html#why-typical-rct-frameworks-can-be-limiting",
    "title": "Three Components of a Successful Prevention Program",
    "section": "Why Typical RCT Frameworks Can Be Limiting",
    "text": "Why Typical RCT Frameworks Can Be Limiting\n\nBlack-Box Design\n\nTraditional RCTs treat the entire program as a black box. If we see a difference in outcomes, we infer that the program caused it. Yet we rarely dissect which portion of the program functioned well or poorly. If results are mediocre, we cannot easily pinpoint whether it was the theory that failed or the implementation.\nThese three components (T, I, E) can have different effects on the outcomes for example:\n\nIf T is flawed (the theory is incorrect or incomplete), the intervention might accidentally help in other ways or might fail entirely.\nIf I is compromised (poor training, inadequate resources, low fidelity), the intervention as delivered differs substantially from the intervention as theorized.\nIf E is misaligned (measuring the wrong outcome or using unreliable methods), results can be misleading, even if the theory and the implementation were sound.\n\n\nConfounded by Implementation\n\nConsider an RCT run across multiple sites. Some sites deliver the program faithfully; others do not. If you average outcomes across all sites, the overall effect could appear modest, even though high-fidelity sites might have shown strong results. Hence, the classical RCT approach can mislead us into dismissing a promising program (or adopting one that only worked under special conditions)."
  },
  {
    "objectID": "posts/2024-12-28-three-components/three-components.html#strengthening-causal-claims-a-systems-view",
    "href": "posts/2024-12-28-three-components/three-components.html#strengthening-causal-claims-a-systems-view",
    "title": "Three Components of a Successful Prevention Program",
    "section": "Strengthening Causal Claims: A Systems View",
    "text": "Strengthening Causal Claims: A Systems View\nBy viewing prevention programs as systems, we can better diagnose why we see certain outcomes:\n\nDisaggregate the Results Break down the program into T, I, and E. Evaluate fidelity, did staff deliver the planned activities (Implementation)? Did the underlying logic hold up in practice (Theory)? Did the measurement tools capture the actual changes (Evaluation)?\nAssess Fidelity and Adaptation A high- or low-fidelity site comparison can reveal how much Implementation quality shapes outcomes. If only high-fidelity sites show significant improvements, the program’s fundamentals might be valid; the challenge is ensuring consistent delivery."
  },
  {
    "objectID": "posts/2024-12-28-three-components/three-components.html#rcts-are-still-usefulbut-we-need-more",
    "href": "posts/2024-12-28-three-components/three-components.html#rcts-are-still-usefulbut-we-need-more",
    "title": "Three Components of a Successful Prevention Program",
    "section": "RCTs Are Still Useful—But We Need More",
    "text": "RCTs Are Still Useful—But We Need More\nThis is not to say that RCTs lack value. Random assignment remains a powerful way to rule out many external factors that might otherwise explain an effect. The point is that an RCT, on its own, cannot fully disentangle why a program succeeded or failed—particularly if we do not measure or account for the intricacies of T, I, and E. The RCT’s rigorous comparison can show whether a difference in outcome exists, but interpreting that difference requires a deeper system-level understanding."
  },
  {
    "objectID": "posts/2024-12-28-three-components/three-components.html#conclusion",
    "href": "posts/2024-12-28-three-components/three-components.html#conclusion",
    "title": "Three Components of a Successful Prevention Program",
    "section": "Conclusion",
    "text": "Conclusion\nDetermining whether an observed effect is genuinely caused by a prevention program involves more than a simple yes/no answer from an RCT. When we treat these interventions as systems—each with a distinct theory, mode of implementation, and evaluation metric—our classical causal framework can become opaque, leading to confusion and misguided conclusions. Flaws in implementation might overshadow a perfectly valid theory; incorrect or narrow evaluation metrics can inflate or hide real changes."
  },
  {
    "objectID": "posts/2022-12-01-ChessOlympiad22/index.html",
    "href": "posts/2022-12-01-ChessOlympiad22/index.html",
    "title": "Can ELO Predict the Outcome of Chess Games?",
    "section": "",
    "text": "Code\nlibrary(ChessOlympiad22)\nlibrary(rpart.plot)\nlibrary(tidymodels)\n\n\nWarning: package 'scales' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'recipes' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'yardstick' was built under R version 4.2.3\n\n\nThe ELO rating system is a method used to calculate the relative skill levels of chess players, based on their game results. A higher ELO rating indicates a higher perceived skill level.\nIn this document, the ELO difference between two players is used to evaluate the probability of one player winning the match.\nI will use the The ChessOlympiad package. It contains two datasets: players and results. The players dataset contains the ELO ratings of all players participating in the 44th Chess Olympiad, which took place in Chennai, India, in 2022. The results dataset contains the results of all matches played in the tournament.\n\nDistribution of ELO\n\n\nCode\ndata(\"players\")\n\n\nFirst, let’s visualize the ELO distribution of the players.\nIt is important to filter out players with an ELO rating of zero from the analysis, as these players have not yet been rated and do not have a known skill level.\n\n\nCode\nplayers |&gt; \n  filter(rtg!=0) |&gt; \n  ggplot(aes(rtg)) + \n  geom_histogram(bins=90,fill=\"gray90\", alpha=0.85, color=\"gray95\") +\n  scale_x_continuous(breaks = seq(1000,2900,150))+\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        panel.grid = element_blank()) +\n  labs(x=\"Elo Rating System (Higher values indicate greater player strength)\",\n       y=\"\",\n       title = \"Elo Rating System of players in Chess Olympiad, 2022. Chennai, India \",\n       caption = \"Magnus Carlsen has the highest Elo rating in the tournament: 2864\") +\n  geom_text(data = players %&gt;% \n    filter(rtg &gt; 2800),\n            aes(x = 2550, \n            y = 4.5, \n            label = \"Magnus Carlsen\"),\n            size = 3,\n            color = \"black\") +\n  geom_curve(data = players %&gt;% filter(rtg &gt; 2800),\n               aes(x = 2700, \n               y = 4.4, xend = 2864.8, \n               yend = 1),\n               arrow = arrow(length = unit(0.3, \"cm\")),\n               color = \"red\",\n               linewidth = 0.5,\n               curvature = -0.5)\n\n\n\n\n\n\n\n\n\nIt may be interesting to plot the ELO ratings of players by federation, as the Chess Olympiad is played by national teams. By examining the ELO ratings of players within each federation, we can get a sense of the overall strength of the teams participating in the event. This analysis could potentially provide insight into the results of the Chess Olympiad and help predict the outcomes of matches.\n\n\nCode\nplayers |&gt; \n  filter(rtg&gt;2600) |&gt; \n  ggplot(aes(reorder(fed, rtg),rtg)) + \n  theme_minimal() +\n  geom_boxplot() +\n  coord_flip() +\n  theme(panel.grid = element_blank()) +\n  labs(x=\"\", y=\"ELO\")\n\n\n\n\n\n\n\n\n\nAccording to ELO ratings, the United States fielded the strongest team in the tournament.\n\n\nCode\ndata(\"results\")\n\n\n\n\nDifferences in ELO by round\nThe Chess Olympiad followed a Swiss-style tournament, meaning that players are paired with opponents with similar scores in each round. Specifically, in the first round, the highest-ranked player is matched against the median-ranked player, followed by the second-highest ranked player against the next below median, and so forth.\nA visual representation of the differences in ELO by round are presented in the following graph.\n\n\nCode\nresults |&gt; \n  filter(elo_difference&gt;=-1000, \n         elo_difference&lt;=1000, \n         !is.na(elo_white), !is.na(elo_black), elo_white!=0,elo_black!=0) |&gt; \n  ggplot(aes(as.numeric(elo_white),elo_difference, \n             fill=factor(result_white), \n             color=factor(result_white))) + \n  geom_point(shape = 21, alpha=0.85,\n             size = 3, stroke = 0.5) +\n  theme_minimal() +\n  scale_fill_manual(values=c(\"Lost\"=\"black\", \"Draw\"=\"gray50\",\"Won\"=\"white\")) +\n  scale_color_manual(values=c(\"Lost\"=\"black\", \"Draw\"=\"black\",\"Won\"=\"black\")) +\n  labs(fill=\"Result\", color=\"Result\", \n       x=\"Player Elo\", \n       y=\"Elo difference\",\n       caption = \"Difference greater than zero indicates stronger player\n       44th Chess Olympiad. Chennai, 2022 Open\") +\n  facet_wrap(~ round)\n\n\n\n\n\n\n\n\n\n\n\nModel the winning chances for players with the white pieces based on ELO difference\nNow, let’s try to identify the optimal divisions in ELO rating differences that could potentially classify the outcomes of chess games. For this, I will be using the tidymodels package to estimate a Classification and Regression Trees (CART) model.\n\n\nCode\ncart_spec &lt;-\n   decision_tree() |&gt; \n   set_engine(\"rpart\") |&gt;\n   set_mode(\"classification\")\n\n\nI will add two steps in the recipe. One to filter the data set by round, and the other to convert results in a factor variable. I also will limit my analysis to players with more than 1600 in ELO.\n\n\nCode\nresults &lt;- results |&gt; \n  filter(as.numeric(elo_white)&gt;1600) |&gt; \n  filter(as.numeric(elo_black)&gt;1600) \n\nrecipe &lt;- recipe(\n  result_white ~ elo_difference + round_number, data = results) |&gt; \n    step_filter(round_number==round)\n\n\n\n\nCode\nwrkfl &lt;- workflow() |&gt; \n  add_model(cart_spec) |&gt; \n  add_recipe(recipe)\n\n\nLet’s estimate the model for the round 1.\n\n\nCode\nround &lt;- 1\n\ncart_fit &lt;- wrkfl |&gt; \n  fit(data=results) |&gt; \n  extract_fit_parsnip()\n\n\n\nDraw an tree to understand the results\nFinally, I will create a tree showing the splits\n\n\nCode\ncart_fit &lt;- repair_call(cart_fit, data = round)\n\ncart_tree_fit &lt;- cart_fit$fit\n\nrpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)\n\n\n\n\n\n\n\n\n\nAccording to the model, a difference of 12 in the ELO is sufficient to accurately predict the winner in 95% of cases and the loser in 91% of cases.\n\n\n\nModel the last round\nNow, let’s apply the model to the final round, which featured matches between the most formidable opponents.\nI will add a the tree_depth parameter to my model. The depth of the tree refers to the number of levels the tree has.\n\n\nCode\ncart_spec &lt;-\n   decision_tree(tree_depth = 4) |&gt; \n   set_engine(\"rpart\") |&gt;\n   set_mode(\"classification\")\n\nwrkfl &lt;- workflow() |&gt; \n  add_model(cart_spec) |&gt; \n  add_recipe(recipe)\n\n\n\n\nCode\nround &lt;- 11\n\ncart_fit &lt;- wrkfl |&gt; \n  fit(data=results)\n\ncart_fit &lt;- wrkfl |&gt; \n  fit(data=results) |&gt; \n  extract_fit_parsnip()\n\ncart_fit &lt;- repair_call(cart_fit, data = round)\n\ncart_tree_fit &lt;- cart_fit$fit\n\nrpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nCode\nrpart.rules(cart_tree_fit, cover = TRUE)\n\n\n  ..y  Dra Los Won                                       cover\n Draw [.45 .30 .25] when elo_difference is -176 to  85     51%\n Draw [.55 .09 .36] when elo_difference &gt;=         333      4%\n Draw [.69 .15 .15] when elo_difference is   95 to 115      4%\n Lost [.29 .61 .10] when elo_difference &lt;  -176            20%\n  Won [.10 .30 .60] when elo_difference is   85 to  95      3%\n  Won [.17 .20 .63] when elo_difference is  115 to 333     18%\n\n\nThe color-coding of the decision tree leaves suggests that differences in ELO ratings remain a critical factor, even in the final round of the chess tournament. It seems that the model is particularly adept at predicting outcomes when the ELO differences are substantial. For example, if you have more than 333 points in ELO, the model predicts 63% wining chances for you. However, if the ELO difference is less than 333 (but more than 115), the model predicts 55% of draw.\nWhen the ELO difference is less than 115, the model’s predictions become more interesting. If a player has 176 points less than their opponent, the model is more likely to classify them as a loser (61%). However, if the ELO difference is less than 85 points, the player still has a good chance of winning the game. This could be seen as an indicator of the performance of some players with lower ELO ratings who are having a strong tournament. On the other hand, if the ELO difference is greater than 85 points, most of the chances are for a draw.\nThis serves as a quick demonstration of how the ChessOlympiad package can be utilized in predictive modeling.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2022-11-27-trustworthy/trustworthy.html",
    "href": "posts/2022-11-27-trustworthy/trustworthy.html",
    "title": "Trustworthy",
    "section": "",
    "text": "Francisco Cardozo, Pablo Montero-Zamora\nConfidence in students’ survey responses is commonly questioned when performing drug use research. For example, one’s can wonder how do you know students are telling the truth about their alcohol consumption? We recognize this as a legitimate question that can be even more complex when considering two types of students: 1) those who say they have not used alcohol but used it (i.e., deniers), and 2) those who say they have used it but never use it (i.e., braggers). It is crucial to understand how this reporting bias affects the validity of our measurements and research findings. Therefore, we propose the following app to know how the proportions of deniers, braggers, and drug use prevalence can influence confidence levels in self-reported measures collected in adolescents.\n\nTrue prevalence: number of students using a drug divided by the population.\n\nDeniers: students who say they have not used a drug but used it.\nBraggers: students who say they have used a drug but never used it.\n\nGiven the information above, we can estimate the probability of a student’s drug use behavior if they respond Yes or No to a drug use question.\nTo model this probability, we can use the Bayes theorem:\n\nP(A|B) = Drug use given they say yes in the questionnaire.\nP(B|A) = Say yes given that they have used drugs.\nP(A) = Drug use prevalence.\nP(B) = Say yes in the questionnaire."
  },
  {
    "objectID": "posts/2023-08-01-while/while.html",
    "href": "posts/2023-08-01-while/while.html",
    "title": "The Chessboard and the Wise Courtier",
    "section": "",
    "text": "The Story\nOnce upon a time in ancient India, a wise courtier presented a challenge to his king using a chessboard and some grains of rice. He placed a single grain of rice on the first square of the chessboard, then asked the king to double the number of grains on each subsequent square until all 64 squares were filled. Initially, the king thought that the result would be a meaningless amount of rice, but he soon realized the astronomical sum this would amount to.\n\n\n\n\n\n“Generated with Midjourney”\n\n\n\n\n\nThis story illustrates the power of exponential growth, a mathematical concept that can describe certain patterns in nature and society. Exponential growth occurs when the rate of growth is proportional to the current value, leading to increasingly rapid growth over time. This type of growth can be found in many real-world phenomena, including the spread of disease, and the adoption of new technologies.\n\n\n\n\n\n\n\n\nNote\n\n\n\nPandemics: In the early stages of a pandemic, such as COVID-19, the number of infections may double within a fixed time period, leading to an exponential increase.\nSocial Media and Viral Content: Posts or videos that “go viral” on social media platforms can exhibit exponential growth. A piece of content might be shared by a few people initially, but if each person who sees it shares it with others, the total number of views can grow exponentially.\nEnvironmental Concerns: Exponential growth can also have negative consequences, such as in the spread of invasive species or the consumption of non-renewable resources, leading to potential environmental damage.\n\n\nI will use the story of chess to demonstrate how quickly the number of grains grows when it doubles over the course of only 64 steps. I will also provide Python code to calculate the total number of grains on the chessboard.\n\n\nPython Code to Compute the Total Number of Grains\nImagine you have a bag of grains, and you want to fill a chessboard with them. You start with one grain on the first square, then double the number of grains on each subsequent square until you reach the 64th square. You can think of this process like a snowball rolling down a hill, getting bigger and bigger as it goes.\nIn Python, we can use something called a while loop to replicate this process. It’s like having a robot that puts the grains on the squares for you, following your exact instructions: “Start with one grain, then double the number, and keep going until you fill all 64 squares.”\n\n\n\n\n\n\nWhile Loop\n\n\n\nA while loop continues to execute the block of code as long as a specified condition is true. It checks the condition before each iteration, and if the condition is false, it exits the loop.\n\n\nNow, let’s think about a different way to approach the task using what’s called a for loop. Imagine instead that you have 64 small containers, each representing a square on the chessboard, and you know exactly how many grains should go in each one. Starting with the first container, you fill it with one grain, then move to the next one and fill it with double the grains of the previous container, and so on, until you reach the 64th container.\n\n\n\n\n\n\nFor Loop\n\n\n\nA for loop works like this staircase-building process. You tell the computer exactly how many times to repeat something, and it follows your instructions step by step. Unlike the snowball rolling down the hill, which might take an unpredictable path, building the staircase (or using a “for loop”) is more controlled and precise.\n\n\n\n\nWhile Loop in python\nLets to compute that number of grains using the while loop.\n\n# Initialize variables\nsquare_number = 1  # The number of the current square\ngrains_on_square = 1  # The number of grains on the current square\ntotal_grains = 0  # The total number of grains so far\n\n# Loop through each square on the chessboard\nwhile square_number &lt;= 64:\n    # Add the number of grains on the current square to the total\n    total_grains += grains_on_square\n    # Double the number of grains on the current square for the next square\n    grains_on_square *= 2\n    # Move to the next square\n    square_number += 1\n\nprint(f\"The total number of grains on the chessboard is: \\n {total_grains:,}\")\n\nThe total number of grains on the chessboard is: \n 18,446,744,073,709,551,615\n\n\nSo, it’s a big number. Here are some interesting facts about this number:\n\n\n\n\n\n\nNote\n\n\n\n\nThis number in words is eighteen quintillion, four hundred forty-six quadrillion, seven hundred forty-four trillion, seventy-three billion, seven hundred nine million, five hundred fifty-one thousand, six hundred fifteen, or approximately 18 quintillion, or 18 billion billion.\nIn seconds, it is equal to 293,274,701,009 years, 3 weeks, 3 days, 15 hours, 30 minutes, 7 seconds.\nIf each grain of rice were 0.05 grams in weight, then the total weight of rice would be 922337203685.48 tons.\n\n\n\n\n# Total number of grains as computed earlier\ntotal_grains = 18446744073709551615\n\n# Weight of a single grain in grams\nweight_per_grain = 0.05\n\n# Compute the total weight in grams\ntotal_weight_grams = total_grains * weight_per_grain\n\n# Convert to kilograms\ntotal_weight_kilograms = total_weight_grams / 1000\n\n# Convert to metric tons\ntotal_weight_metric_tons = total_weight_kilograms / 1000\n\nprint(f\"The total weight of the rice in grams is {total_weight_grams:.2f} grams\")\nprint(f\"The total weight of the rice in kilograms is {total_weight_kilograms:.2f} kilograms\")\nprint(f\"The total weight of the rice in metric tons is {total_weight_metric_tons:.2f} metric tons\")\n\nThe total weight of the rice in grams is 922337203685477632.00 grams\nThe total weight of the rice in kilograms is 922337203685477.62 kilograms\nThe total weight of the rice in metric tons is 922337203685.48 metric tons\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nFind more facts here:\n“18446744073709551615 - Facts”\n\n\nLet’s finish this post by comparing the while loop with the for loop.\n\n\n\n\n\n\n\n\nAspect\nWhile Loop\nFor Loop\n\n\n\n\nCondition vs Sequence\nUses a condition that can be any logical expression, continues iterating as long as the condition is true.\nIterates over a sequence of values, executing the code block once for each value.\n\n\nControl Over Iteration\nYou have full control over how many times the loop iterates by manipulating the condition.\nThe number of iterations is defined by the length of the sequence.\n\n\nPotential for Infinite Loop\nThere’s a risk of creating an infinite loop if the condition never becomes false.\nGenerally not at risk for infinite loops, as it iterates over a finite sequence.\n\n\nUse Cases\nOften used when you don’t know how many times you’ll need to iterate.\nUsed when you want to iterate a known number of times or over a specific sequence.\n\n\n\n\nConclusion\nThe chessboard problem illustrates how quickly numbers can grow when they double with each step. By using a while loop in Python, we’ve computed this enormous figure, providing insight into the power of exponential growth.\nThis example shows how Python can be used to solve problems that would be otherwise challenging to compute manually. Feel free to explore the code further, perhaps by changing the number of squares or the initial number of grains. What other intriguing patterns might you discover?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-05-10-recipes/recipes.html",
    "href": "posts/2023-05-10-recipes/recipes.html",
    "title": "An Introduction to the Recipes Package for Data Preprocessing",
    "section": "",
    "text": "I liked this presentation done by Max Kuhn.\n\n\nHere are some notes:\nThe recipes package provides a framework for preprocessing data prior to modeling or visualization. With its pipeable sequence of steps and syntax similar to dplyr, the package simplifies a wide range of preprocessing tasks, from data normalization and missing data imputation, to categorical variable encoding and data transformation.\nIt’s important to remember when using the recipes package, the type of model you’re fitting can determine the necessary preprocessing steps for your data.\nIn addition to model-driven preprocessing steps, the recipes package also provides functions for feature engineering. This involves representing your data in ways most effective for your particular problem. For instance, you might create interaction terms, polynomial terms, or spline terms to capture non-linear relationships between predictors and the outcome.\nHere are some useful preprocessing steps:\n\nData normalization: The step_normalize() function normalizes your data by centering and scaling the variables. This is useful when working with models that require predictors to be on the same scale, such as k-nearest neighbors or neural networks.\nMissing data imputation: The step_impute_*() functions impute missing data using various methods, like mean imputation, median imputation, or k-nearest neighbors imputation.\nCategorical variable encoding: The step_dummy() function creates dummy variables for categorical predictors. This is handy when working with models that can’t handle categorical predictors directly, like linear regression or logistic regression.\nData transformation: The step_*() functions transform your data in various ways, such as applying the logarithm, square root, or Box-Cox transformation to a variable. This is useful when working with data that isn’t normally distributed or when trying to improve the linearity of the relationship between predictors and the outcome.\nFeature engineering: The step_*() functions are also used for feature engineering, such as creating interaction terms, polynomial terms, or spline terms. This is beneficial when trying to capture non-linear relationships between predictors and the outcome.\n\nLink to the package documentation\nThings to think:\nA point of confusion might be whether preprocessing is considered part of data cleaning or data transformation for modeling. It appears that there’s an overlap between data cleaning and data transformation, and it can sometimes be difficult to distinguish between these stages. It would be helpful to clarify the difference between these concepts and data preprocessing.\nWhen I try to imagine where recipes fits into these models, it’s not completely clear to me.\n\n\n\nThe data science process. From R for Data Science\n\n\n\n\n\nModeling Process. From Tidymodels book\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2025-03-19-sci-fi/predict-efficacy.html",
    "href": "posts/2025-03-19-sci-fi/predict-efficacy.html",
    "title": "Welcome to prevention science meets sci-fi.",
    "section": "",
    "text": "AI is knocking at the door of prevention science, carrying predictions instead of theories and probabilities instead of certainties. It’s exciting, slightly unsettling, and entirely unavoidable-welcome to efficacy’s existential crisis. ChatGPT 4.0\n\nPrevention science has established Standards of Evidence to evaluate intervention efficacy, effectiveness, and readiness for scale-up. These standards originated with Flay et al. (2005), who set criteria defining when an intervention can be considered “tested and efficacious” or “effective,” primarily focusing on rigorous randomized controlled trials (RCTs) and replication. A decade later, the Society for Prevention Research (SPR) updated these standards in their influential paper, “Standards of Evidence for Efficacy, Effectiveness, and Scale-up Research in Prevention Science: Next Generation” (Gottfredson et al. 2015). The updated standards expanded guidance to include replication, scale-up, theoretical grounding, comprehensive intervention descriptions, implementation quality, adaptation documentation, and outcome reporting. These standards underscore that preventive programs should demonstrate impact both under ideal, controlled conditions (efficacy) and in real-world settings (effectiveness), complete with clear theoretical rationale and fidelity monitoring.\nNow, enter AI-based predictive models into this neat picture, and suddenly the tidy standards start feeling like they’re missing something. Traditional efficacy standards assume theories, RCTs, systematic replication, and smooth scale-ups. Predictive analytics (based on AI), however, uses messy observational big data or continuously adapting algorithms—methods that don’t exactly play by the traditional RCT rulebook. This naturally begs the question: how do AI-driven methods fit into the world of SPR Standards?\nBefore venturing into deep waters, it’s worth noting that the Standards of Evidence frame efficacy not simply as a little badge a program wears, but as a function of several moving parts: the program itself, how it’s implemented, the population receiving it, and even the timing and setting (an efficacy statement might look like: producing outcome Y for population Z at time T in setting S). It’s tempting—and frankly easier—to just slap on the label “efficacious” or “not efficacious” as if it’s a permanent tattoo, independent of the real-world details.\nTo not make this post too long, because there is too much to say, for example, how AI can be used to “predict efficacy”, let’s mention now two tensions between AI and traditional standards of efficacy.\nFirst, SPR’s beloved standards demand theory and clear causal mechanisms. They don’t just want to see if something works, they also insist on understanding why it works. AI, in contrast, often hands us remarkably accurate predictions while keeping the secrets of how it got there (the notorious “black box” scenario). This leaves prevention scientists scratching their heads: should we trust an algorithm predicting substance-use initiation if it stubbornly refuses to explain itself? Ironically, if we say “yes,” we might need to rewrite prevention science’s entire history—one that proudly resisted programs that mysteriously “just worked.”\nSecond, traditional standards keep intervention and evaluation separated, like good neighbors with clear fences. But AI casually strolls in and knocks these fences down. Imagine interventions that aren’t just evaluated by AI but continuously reshaped and adapted by it in real-time. Soon, we could have preventive programs with no two identical implementations—possibly no fixed interventions at all. Heck, why stop there? Maybe we’ll even have programs without human participants! Welcome to prevention science meets sci-fi.\nThis suggests it might be time to update our Standards of Evidence, think of it as “Evidence 2.0”, if you’re feeling trendy. Sure, rigorous evaluations, replication, and real-world effectiveness remain essential. But in this brave new AI-driven world, we also need fresh guidelines. New standards might include performance benchmarks and regular bias audits for predictive models, transparency mandates (open-source algorithms, anyone?), and perhaps greater acceptance of robust quasi-experimental or even simulation-based evidence. The good ol’ RCT might not reign supreme forever (cue dramatic gasp).\nIn short, prevention science is now in the exciting and delightfully chaotic territories of balancing classic rigor with AI’s transformative unpredictability, like a sci-fi movie where the hero is a bit too unpredictable for their own good.\n\n\n\n\n Back to topReferences\n\nFlay, Brian R., Anthony Biglan, Robert F. Boruch, Felipe González Castro, Denise Gottfredson, Sheppard Kellam, Eve K. Mościcki, Steven Schinke, Jeffrey C. Valentine, and Peter Ji. 2005. “Standards of Evidence: Criteria for Efficacy, Effectiveness and Dissemination.” Prevention Science 6 (3): 151–75. https://doi.org/10.1007/s11121-005-5553-y.\n\n\nGottfredson, Denise C., Thomas D. Cook, Frances E. M. Gardner, Deborah Gorman-Smith, George W. Howe, Irwin N. Sandler, and Kathryn M. Zafft. 2015. “Standards of Evidence for Efficacy, Effectiveness, and Scale-up Research in Prevention Science: Next Generation.” Prevention Science 16 (7): 893–926. https://doi.org/10.1007/s11121-015-0555-x."
  },
  {
    "objectID": "posts/2023-12-03-most/most.html",
    "href": "posts/2023-12-03-most/most.html",
    "title": "Balancing Efficiency and Flexibility: The Challenges of Over-Optimization in the Multiphase Optimization Strategy (MOST)",
    "section": "",
    "text": "The Multiphase Optimization Strategy (MOST) is an approach to intervention development in prevention science. MOST is structured around three phases: the Screening Phase, where intervention components are initially selected or eliminated based on efficacy; the Refining Phase, which is dedicated to ‘calibrating’ these elements to determine their optimal levels and combinations; and the Confirming Phase, during which the refined intervention is subjected to rigorous evaluation via a conventional randomized controlled trial. Although this strategy is designed to optimize intervention development, it faces a significant challenge: the possibility that excessive optimization could be counterproductive.\nAlthough it may seem counterintuitive, pursuing efficiency can sometimes lead to inferior outcomes. This paradox is named Goodhart’s Law, which suggests that once a measure becomes a target, it can end up distorting the very outcome it was intended to assess. This happens when individuals or groups start to ‘game the system’ to meet these targets, thereby neglecting the real improvements these measures are designed to track.\nRegarding MOST, the issue does not always stem from intentional misbehavior or manipulation; rather, it arises from an overemphasis on efficiency that can result in an excessive focus on optimization, which may neglect components that benefit achieving the outcomes of the intervention. To illustrate how an excessive focus on optimization can lead to worse outcomes, consider students who are overly prepared to excel at standardized tests, which may cause them to neglect to develop a wider range of skills that are crucial for overall life success. Similarly, overly incentivizing researchers with bonuses can encourage fraudulent activities and undermine the integrity of scientific research.\nTherefore, within MOST, over-selecting components may lead to overfitting in interventions, which is counterproductive given the need for interventions suitable for diverse populations. Moreover, there is a risk that prioritizing intervention components based solely on their individual ‘effectiveness’ can overlook the principle that the total impact may not always equal the sum of its parts. These two aspects represent fundamental challenges inherent in the MOST methodology.\nTo address this challenge, intervention developers must take into consideration an appropriate balance between the efficiency of components and the variability of their effects under diverse conditions. In addressing this, it is crucial to acknowledge that samples are less diverse than populations, that other factors such as implementation components also contribute to program effectiveness, and, overall, that interventions may not be universally beneficial across all population groups.\nIn conclusion, while MOST is a promising framework for intervention development, it is vital to recognize that it is not perfect. The applicability of the MOST methodology is not universal across all interventions, and the advantages of efficiency must be carefully weighed against the risks of overfitting.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html",
    "href": "posts/2024-04-06-getref/getref.html",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "",
    "text": "Dealing with references can feel like a real headache. However, recently I came across a fantastic solution by Richard Mclearn. This blog post is all about making your life easier with a simple tutorial for Mac users. I’ll show you how to set up a getref function that quickly grabs bibliographic information in BibTeX format from a DOI or a DOI URL."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-1-prepare-your-workspace",
    "href": "posts/2024-04-06-getref/getref.html#step-1-prepare-your-workspace",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 1: Prepare Your Workspace",
    "text": "Step 1: Prepare Your Workspace\nFirst, we need to create a dedicated folder to store your getref function. This not only helps in organizing your scripts but also in managing your PATH environment efficiently.\nYou can create the folder following this steps:\n1.1. Open the terminal (cmd+space then type terminal).\n1.2. Create a new folder named bin in your home directory:\ntype\n\nmkdir ~/bin\n\nand press return in your keyboard.\n\n\n\n\n\n\nNote\n\n\n\nI created the bin folder in my home directory. It will make it easier to access the script later on."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-2-move-to-the-bin-folder",
    "href": "posts/2024-04-06-getref/getref.html#step-2-move-to-the-bin-folder",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 2: Move to the bin Folder",
    "text": "Step 2: Move to the bin Folder\nNavigate to the newly created folder by typing\n\ncd ~/bin"
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-3-create-the-script-file",
    "href": "posts/2024-04-06-getref/getref.html#step-3-create-the-script-file",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 3: Create the Script File",
    "text": "Step 3: Create the Script File\nCreate a new file named getref by typing\n\ntouch getref\n\nThis file will be our script."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-4-add-the-script-code",
    "href": "posts/2024-04-06-getref/getref.html#step-4-add-the-script-code",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 4: Add the Script Code",
    "text": "Step 4: Add the Script Code\nOpen the getref file in your favorite text editor. For simplicity, you can use Nano.\n\nnano getref\n\nIn the inside of the file, copy and paste the following code snippet. This script checks if a DOI or DOI URL is provided as an argument and then fetches the bibliographic information in BibTeX format.\n\n#!/bin/bash\n# Check if an argument is provided\nif [ \"$#\" -ne 1 ]; then\n    echo \"Usage: $0 &lt;DOI or DOI URL&gt;\"\n    exit 1\nfi\n\n# Determine if the input is a DOI URL or just a DOI number\nif [[ \"$1\" =~ ^https:// ]]; then\n    # Extract the DOI number from the URL\n    DOI=$(echo \"$1\" | sed 's|https://doi.org/||')\nelse\n    # Assume the input is just a DOI number\n    DOI=\"$1\"\nfi\n\n# Extract and print the reference\ncurl -LH \"Accept: application/x-bibtex\" \"https://doi.org/$DOI\""
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-5-make-the-script-executable",
    "href": "posts/2024-04-06-getref/getref.html#step-5-make-the-script-executable",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 5: Make the Script Executable",
    "text": "Step 5: Make the Script Executable\nEnsure the script can be executed by running in the terminal\n\nchmod +x ~/bin/getref\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that I used the path to my file, which is located in the bin folder in my home directory."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-6-update-your-shell-configuration",
    "href": "posts/2024-04-06-getref/getref.html#step-6-update-your-shell-configuration",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 6: Update Your Shell Configuration",
    "text": "Step 6: Update Your Shell Configuration\nNow, Open your shell configuration file (.zshrc for Zsh or .bashrc for Bash) using a text editor, e.g.,\n\nnano ~/.zshrc\n\nYou will need to add ~/bin to your PATH. To do this, add the following line at the end of the file:\n\nexport PATH=\"$HOME/bin:$PATH\"\n\nSave and close the file."
  },
  {
    "objectID": "posts/2024-04-06-getref/getref.html#step-7-apply-the-changes",
    "href": "posts/2024-04-06-getref/getref.html#step-7-apply-the-changes",
    "title": "How to Create a Bash Script to Extract Bibliographic References from DOIs",
    "section": "Step 7: Apply the Changes",
    "text": "Step 7: Apply the Changes\nApply the changes to your current session by sourcing your configuration file:\n\nsource ~/.zshrc\n\nfor Zsh\nor\n\nsource ~/.bashrc\n\nfor Bash."
  },
  {
    "objectID": "posts/2024-01-27-explain/explain.html",
    "href": "posts/2024-01-27-explain/explain.html",
    "title": "To Explain or to Predict",
    "section": "",
    "text": "There is a current debate regarding the appropriate circumstances to employ machine learning (ML) over traditional statistics for statistical modeling. Some traditionalists assert that ML often represents nothing more than a rebrand version of existing methods, occasionally rendering it unnecessary. Their primary criticism is that while ML can be advantageous in some circumstances, it frequently operates as a “black box”, offering limited transparency and scant insights into the underlying phenomena. On the other hand, ML advocates argue that traditional statistics often fail to grasp the intricate complexities of real-world data, positioning ML as the superior option. Both arguments, however, might be missing the essence of the issue. This debate is not merely about the characteristics of the tools used but more about the foundational objectives of scientific research.\nScientific research predominantly deals with questions to respond ‘why’ and ‘when’ something occurs. When scientists seek explanations, they are interested in the primary causes or influencers of observed events. Predictive endeavors, conversely, are inherently forward-looking, valuing future forecasts over causal understanding. While no single statistical method can directly answer causal questions, prediction can be approached using both traditional regression techniques and ML models. Thus, the real dilemma emerges in choosing methods for predictive situations, as tools for causal explanations are beyond mere statistical techniques.\nWhen predictive modeling is chosen, machine learning often proves more advantageous than traditional statistical models. These advantages stem from distinct characteristics inherent in the construction of ML models. Primarily, this is because the construction of machine learning models is guided by out-of-sample metrics, tailored to maximize the model’s ability to generalize to unseen data. In essence, machine learning models seek a balance between variance and bias. Conversely, traditional statistical methods focus on hypothesis testing to draw conclusions from models, often overlooking concerns such as overfitting. This oversight includes practices like using the same dataset for both training and testing, and a lack of emphasis on metrics such as the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), or adjusted R², which are crucial for evaluating model complexity. It should be mentioned that this traditional approach is not inherently wrong, but it is not the most appropriate for predictive modeling. Instead, these methods are excellent tools for descriptive analysis and most research in public health is descriptive (unfortunately?).\nWhile machine learning models offer advantages, they also present several challenges. They frequently require extensive computational resources and, in numerous cases, cannot offer insights about underlying phenomena. Such shortcomings can be significant limitations in various scientific contexts. In these scenarios, traditional statistics often emerge as the preferred approach, due to their ability to summarize data relationships more straightforwardly.\nIn prevention science, differentiating between prediction and explanation is crucial. However, this distinction is often overlooked, resulting in innovative missteps. In the absence of clear guidelines, practitioners may opt for inappropriate tools, leading to confusion between predictive outcomes and causal relationships. For example, when assessing interventions, programs are typically evaluated with a focus on their theoretical foundations and the program’s underlying mechanisms, necessitating explanatory modeling. Yet, in practice, the models often tend to be predictive (or descriptive), thereby overlooking the methods essential for explanatory analysis.\nIn summary, the distinction between explanation and prediction is not merely about the methods or algorithms employed but centers on the overarching purpose of the research. While both are fundamental to scientific research, their practical applications and tools, especially in prevention science, can differ significantly.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html",
    "href": "posts/2023-01-10-posit-education/Posit.html",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "",
    "text": "I recently watched a presentation by Garrett Grolemund, the Director of Learning at Rstudio, on the POSIT Enterprise Community Meetup on YouTube. I wanted to share with you some of the interesting thoughts about teaching R that he mentioned."
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html#context",
    "href": "posts/2023-01-10-posit-education/Posit.html#context",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "Context",
    "text": "Context\n\nGarret believes that the success of the training ultimately depends on how it’s delivered.\nTeachers out there really aren’t that good and online courses are not so great.\nPeople who become experts in R by years of practice. However, they may not have had the opportunity to devote those years to being a good teacher.\nThere is theories in psychology(such as cognitive load theory and multimedia learning theory) that can be applied to make training more successful.\nResearch shows that students only retain about 56% of the content from a lecture\nThe retention rate decreases over time.\nStudies on procedural instruction show similar results, with only 60% of students able to reproduce the procedure immediately after training.\nSix months later, the number drops to 40%, and a year later it drops to 30%\nTraining outcomes are not always great and this is a “dirty secret” of the training industry.\nTrainers are not always trained to be trainers, which is a challenge in the field.\nEducation is not simply about transferring information from an educator to a student, and the student then becoming an expert."
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html#insights",
    "href": "posts/2023-01-10-posit-education/Posit.html#insights",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "Insights",
    "text": "Insights\n\nPractice\n\nThe more the task is practiced, the more the brain will conserve the neural networks and retain the ability to perform the task. This is not likely to occur in a workshop that lasts only half a day or two days.\nTo build a robust neural network, it is important to sleep.\nThe ability to perform a new skill in six months from now dependent on the amount of practice they receive after initial instruction, rather than the instruction itself.\n\n\n“Well, the revelation we had at our studio is that data science is a skill, and if you want to learn to do good data science with code or otherwise, you have to practice it and learn it as a skill.”\n\n\n\nMentors, Mates and Accountability\n\nAs a student, it is possible to practice a skill incorrectly without even being aware of it. To practice effectively and efficiently, it is crucial to receive feedback and guidance to make sure the proper techniques are being utilized.\nIndividuals do not acquire knowledge in a vacuum, they are driven by social influences and the identity they construct by participating in a community that is studying data science.\nThe lacking factor in online courses is motivation, which can be obtained through interaction with a mentor or peers. Talking to them can provide the necessary inspiration.\nHaving a mentor or being part of a group provides accountability, as one is expected to show up with something to demonstrate to them.\n\n\n“So, as you go through the course, not only do you have an expert who has your back who’s coaching you, you also have fellow travelers who you could discuss things with. You could work through problems together and, you might not even realize it, but you can hold each other accountable and motivated as you go through the process.”\n\n\n\nRecomendations\n\nHow Learning Happens by Paul Kirschner and Carl. link to amazon. But also consult with your librarian friend\nVisit Posit Academy. You can learn in detail about the POSIT Academy model in minute 24."
  },
  {
    "objectID": "posts/2024-07-06-shiny-trust/Trust-live.html",
    "href": "posts/2024-07-06-shiny-trust/Trust-live.html",
    "title": "How much do we trust a student’s answer about drug use?",
    "section": "",
    "text": "Confidence in students’ survey responses is often questioned when researching drug use, as it can be difficult to determine whether students are being truthful about their alcohol and other drug consumption. This concern is valid because self-reported drug use data can be influenced by various reporting biases. For instance, some students may underreport their drug use (i.e., deniers), while others may overreport (i.e., braggers). It is crucial to understand how these biases can impact the validity of research findings and to take steps to minimize their influence.\nTo examine this issue, we created a simulation-based app. This app can analyze the potential effects of deniers and braggers at different prevalence levels. It aims to improve understanding of how these biases affect research validity and to develop strategies to minimize their influence. By simulating different scenarios and analyzing the results, researchers can better understand the impact of deniers and braggers on research findings and take appropriate steps to mitigate these biases.\nLet’s define some key concepts:\n\nTrue prevalence: The true number of students using a drug divided by the total population.\n\nDeniers: Students who lie about not using drugs when they actually do.\nBraggers: Students who lie about using drugs when they actually don’t.\n\nWe can use a student’s response to a drug use question to estimate the probability of their actual drug use behavior given hypothetical values of deniers, braggers, and true prevalence.\nTo model this probability, we can use Bayes’ theorem:\n\\[\nP(A | B) = P(A) \\frac{P(B | A)}{P(B)}\n\\]\nBy replacing:\n\n(P(A|B)): The probability of drug use, given that the student says “yes” in the questionnaire.\n(P(B|A)): The probability of saying “yes” in the questionnaire, given that the student has used drugs.\n(P(A)): The probability of drug use (drug use prevalence).\n(P(B)): The probability of saying “yes” in the questionnaire.\n\nWe present simulated scenarios to understand how the trustworthiness of student responses changes based on the proportions of braggers, deniers, and the prevalence of alcohol use.\n\n\n\n\n\n\nNote\n\n\n\nDefinition of trust:\n\\[\n\\text{trust} = P(\\text{Actual Drug Use} | \\text{Reports Drug Use})\n\\] The definition of trust we use is the conditional probability that a student who reports drug use in the survey is actually telling the truth, and it represents how much confidence we can have in the validity of a yes response from the students regarding their drug use.\n\n\n\nThree Possible ScenariosSimulation PlaygroundExample Scenario\n\n\n\nThree Possible Scenarios\nWe explore three different scenarios to understand how varying levels of dishonesty impact the trustworthiness of self-reported data on drug use among students. By adjusting the proportions of deniers (those who falsely deny drug use) and braggers (those who falsely claim drug use), we can observe the effects on data reliability in each scenario.\n\nMinimal Dishonesty: This scenario simulates a situation where almost nobody lies (5%) about their drug use, resulting in very low proportions of both deniers and braggers.\nHigh Dishonesty: This scenario represents an extreme where there are high proportions of both deniers and braggers (95%).\nMixed Dishonesty: In this scenario, we simulate a situation with mixed levels of dishonesty, specifically with 20% deniers and 10% braggers.\n\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(scales)\nlibrary(bslib)\n\nui &lt;- page_fixed(\n    card(\n      card_header(\"Low Dishonesty\"),\n      card_body(\n        class = \"lead container\",\n        plotOutput(\"plot_low\")\n      )\n    ),\n    card(\n      card_header(\"High Dishonesty\"),\n      card_body(\n        class = \"lead container\",\n        plotOutput(\"plot_high\")\n      )\n        ),\n    card(\n      card_header(\"Mixed Dishonesty\"),\n      card_body(\n        class = \"lead container\",\n        plotOutput(\"plot_mixed\")\n      )\n    )\n  )\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  conf &lt;- function(prevalence, deniers, braggers) { \n    tibble(trust = (prevalence * (1 - deniers)) / \n             (\n               (prevalence * (1 - deniers)) + (braggers * (1 - prevalence))\n              ),\n           prevalence = prevalence,\n           braggers = braggers, \n           deniers = deniers,\n           group = c(1)) \n  }\n  \n  \n  generate_df &lt;- function(deniers, braggers) {\n    l &lt;- list(prevalence = seq(0.01, 0.99, 0.01),\n              deniers = rep(deniers, 99),\n              braggers = rep(braggers, 99))\n    pmap_dfr(l, conf)\n  }\n  \n  df_low &lt;- reactive({\n    generate_df(0.05, 0.05)\n  })\n  \n  df_high &lt;- reactive({\n    generate_df(0.95, 0.95)\n  })\n  \n  df_mixed &lt;- reactive({\n    generate_df(0.2, 0.1)\n  })\n  \n  output$plot_low &lt;- renderPlot({\n    ggplot(df_low(), aes(prevalence, trust, group = group)) +\n      geom_line(size = 1.2, color = \"#9966cc\") +\n      scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1)) +\n      scale_x_continuous(labels = scales::percent, breaks = seq(0, 1, 0.15)) +\n      labs(x = \"Drug prevalence\",\n           y = \"Trust\") +\n      theme_minimal() \n  })\n  \n  output$plot_high &lt;- renderPlot({\n    ggplot(df_high(), aes(prevalence, trust, group = group)) +\n      geom_line(size = 1.2, color = \"#9966cc\") +\n      scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1)) +\n      scale_x_continuous(labels = scales::percent, breaks = seq(0, 1, 0.15)) +\n      labs(x = \"Drug prevalence\",\n           y = \"Trust\") +\n      theme_minimal() \n  })\n  \n  output$plot_mixed &lt;- renderPlot({\n    ggplot(df_mixed(), aes(prevalence, trust, group = group)) +\n      geom_line(size = 1.2, color = \"#9966cc\") +\n      scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1)) +\n      scale_x_continuous(labels = scales::percent, breaks = seq(0, 1, 0.15)) +\n      labs(x = \"Drug prevalence\",\n           y = \"Trust\") +\n      theme_minimal() \n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\nYou can explore different scenarios to understand the impact of dishonesty on self-reported data about drug use. Use the sliders to adjust the proportions of deniers and braggers, and observe how these changes affect the trustworthiness of the data across various scenarios.\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(scales)\nlibrary(bslib)\n\nui &lt;- page_fixed(\n  \n  layout_columns(\n    card(sliderInput(\"deniers\",\n                     \"Deniers: say no, but yes\",\n                     min = 0.001,\n                     max = 0.999,\n                     value = 0.001)\n    ),\n    card(sliderInput(\"braggers\",\n                     \"Braggers: say yes, but no\",\n                     min = 0.001,\n                     max = 0.999,\n                     value = 0.001)\n    )\n  ),\n  card(\n    plotOutput(\"plot_sim\")\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  conf &lt;- function(prevalence, deniers, braggers) { \n    tibble(trust = (prevalence * (1 - deniers)) / \n             ((prevalence * (1 - deniers)) + (braggers * (1 - prevalence))),\n           prevalence = prevalence,\n           braggers = braggers, \n           deniers = deniers,\n           group = c(1)) \n  }\n  \n  df &lt;- reactive({\n    l &lt;- list(prevalence = seq(0.01, 0.99, 0.01),\n              deniers = rep(input$deniers, 99),\n              braggers = rep(input$braggers, 99))\n    pmap_dfr(l, conf)\n  })\n  \n  \n  output$plot_sim &lt;- renderPlot({\n    ggplot(df(), aes(prevalence, trust, group = group)) +\n      geom_line(size = 1.2, color = \"#9966cc\") +\n      scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1)) +\n      scale_x_continuous(labels = scales::percent, breaks = seq(0, 1, 0.15)) +\n      labs(x = \"Drug prevalence\",\n           y = \"Trust\") +\n      theme_minimal() \n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\nImagine a school with 100 students. Here’s how we can calculate the trust with different levels of deniers and braggers.\n\nTrue Prevalence of Drug Use: Let’s say the actual prevalence of drug use is 50%. So, 50 students actually use drugs.\nNo Dishonesty: All students report honestly.\nDeniers (50%): 50% of actual drug users deny using drugs.\nBraggers (5%): 5% of non-drug users falsely claim to use drugs.\n\n\nNo Dishonesty\n\n# Actual Drug Users: 50\n# Reported Drug Users: 50 (all 50 actual users report truthfully)\n\nprevalence &lt;- 50 / 100\ndeniers &lt;- 0\nbraggers &lt;- 0\n\nnumerator &lt;- prevalence * (1 - deniers) * 100\ndenominator &lt;- (prevalence * (1 - deniers) + braggers * (1 - prevalence)) * 100\n\ntrust &lt;- numerator / denominator\npaste(trust, \"confidence\")  \n\n[1] \"1 confidence\"\n\n\n\n\nWith 50% Deniers\n\n# Actual Drug Users: 50\n# Deniers (50%): 25 students who use drugs falsely report not using them.\n# Truthful Reports from Drug Users: 25\n\nprevalence &lt;- 50 / 100\ndeniers &lt;- 0.5\nbraggers &lt;- 0\n\nnumerator &lt;- prevalence * (1 - deniers) * 100\ndenominator &lt;- (prevalence * (1 - deniers) + braggers * (1 - prevalence)) * 100\n\ntrust &lt;- numerator / denominator\npaste(trust, \"confidence\")  \n\n[1] \"1 confidence\"\n\n\nIn this scenario, where there are 50% deniers and no braggers, the trust in a “yes” response is 100%. This means that if a student reports “yes” to using drugs, we can be completely confident that the student is indeed telling the truth, at a prevalence of 50%. This confidence arises because the 25 students who reported “yes” are all actual drug users, as there are no false positives. The presence of deniers, who are students that use drugs but falsely report “no,” does not affect the trust in a “yes” response. This is because these deniers simply reduce the number of “yes” responses from actual drug users, but the “yes” responses that do exist are entirely truthful.\n\n\n5% of Braggers\n\n# Actual Drug Users: 50\n# Truthful Reports from Drug Users: 50\n# Braggers (5%): 5% of 50 non-drug users falsely report using drugs: 5\n\nprevalence &lt;- 50 / 100\ndeniers &lt;- 0\nbraggers &lt;- 0.05\n\nnumerator &lt;- prevalence * (1 - deniers) * 100\ndenominator &lt;- (prevalence * (1 - deniers) + braggers * (1 - prevalence)) * 100\n\ntrust &lt;- numerator / denominator\npaste(trust, \"confidence\")\n\n[1] \"0.952380952380952 confidence\"\n\n\n\n\n\n\n\nConclusion\nThis simulation app shows how varying levels of dishonesty among students impact the reliability of self-reported data on drug use. Whether dishonesty is minimal, high, or mixed, the accuracy of survey results is affected, underscoring the importance of addressing reporting biases in adolescent drug use research. By understanding the potential effects of underreporting and overreporting, researchers can develop more accurate data collection methods and analytical strategies, leading to more reliable findings. These insights are crucial for informing interventions aimed at addressing drug use among adolescents.\n\n\nSome Recommendations for Measuring Drug Use\n\nInclude a Fictitious Drug in the Report: Adding a fictitious drug to the questionnaire can help identify and exclude responses from students who overreport (braggers).\nGuarantee Anonymity: Ensure students’ anonymity to encourage honest reporting. For example, do not collect data in the presence of familiar adults, which can create pressure and lead to dishonest answers.\nEducate on the Importance of Honesty: Explain the benefits of honest reporting to students. Emphasize how accurate data can help create better policies and interventions that benefit their community.\nCompare with Other Reports: Cross-check your prevalence data with other studies and reports. This can help identify discrepancies and validate the accuracy of your findings.\nConduct Consistency Analysis: Analyze the consistency of responses to ensure coherence. For example, lifetime prevalence should be consistent with last month’s prevalence; it is illogical for someone to report drug use in the past month but not in their lifetime.\nRandomized Response Techniques: Use randomized response techniques to increase the likelihood of truthful responses. This method allows respondents to answer sensitive questions while maintaining privacy, reducing the fear of disclosure.\nUse Validated Questionnaires: Employ well-validated and standardized questionnaires that have been proven to minimize bias and elicit more accurate responses.\nProvide Confidential Environments: Create a confidential and comfortable environment for survey administration. Ensure that students feel safe and that their responses will not be traced back to them.\nImplement Follow-Up Questions: Include follow-up questions that cross-validate initial responses. Inconsistencies in follow-up responses can highlight potential dishonesty.\nTrain Data Collectors: Train those administering the surveys to build rapport with students and encourage honesty. Skilled data collectors can significantly impact the quality of the data collected.\nPilot Testing: Conduct pilot testing of the survey to identify potential issues and biases before the full-scale implementation. This allows for adjustments to improve accuracy.\nLongitudinal Studies: Where possible, use longitudinal studies to track changes over time, which can help validate self-reported data through consistency checks over multiple points in time.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-11-09-organize-this/organize-this.html",
    "href": "posts/2024-11-09-organize-this/organize-this.html",
    "title": "How to Create a Bash Script to Automatically Organize Your Files",
    "section": "",
    "text": "Ever found yourself drowning in a sea of unorganized files? Try this simple but powerful Bash function that automatically organizes files into appropriate folders based on their extensions. This blog post will guide you through setting up an organize_files function that will help keep your directories neat and tidy."
  },
  {
    "objectID": "posts/2024-11-09-organize-this/organize-this.html#step-1-prepare-your-workspace",
    "href": "posts/2024-11-09-organize-this/organize-this.html#step-1-prepare-your-workspace",
    "title": "How to Create a Bash Script to Automatically Organize Your Files",
    "section": "Step 1: Prepare Your Workspace",
    "text": "Step 1: Prepare Your Workspace\nJust like in our previous scripts here, we’ll store this function in your bin directory. If you haven’t created one yet:\n1.1. Open the terminal (cmd+space then type terminal).\n1.2. Create a new folder named bin in your home directory:\nmkdir ~/bin\n\n\n\n\n\n\nNote\n\n\n\nIf you already have a bin folder from previous scripts, you can skip this step."
  },
  {
    "objectID": "posts/2024-11-09-organize-this/organize-this.html#step-2-create-the-script-file",
    "href": "posts/2024-11-09-organize-this/organize-this.html#step-2-create-the-script-file",
    "title": "How to Create a Bash Script to Automatically Organize Your Files",
    "section": "Step 2: Create the Script File",
    "text": "Step 2: Create the Script File\nNavigate to your bin directory and create a new file named organize:\ncd ~/bin\ntouch organize"
  },
  {
    "objectID": "posts/2024-11-09-organize-this/organize-this.html#step-3-add-the-script-code",
    "href": "posts/2024-11-09-organize-this/organize-this.html#step-3-add-the-script-code",
    "title": "How to Create a Bash Script to Automatically Organize Your Files",
    "section": "Step 3: Add the Script Code",
    "text": "Step 3: Add the Script Code\nOpen the file in your preferred text editor. You can use Nano for simplicity:\nnano organize\nCopy and paste the following code into the file:\n#!/bin/bash\n\n# Enable nullglob to avoid errors when no files match a pattern\nshopt -s nullglob\n\n# Check if directory exists\nif [ ! -d \"$1\" ]; then\n    echo \"Error: Directory '$1' does not exist\"\n    exit 1\nfi\n\ntarget_dir=\"$1\"\necho \"Organizing files in: $target_dir\"\n\n# Create directories if they don't exist\nmkdir -p \"$target_dir\"/{Images,Videos,Pdf,PowerPoint,Text,Archives,Excel,Others}\n\n# Change to target directory\ncd \"$target_dir\" || exit 1\n\n# Function to move files and handle duplicates\nmove_file() {\n    local file=\"$1\"\n    local target_folder=\"$2\"\n    local filename=$(basename \"$file\")\n    \n    if [ -e \"$target_folder/$filename\" ]; then\n        # If file with the same name exists, add \"-duplicate\" before the extension\n        local name=\"${filename%.*}\"\n        local extension=\"${filename##*.}\"\n        local duplicate_name=\"${name}-duplicate.${extension}\"\n        \n        # Check again in case a duplicate already exists\n        while [ -e \"$target_folder/$duplicate_name\" ]; do\n            name=\"${name}-duplicate\"\n            duplicate_name=\"${name}.${extension}\"\n        done\n        \n        mv -v \"$file\" \"$target_folder/$duplicate_name\"\n    else\n        mv -v \"$file\" \"$target_folder/\"\n    fi\n}\n\n# Move files based on their extensions\n\n# Images\nfor file in *.{jpg,jpeg,png,gif,HEIC}; do\n    [ -e \"$file\" ] && move_file \"$file\" \"Images\"\ndone\n\n# Videos\nfor file in *.{mp4,avi,mov}; do\n    [ -e \"$file\" ] && move_file \"$file\" \"Videos\"\ndone\n\n# PDF\nfor file in *.pdf; do\n    [ -e \"$file\" ] && move_file \"$file\" \"Pdf\"\ndone\n\n# PowerPoint\nfor file in *.pptx; do\n    [ -e \"$file\" ] && move_file \"$file\" \"PowerPoint\"\ndone\n\n# Text\nfor file in *.{docx,txt}; do\n    [ -e \"$file\" ] && move_file \"$file\" \"Text\"\ndone\n\n# Archives\nfor file in *.{zip,rar}; do\n    [ -e \"$file\" ] && move_file \"$file\" \"Archives\"\ndone\n\n# Excel\nfor file in *.{xlsx,csv}; do\n    [ -e \"$file\" ] && move_file \"$file\" \"Excel\"\ndone\n\n# Others (any remaining files)\nfor file in *; do\n    [ -f \"$file\" ] && move_file \"$file\" \"Others\"\ndone\n\necho \"File organization complete in $target_dir!\"\nSave and close (cmd+x then Y then return) the file.\n\n\n\n\n\n\nTip\n\n\n\nThis script creates separate folders for different file types and moves files accordingly. It handles various common file formats including images, videos, documents, and more."
  },
  {
    "objectID": "posts/2024-11-09-organize-this/organize-this.html#step-4-make-the-script-executable",
    "href": "posts/2024-11-09-organize-this/organize-this.html#step-4-make-the-script-executable",
    "title": "How to Create a Bash Script to Automatically Organize Your Files",
    "section": "Step 4: Make the Script Executable",
    "text": "Step 4: Make the Script Executable\nMake your script executable by running:\nchmod +x ~/bin/organize"
  },
  {
    "objectID": "posts/2024-11-09-organize-this/organize-this.html#step-5-update-your-shell-configuration",
    "href": "posts/2024-11-09-organize-this/organize-this.html#step-5-update-your-shell-configuration",
    "title": "How to Create a Bash Script to Automatically Organize Your Files",
    "section": "Step 5: Update Your Shell Configuration",
    "text": "Step 5: Update Your Shell Configuration\nIf you haven’t already added your bin directory to your PATH (from previous scripts), you’ll need to do so. Open your shell configuration file:\nFor Zsh users:\nnano ~/.zshrc\nFor Bash users:\nnano ~/.bashrc\nAdd this line if it’s not already there:\nexport PATH=\"$HOME/bin:$PATH\""
  },
  {
    "objectID": "posts/2024-11-09-organize-this/organize-this.html#step-6-apply-the-changes",
    "href": "posts/2024-11-09-organize-this/organize-this.html#step-6-apply-the-changes",
    "title": "How to Create a Bash Script to Automatically Organize Your Files",
    "section": "Step 6: Apply the Changes",
    "text": "Step 6: Apply the Changes\nSource your configuration file to apply the changes:\nFor Zsh:\nsource ~/.zshrc\nFor Bash:\nsource ~/.bashrc"
  },
  {
    "objectID": "posts/2024-03-12-imbalance/imbalance_ci.html",
    "href": "posts/2024-03-12-imbalance/imbalance_ci.html",
    "title": "Balancing Data in Machine Learning: When Good Intentions Meet Slippery Slopes",
    "section": "",
    "text": "Have you ever attended a party where the host tries so hard to make everyone happy that they end up achieving the exact opposite? That’s somewhat like what happens when we overenthusiastically balance data in machine learning. The intention is noble, but the outcome? Not always a hit.\nIn the debate over whether to balance data or not (Elor & Averbuch-Elor, 2022; Goorbergh et al., 2022), simulations often act as referees, showcasing results that typically advise against manipulating data to achieve balance. But let’s be honest, can we trust everything that comes out of a simulation? Shouldn’t we explore different angles to grasp why data balancing might not always be the wisest course of action?\n\nRubin to the Rescue… Or Not?\nLet’s turn the spotlight onto a different perspective. Picture yourself in an examination hall, faced with a challenging test. In a moment of desperation, you decide to copy answers from the student next to you. However, you have no evidence they’re the top student. So, you start sizing them up. Glasses? Check. Philosophical expressions? Check. Or maybe they’re a math wiz because, obviously, that’s the universal sign of genius, right?\nAs your anxiety mounts, something extraordinary happens: a fairy godmother appears, not with a magic wand, but with important message: she whispers your neighbor’s grade. Bingo! With newfound confidence, you embark on your note-transcribing quest.\nThis little adventure parallels adjusting your data based on observed outcomes. Before the fairy godmother’s revelation, you’re in the dark, much like when we lack insight into the observed outcomes. You might make assumptions (glasses mean brains, right?), but that’s shaky ground and we might just be copying the wrong answers, mistaking luck for wisdom.\nAnd that’s where Rubin’s thinking comes in. It’s not like the fairy godmother, it is much like venturing into an enchanted forest where paths diverge and outcomes remain uncertain.\nThe Rubin causal model, also known as the potential outcomes framework, is a way to think about cause and effect in a systematic manner. Imagine every time you make a decision, there are two parallel universes: one where you made one choice and another where you made a different one. The Rubin model compares what happens in these two universes to understand the true impact of that choice. It’s like having a twin who makes different decisions, and by looking at both your lives, you can see the effects of those decisions. In practice, since we can’t see parallel universes, this model uses statistics to estimate what would have happened in the alternate scenario, helping to isolate the cause of an outcome from all the noise. In this context, balancing data is contrary to observing divergence outcomes across parallel universes; it artificially aligns one universe to resemble the other, potentially obscuring causal relationships rather than revealing them.\nBut wait, as in fairy tales, there’s a twist! Machine learning loves predictions like cats love cardboard boxes. And in this arena, confounders could be seen as those annoying but sometimes helpful pieces of furniture. Sure, they make your model look cluttered, but they might just help it predict better, and we have the tools to deal with the ‘cluttering’: ’In the force of data, too much bias, the clarity it clouds; too much variance, the truth it obscures.\nIn essence, while machine learning and causal reasoning might seem like strange bedfellows, they both aim to make sense of our chaotic, data-driven world. Perhaps it’s time we invite them both to the party, have them shake hands, and work together. After all, the best parties are those where everyone gets along, right?\n\n\n\n\n\n Back to topReferences\n\nElor, Y., & Averbuch-Elor, H. (2022). To SMOTE, or not to SMOTE? CoRR, abs/2201.08528. https://arxiv.org/abs/2201.08528\n\n\nGoorbergh, R. van den, Smeden, M. van, Timmerman, D., & Van Calster, B. (2022). The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression. Journal of the American Medical Informatics Association, 29(9), 1525–1534. https://doi.org/10.1093/jamia/ocac093"
  },
  {
    "objectID": "posts/2023-07-22-collect_metrics/collect_metrics.html",
    "href": "posts/2023-07-22-collect_metrics/collect_metrics.html",
    "title": "Computing Sensitivity and Specificity Using collect_metrics",
    "section": "",
    "text": "(in progress)\nIn the last post, we discussed computing sensitivity and specificity using the sensitivity() and specificity() functions from the yardstick package. In this post, we’ll demonstrate how to compute these metrics using the collect_metrics function from the tidymodels package."
  },
  {
    "objectID": "posts/2023-07-22-collect_metrics/collect_metrics.html#summary",
    "href": "posts/2023-07-22-collect_metrics/collect_metrics.html#summary",
    "title": "Computing Sensitivity and Specificity Using collect_metrics",
    "section": "Summary",
    "text": "Summary\nIn this blog post, we discussed the use of the tidymodels package to compute the sensitivity and specificity of a model - two important measures for evaluating model performance. We started with the recreation of a confusion matrix, then used the tbl_cross() function to perform some data wrangling.\nNext, we leveraged fit_resamples() from the tidymodels package to estimate our model multiple times, using cross-validation with 10 repeated folds to create 100 datasets in total.\nWe set up a logistic regression model, fit it to our data, and estimated the model multiple times using the created folds. The saved predictions allowed us to compute sensitivity and specificity for each fold.\nWe defined a custom set of metrics - roc_auc, sensitivity, and specificity - using the metric_set() function from the yardstick package. Finally, we used collect_metrics() to compute the average of these metrics for all the folds, and visualized the distribution of the sensitivity and specificity values.\nFuture posts will delve into calculating confidence intervals and evaluating the variability of these values across resamples."
  },
  {
    "objectID": "posts/2024-11-17-to-duckdb/to-duckdb.html",
    "href": "posts/2024-11-17-to-duckdb/to-duckdb.html",
    "title": "How to Create a Bash Script for Easy File Conversion Using DuckDB",
    "section": "",
    "text": "Working with different file formats can be challenging, especially when dealing with large datasets. Here’s a simple but powerful Bash script that leverages DuckDB to convert files between different formats (like CSV and Parquet) directly from your terminal.\nI took the script from here.\nPlease visit the Duckdb website. They are doing some of the most amazing work in the data space.\nAlso, visite the Youtube channel for some amazing content. And Motherduck for some more amazing things."
  },
  {
    "objectID": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-1-install-duckdb",
    "href": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-1-install-duckdb",
    "title": "How to Create a Bash Script for Easy File Conversion Using DuckDB",
    "section": "Step 1: Install DuckDB",
    "text": "Step 1: Install DuckDB\nBefore we begin, make sure you have DuckDB installed. You can install it using Homebrew:\nbrew install duckdb\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t have Homebrew installed, you can get it from brew.sh."
  },
  {
    "objectID": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-2-prepare-your-workspace",
    "href": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-2-prepare-your-workspace",
    "title": "How to Create a Bash Script for Easy File Conversion Using DuckDB",
    "section": "Step 2: Prepare Your Workspace",
    "text": "Step 2: Prepare Your Workspace\nJust like in our previous scripts (getref and organize), we’ll store this function in your bin directory. If you haven’t created one yet:\nmkdir ~/bin"
  },
  {
    "objectID": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-3-create-the-script-file",
    "href": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-3-create-the-script-file",
    "title": "How to Create a Bash Script for Easy File Conversion Using DuckDB",
    "section": "Step 3: Create the Script File",
    "text": "Step 3: Create the Script File\nNavigate to your bin directory and create a new file named toduck:\ncd ~/bin\ntouch toduck"
  },
  {
    "objectID": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-4-add-the-script-code",
    "href": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-4-add-the-script-code",
    "title": "How to Create a Bash Script for Easy File Conversion Using DuckDB",
    "section": "Step 4: Add the Script Code",
    "text": "Step 4: Add the Script Code\nOpen the file in your preferred text editor:\nnano toduck\nCopy and paste the following code:\n#!/bin/bash\n\nconvert_file() {\n    local input_file=\"$1\"\n    local output_extension=\"$2\"\n\n    # Extracting the filename without extension\n    local base_name=$(basename -- \"$input_file\")\n    local name=\"${base_name%.*}\"\n\n    # Constructing the output filename\n    local output_file=\"${name}.${output_extension}\"\n\n    # Performing the conversion\n    duckdb -c \"copy (select * from '${input_file}') to '${output_file}'\"\n\n    echo \"Conversion complete: ${output_file}\"\n}\n\n# Check if the number of arguments is less than 2\nif [ \"$#\" -lt 2 ]; then\n    echo \"Usage: $0 &lt;input_file&gt; &lt;output_extension&gt;\"\n    echo \"Example: $0 example.parquet csv\"\n    exit 1\nfi\n\n# Call the conversion function with the provided arguments\nconvert_file \"$1\" \"$2\"\nExit the editor and save the file.\n\n\n\n\n\n\nTip\n\n\n\nThis script uses DuckDB’s SQL capabilities to read and write files in different formats. DuckDB automatically detects the input format and handles the conversion seamlessly."
  },
  {
    "objectID": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-5-make-the-script-executable",
    "href": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-5-make-the-script-executable",
    "title": "How to Create a Bash Script for Easy File Conversion Using DuckDB",
    "section": "Step 5: Make the Script Executable",
    "text": "Step 5: Make the Script Executable\nMake your script executable by running:\nchmod +x ~/bin/toduck"
  },
  {
    "objectID": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-6-update-your-shell-configuration",
    "href": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-6-update-your-shell-configuration",
    "title": "How to Create a Bash Script for Easy File Conversion Using DuckDB",
    "section": "Step 6: Update Your Shell Configuration",
    "text": "Step 6: Update Your Shell Configuration\nIf you haven’t already added your bin directory to your PATH (from previous scripts), you’ll need to do so. Open your shell configuration file:\nFor Zsh users:\nnano ~/.zshrc\nFor Bash users:\nnano ~/.bashrc\nAdd this line if it’s not already there:\nexport PATH=\"$HOME/bin:$PATH\""
  },
  {
    "objectID": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-7-apply-the-changes",
    "href": "posts/2024-11-17-to-duckdb/to-duckdb.html#step-7-apply-the-changes",
    "title": "How to Create a Bash Script for Easy File Conversion Using DuckDB",
    "section": "Step 7: Apply the Changes",
    "text": "Step 7: Apply the Changes\nSource your configuration file to apply the changes:\nFor Zsh:\nsource ~/.zshrc\nFor Bash:\nsource ~/.bashrc"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Hi, I’m Francisco Cardozo! I’m a researcher passionate about using data and technology to make a difference in prevention science. My work is all about developing and evaluating programs that prevent health challenges, and I’m especially excited about how AI can improve the way these interventions are implemented. I’m also a big believer in open-source software and love contributing tools and resources to make research more accessible and transparent. At the end of the day, my goal is to bring innovation into public health to create meaningful, lasting change."
  },
  {
    "objectID": "index.html#workshops",
    "href": "index.html#workshops",
    "title": "About me",
    "section": "Workshops",
    "text": "Workshops\n\nWorkshops website"
  },
  {
    "objectID": "index.html#web-projects",
    "href": "index.html#web-projects",
    "title": "About me",
    "section": "Web Projects",
    "text": "Web Projects\n\nBusiness That Care Toolkit\nThrustworthy"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "About me",
    "section": "R Packages",
    "text": "R Packages\n\nBreakNBuild: Designed to evaluate model performance with progressively sampled data.\nDocumentData: Designed to facilitate dataset documentation during package creation.\nwandR: Designed to transform R learning into an entertaining journey for all, by weaving the magic of the wizarding world into R programming.\nChessOlympiad: This package contains three datasets to analyze the performance of 937 players from 188 countries during the Chess Olympiad in Chennai, India in 2022."
  },
  {
    "objectID": "index.html#communities",
    "href": "index.html#communities",
    "title": "About me",
    "section": "Communities",
    "text": "Communities\n\nSoftware Carpentry\nrOpenSci\nSociety for Prevention Research\nInternational Network for Social Network Analysis"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "About me",
    "section": "Contact",
    "text": "Contact\n\n📧 foc9@miami.edu\nPersonal card"
  }
]