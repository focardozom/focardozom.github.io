[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\nReading Time\n\n\n\n\n\n\nDec 3, 2023\n\n\nBalancing Efficiency and Flexibility: The Challenges of Over-Optimization in the Multiphase Optimization Strategy (MOST)\n\n\nMOST,Prevention,optimization\n\n\n3 min\n\n\n\n\nAug 9, 2023\n\n\nThe Chessboard and the Wise Courtier\n\n\nChess,Python,loops,exponential growth\n\n\n8 min\n\n\n\n\nJul 22, 2023\n\n\nComputing Sensitivity and Specificity Using collect_metrics\n\n\nSensitivity,Tidymodels,Specificity\n\n\n7 min\n\n\n\n\nJul 16, 2023\n\n\nComputing Sensitivity and Specificity using tidymodels\n\n\nSensitivity,Tidymodels,Specificity\n\n\n7 min\n\n\n\n\nMay 10, 2023\n\n\nAn Introduction to the Recipes Package for Data Preprocessing\n\n\nTidymodels\n\n\n3 min\n\n\n\n\nFeb 10, 2023\n\n\nMaximizing the Impact of Your R Teaching: Insights from Garrett Grolemund\n\n\nTeaching\n\n\n3 min\n\n\n\n\nDec 1, 2022\n\n\nCan ELO Predict the Outcome of Chess Games?\n\n\nChess,Trees,Machine Learning\n\n\n7 min\n\n\n\n\nNov 27, 2022\n\n\nTrustworthy\n\n\nAlcohol\n\n\n2 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/2023-07-15-yarstick/2023-07-15.html",
    "href": "posts/2023-07-15-yarstick/2023-07-15.html",
    "title": "Computing Sensitivity and Specificity using tidymodels",
    "section": "",
    "text": "Sensitivity and specificity are two crucial metrics used to evaluate the performance of binary classification models. In this article, we delve into the process of computing these metrics using the ‚Äòtidymodels‚Äô package in R.\nWe begin by simulating a dataset. Assuming we have access to the true value of the phenomenon under consideration, as well as our model‚Äôs predictions, we can proceed to compute the sensitivity and specificity of our model.\n\nlibrary(tidyverse)  # Load tidyverse package\nlibrary(gt)         # Load gt package\nlibrary(gtsummary)  # Load gtsummary package\n\n\n# Create a tibble with the confusion matrix data\nconfusion_matrix_long &lt;- tibble(\n  'Predicted Class' = c('1', '1', '0', '0'),\n  'True Class' = c('True 1', 'True 0', 'True 1', 'True 0'),\n  'Count' = c(40, 15, 10, 35)\n)\n\nconfusion_matrix_long  |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      Predicted Class\n      True Class\n      Count\n    \n  \n  \n    1\nTrue 1\n40\n    1\nTrue 0\n15\n    0\nTrue 1\n10\n    0\nTrue 0\n35\n  \n  \n  \n\n\n\n\nFunction tbl_cross() offers a handy feature for obtaining percentages, which is useful when computing sensitivity and specificity.\n\n# Duplicate each row of the tibble based on the value in the Count column\nconfusion_matrix_long |&gt;\n  uncount(Count) |&gt;\n  tbl_cross(percent = \"col\")  \n\n\n\n\n\n  \n    \n    \n      \n      \n        True Class\n      \n      Total\n    \n    \n      True 0\n      True 1\n    \n  \n  \n    Predicted Class\n\n\n\n    ¬†¬†¬†¬†0\n35 (70%)\n10 (20%)\n45 (45%)\n    ¬†¬†¬†¬†1\n15 (30%)\n40 (80%)\n55 (55%)\n    Total\n50 (100%)\n50 (100%)\n100 (100%)\n  \n  \n  \n\n\n\n\nSo, we know the values of specificity and sensitivity, which are essentially the results of these two equations:\n\nSensitivity = TP / (TP + FN) = 40 / (40 + 10) = 0.80 (80%)\nSpecificity = TN / (TN + FP) = 35 / (35 + 15) = 0.70 (70%)\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nRefresh your memory on the meaning of these terms by looking at the following table:\n\n\n\n\n\nTrue 1 (Positive)\nTrue 0 (Negative)\n\n\n\n\nPredict 1 (Positive)\nTP = 40\nFP = 15\n\n\nPredict 0 (Negative)\nFN = 10\nTN = 35\n\n\n\nThis table above is the confusion matrix, featuring the following terms:\n\nTP (True Positive): Instances where the model correctly predicted the positive class.\nTN (True Negative): Instances where the model correctly predicted the negative class.\nFP (False Positive): Instances where the model incorrectly predicted the positive class.\nFN (False Negative): Instances where the model incorrectly predicted the negative class.\n\n\n\n\nHowever, the burning question remains: how do we calculate these in R? Let‚Äôs explore this while evaluating the performance of our models using the ‚Äòtidymodels‚Äô package.\n\nModeling with tidymodels\nLet‚Äôs now assume that we have a dataset comprising Y and X values, and we aim to create a model that predicts Y based on X. We‚Äôll use the same dataset we created earlier.\n\ndf &lt;- confusion_matrix_long |&gt; \n  uncount(Count)  |&gt; \n  rename(y = `True Class`, x = `Predicted Class`) # Rename the columns to shorter names\n\nFirst, we‚Äôll load the tidymodels library and specify our model.\n\n# Load the tidymodels package\nlibrary(tidymodels)\n\n# Define a logistic regression model specification\nmodel_spec &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt; \n  set_mode(\"classification\")\n\n# Define a recipe for preprocessing the data\nrecipe_glm &lt;- \n  recipe(y ~ x, data = df) |&gt; \n  # Convert all nominal variables to dummy variables\n  step_dummy(all_nominal(), -all_outcomes()) \n\nLet‚Äôs examine the dataset we‚Äôve created.\n\n# Preprocess the data using the recipe\n# This includes converting nominal variables to dummy variables\nrecipe_glm |&gt;\n  prep() |&gt;\n  # Apply the recipe to new data\n  bake(new_data = df) |&gt;\n  # View the first few rows of the preprocessed data\n  skimr::skim()\n\n\nData summary\n\n\nName\nbake(prep(recipe_glm), ne‚Ä¶\n\n\nNumber of rows\n100\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ny\n0\n1\nFALSE\n2\nTru: 50, Tru: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nx_X1\n0\n1\n0.55\n0.5\n0\n0\n1\n1\n1\n‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\n\n\n\nRemeber that our outcome is a binary variable with values True 0 and True 1.\nNext, we can create a workflow that combines our recipe and the model specification, and fit the model.\n\n# Define a workflow for fitting the logistic regression model\nwr_glm &lt;- workflow() |&gt; \n  add_recipe(recipe_glm) |&gt; \n  add_model(model_spec)  \n\n# Fit the logistic regression model to the preprocessed data\nmodel &lt;- wr_glm |&gt; \n  fit(data = df) \n\n# Extract the model coefficients and create a summary table\nmodel |&gt; \n  extract_fit_parsnip() |&gt; \n  tbl_regression()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    x_X1\n2.2\n1.3, 3.2\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nSo, we‚Äôve established that the model predicts the Y values based on the X values. But how do we calculate the sensitivity and specificity using the model results?\nFirst, we need to generate predictions. The augment() function from the dplyr package is an excellent choice for this task.\n\n# Load the reactable package\nlibrary(reactable)\nlibrary(knitr)\n\n# Use the augment() function to add predicted values to the data frame\nthe_predictions &lt;- model |&gt;\n  augment(df)  |&gt; \n  # Rename the \"y\" column to \"Observed\"\n  rename(\"Observed\" = \"y\")\n\n\n# Create an interactive table using the reactable::reactable() function\nthe_predictions |&gt;\n  reactable(\n    groupBy = \"Observed\",\n    columns = list(\n      x = colDef(aggregate = \"unique\"),\n      .pred_class = colDef(aggregate = \"unique\"),\n      `.pred_True 0` = colDef(aggregate = \"mean\", format = colFormat(digits = 2)),\n      `.pred_True 1` = colDef(aggregate = \"mean\", format = colFormat(digits = 2))\n    )\n  )\n\n\n\n\n\n\nAt this point, we have both the predicted class and the observed class. We could use the same method we employed in the earlier demonstration (tbl_cross()), or we can make use of the sensitivity() and specificity() functions from the yardstick package. Let‚Äôs look at how this works.\n\n\nComputing sensitivity and specificity using a cross table\n\nthe_predictions  |&gt; \n    tbl_cross(.pred_class, Observed, percent = \"col\")\n\n\n\n\n\n  \n    \n    \n      \n      \n        Observed\n      \n      Total\n    \n    \n      True 0\n      True 1\n    \n  \n  \n    .pred_class\n\n\n\n    ¬†¬†¬†¬†True 0\n35 (70%)\n10 (20%)\n45 (45%)\n    ¬†¬†¬†¬†True 1\n15 (30%)\n40 (80%)\n55 (55%)\n    Total\n50 (100%)\n50 (100%)\n100 (100%)\n  \n  \n  \n\n\n\n\nThis yields the same results as before.\n\n\nComputing sensitivity and specificity using the yardstick package\nFinally! we can use the sensitivity() and specificity() functions from the yardstick package to calculate the sensitivity and specificity of our model.\n\nsens &lt;- the_predictions  |&gt; \n  mutate(Observed = as.factor(Observed))  |&gt;\n  sensitivity(Observed, .pred_class)\n\nspec &lt;- the_predictions  |&gt; \n  mutate(Observed = as.factor(Observed))  |&gt;\n  specificity(Observed, .pred_class)\n\nbind_rows(sens, spec)  |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    sensitivity\nbinary\n0.7\n    specificity\nbinary\n0.8\n  \n  \n  \n\n\n\n\nOops! That doesn‚Äôt look right. Why is that?\nIn the context of logistic regression, R‚Äôs default behavior is to take the first level of a factor as the reference category when using the glm function. This behavior becomes particularly important when we‚Äôre dealing with binary outcomes, often coded as 0 (absence of the event) and 1 (presence of the event). By default, R will take 0 as the reference level and compare it against 1, due to 0 coming before 1 in numerical order.\nHowever, when we‚Äôre working with factors, the order in which they‚Äôre arranged is somewhat arbitrary and it doesn‚Äôt necessarily make sense to always treat 0 as the absence of an event and 1 as the presence of an event (Kuhn and Silge 2022).\n\n\n\n\n\n\nThis is from here or page 116 in the tidymodels book.\n\n\n\ntidymodels. pag 116\n\n\n\n\n\nThe yardstick package interprets this setup a bit differently. It views the first factor as the most important, leading it to switch the factor levels in the process. This change in order may affect our sensitivity and specificity measures.\nTo maintain consistency with our earlier computations, it‚Äôs necessary to explicitly set the reference level in the sensitivity() and specificity() functions from the yardstick functions, using the event_level = ‚Äúsecond‚Äù argument. This ensures that the factor levels are interpreted in a way that aligns with our initial demonstration.\n\nsens_y  &lt;- the_predictions  |&gt;\n  mutate(Observed = as.factor(Observed))  |&gt;\n  sensitivity(Observed, .pred_class, event_level = \"second\")\n\nspec_y  &lt;- the_predictions  |&gt;\n  mutate(Observed = as.factor(Observed))  |&gt;\n  specificity(Observed, .pred_class, event_level = \"second\")\n\nbind_rows(sens_y, spec_y)  |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    sensitivity\nbinary\n0.8\n    specificity\nbinary\n0.7\n  \n  \n  \n\n\n\n\nNow we‚Äôve restored our original results. üòé\n\n\n\n\n\n\nCaution\n\n\n\nThis ‚Äòswitching‚Äô behavior in yardstick is also apparent in the collect_metrics() function, making it essential to check the event level. Failure to do so may result in inadvertently switching the event level.\n\n\n\n\nSummary\nIn this blog post, we‚Äôve walked through the process of computing sensitivity and specificity using the tidymodels package in R, demonstrating it with a simulated dataset. These metrics are indispensable for evaluating the performance of binary classification models. Don‚Äôt forget: for the most accurate and realistic results, always evaluate your models using separate test data.\n\n\n\n\n\n Back to topReferences\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. O‚ÄôReilly Media, Inc."
  },
  {
    "objectID": "posts/2022-12-01-ChessOlympiad22/index.html",
    "href": "posts/2022-12-01-ChessOlympiad22/index.html",
    "title": "Can ELO Predict the Outcome of Chess Games?",
    "section": "",
    "text": "Code\nlibrary(ChessOlympiad22)\nlibrary(rpart.plot)\nlibrary(tidymodels)\n\n\nThe ELO rating system is a method used to calculate the relative skill levels of chess players, based on their game results. A higher ELO rating indicates a higher perceived skill level.\nIn this document, the ELO difference between two players is used to evaluate the probability of one player winning the match.\nI will use the The ChessOlympiad package. It contains two datasets: players and results. The players dataset contains the ELO ratings of all players participating in the 44th Chess Olympiad, which took place in Chennai, India, in 2022. The results dataset contains the results of all matches played in the tournament.\n\nDistribution of ELO\n\n\nCode\ndata(\"players\")\n\n\nFirst, let‚Äôs visualize the ELO distribution of the players.\nIt is important to filter out players with an ELO rating of zero from the analysis, as these players have not yet been rated and do not have a known skill level.\n\n\nCode\nplayers |&gt; \n  filter(rtg!=0) |&gt; \n  ggplot(aes(rtg)) + \n  geom_histogram(bins=90,fill=\"gray90\", alpha=0.85, color=\"gray95\") +\n  scale_x_continuous(breaks = seq(1000,2900,150))+\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        panel.grid = element_blank()) +\n  labs(x=\"Elo Rating System (Higher values indicate greater player strength)\",\n       y=\"\",\n       title = \"Elo Rating System of players in Chess Olympiad, 2022. Chennai, India \",\n       caption = \"Magnus Carlsen has the highest Elo rating in the tournament: 2864\") +\n  geom_text(data = players %&gt;% \n    filter(rtg &gt; 2800),\n            aes(x = 2550, \n            y = 4.5, \n            label = \"Magnus Carlsen\"),\n            size = 3,\n            color = \"black\") +\n  geom_curve(data = players %&gt;% filter(rtg &gt; 2800),\n               aes(x = 2700, \n               y = 4.4, xend = 2864.8, \n               yend = 1),\n               arrow = arrow(length = unit(0.3, \"cm\")),\n               color = \"red\",\n               linewidth = 0.5,\n               curvature = -0.5)\n\n\n\n\n\nIt may be interesting to plot the ELO ratings of players by federation, as the Chess Olympiad is played by national teams. By examining the ELO ratings of players within each federation, we can get a sense of the overall strength of the teams participating in the event. This analysis could potentially provide insight into the results of the Chess Olympiad and help predict the outcomes of matches.\n\n\nCode\nplayers |&gt; \n  filter(rtg&gt;2600) |&gt; \n  ggplot(aes(reorder(fed, rtg),rtg)) + \n  theme_minimal() +\n  geom_boxplot() +\n  coord_flip() +\n  theme(panel.grid = element_blank()) +\n  labs(x=\"\", y=\"ELO\")\n\n\n\n\n\nAccording to ELO ratings, the United States fielded the strongest team in the tournament.\n\n\nCode\ndata(\"results\")\n\n\n\n\nDifferences in ELO by round\nThe Chess Olympiad followed a Swiss-style tournament, meaning that players are paired with opponents with similar scores in each round. Specifically, in the first round, the highest-ranked player is matched against the median-ranked player, followed by the second-highest ranked player against the next below median, and so forth.\nA visual representation of the differences in ELO by round are presented in the following graph.\n\n\nCode\nresults |&gt; \n  filter(elo_difference&gt;=-1000, \n         elo_difference&lt;=1000, \n         !is.na(elo_white), !is.na(elo_black), elo_white!=0,elo_black!=0) |&gt; \n  ggplot(aes(as.numeric(elo_white),elo_difference, \n             fill=factor(result_white), \n             color=factor(result_white))) + \n  geom_point(shape = 21, alpha=0.85,\n             size = 3, stroke = 0.5) +\n  theme_minimal() +\n  scale_fill_manual(values=c(\"Lost\"=\"black\", \"Draw\"=\"gray50\",\"Won\"=\"white\")) +\n  scale_color_manual(values=c(\"Lost\"=\"black\", \"Draw\"=\"black\",\"Won\"=\"black\")) +\n  labs(fill=\"Result\", color=\"Result\", \n       x=\"Player Elo\", \n       y=\"Elo difference\",\n       caption = \"Difference greater than zero indicates stronger player\n       44th Chess Olympiad. Chennai, 2022 Open\") +\n  facet_wrap(~ round)\n\n\n\n\n\n\n\nModel the winning chances for players with the white pieces based on ELO difference\nNow, let‚Äôs try to identify the optimal divisions in ELO rating differences that could potentially classify the outcomes of chess games. For this, I will be using the tidymodels package to estimate a Classification and Regression Trees (CART) model.\n\n\nCode\ncart_spec &lt;-\n   decision_tree() |&gt; \n   set_engine(\"rpart\") |&gt;\n   set_mode(\"classification\")\n\n\nI will add two steps in the recipe. One to filter the data set by round, and the other to convert results in a factor variable. I also will limit my analysis to players with more than 1600 in ELO.\n\n\nCode\nresults &lt;- results |&gt; \n  filter(as.numeric(elo_white)&gt;1600) |&gt; \n  filter(as.numeric(elo_black)&gt;1600) \n\nrecipe &lt;- recipe(\n  result_white ~ elo_difference + round_number, data = results) |&gt; \n    step_filter(round_number==round)\n\n\n\n\nCode\nwrkfl &lt;- workflow() |&gt; \n  add_model(cart_spec) |&gt; \n  add_recipe(recipe)\n\n\nLet‚Äôs estimate the model for the round 1.\n\n\nCode\nround &lt;- 1\n\ncart_fit &lt;- wrkfl |&gt; \n  fit(data=results) |&gt; \n  extract_fit_parsnip()\n\n\n\nDraw an tree to understand the results\nFinally, I will create a tree showing the splits\n\n\nCode\ncart_fit &lt;- repair_call(cart_fit, data = round)\n\ncart_tree_fit &lt;- cart_fit$fit\n\nrpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)\n\n\n\n\n\nAccording to the model, a difference of 12 in the ELO is sufficient to accurately predict the winner in 95% of cases and the loser in 91% of cases.\n\n\n\nModel the last round\nNow, let‚Äôs apply the model to the final round, which featured matches between the most formidable opponents.\nI will add a the tree_depth parameter to my model. The depth of the tree refers to the number of levels the tree has.\n\n\nCode\ncart_spec &lt;-\n   decision_tree(tree_depth = 4) |&gt; \n   set_engine(\"rpart\") |&gt;\n   set_mode(\"classification\")\n\nwrkfl &lt;- workflow() |&gt; \n  add_model(cart_spec) |&gt; \n  add_recipe(recipe)\n\n\n\n\nCode\nround &lt;- 11\n\ncart_fit &lt;- wrkfl |&gt; \n  fit(data=results)\n\ncart_fit &lt;- wrkfl |&gt; \n  fit(data=results) |&gt; \n  extract_fit_parsnip()\n\ncart_fit &lt;- repair_call(cart_fit, data = round)\n\ncart_tree_fit &lt;- cart_fit$fit\n\nrpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)\n\n\n\n\n\n\n\nCode\nrpart.rules(cart_tree_fit, cover = TRUE)\n\n\n  ..y  Dra Los Won                                       cover\n Draw [.45 .30 .25] when elo_difference is -176 to  85     51%\n Draw [.55 .09 .36] when elo_difference &gt;=         333      4%\n Draw [.69 .15 .15] when elo_difference is   95 to 115      4%\n Lost [.29 .61 .10] when elo_difference &lt;  -176            20%\n  Won [.10 .30 .60] when elo_difference is   85 to  95      3%\n  Won [.17 .20 .63] when elo_difference is  115 to 333     18%\n\n\nThe color-coding of the decision tree leaves suggests that differences in ELO ratings remain a critical factor, even in the final round of the chess tournament. It seems that the model is particularly adept at predicting outcomes when the ELO differences are substantial. For example, if you have more than 333 points in ELO, the model predicts 63% wining chances for you. However, if the ELO difference is less than 333 (but more than 115), the model predicts 55% of draw.\nWhen the ELO difference is less than 115, the model‚Äôs predictions become more interesting. If a player has 176 points less than their opponent, the model is more likely to classify them as a loser (61%). However, if the ELO difference is less than 85 points, the player still has a good chance of winning the game. This could be seen as an indicator of the performance of some players with lower ELO ratings who are having a strong tournament. On the other hand, if the ELO difference is greater than 85 points, most of the chances are for a draw.\nThis serves as a quick demonstration of how the ChessOlympiad package can be utilized in predictive modeling.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-08-01-while/while.html",
    "href": "posts/2023-08-01-while/while.html",
    "title": "The Chessboard and the Wise Courtier",
    "section": "",
    "text": "The Story\nOnce upon a time in ancient India, a wise courtier presented a challenge to his king using a chessboard and some grains of rice. He placed a single grain of rice on the first square of the chessboard, then asked the king to double the number of grains on each subsequent square until all 64 squares were filled. Initially, the king thought that the result would be a meaningless amount of rice, but he soon realized the astronomical sum this would amount to.\n\n\n\n\n\n‚ÄúGenerated with Midjourney‚Äù\n\n\n\n\n\nThis story illustrates the power of exponential growth, a mathematical concept that can describe certain patterns in nature and society. Exponential growth occurs when the rate of growth is proportional to the current value, leading to increasingly rapid growth over time. This type of growth can be found in many real-world phenomena, including the spread of disease, and the adoption of new technologies.\n\n\n\n\n\n\n\n\nNote\n\n\n\nPandemics: In the early stages of a pandemic, such as COVID-19, the number of infections may double within a fixed time period, leading to an exponential increase.\nSocial Media and Viral Content: Posts or videos that ‚Äúgo viral‚Äù on social media platforms can exhibit exponential growth. A piece of content might be shared by a few people initially, but if each person who sees it shares it with others, the total number of views can grow exponentially.\nEnvironmental Concerns: Exponential growth can also have negative consequences, such as in the spread of invasive species or the consumption of non-renewable resources, leading to potential environmental damage.\n\n\nI will use the story of chess to demonstrate how quickly the number of grains grows when it doubles over the course of only 64 steps. I will also provide Python code to calculate the total number of grains on the chessboard.\n\n\nPython Code to Compute the Total Number of Grains\nImagine you have a bag of grains, and you want to fill a chessboard with them. You start with one grain on the first square, then double the number of grains on each subsequent square until you reach the 64th square. You can think of this process like a snowball rolling down a hill, getting bigger and bigger as it goes.\nIn Python, we can use something called a while loop to replicate this process. It‚Äôs like having a robot that puts the grains on the squares for you, following your exact instructions: ‚ÄúStart with one grain, then double the number, and keep going until you fill all 64 squares.‚Äù\n\n\n\n\n\n\nWhile Loop\n\n\n\nA while loop continues to execute the block of code as long as a specified condition is true. It checks the condition before each iteration, and if the condition is false, it exits the loop.\n\n\nNow, let‚Äôs think about a different way to approach the task using what‚Äôs called a for loop. Imagine instead that you have 64 small containers, each representing a square on the chessboard, and you know exactly how many grains should go in each one. Starting with the first container, you fill it with one grain, then move to the next one and fill it with double the grains of the previous container, and so on, until you reach the 64th container.\n\n\n\n\n\n\nFor Loop\n\n\n\nA for loop works like this staircase-building process. You tell the computer exactly how many times to repeat something, and it follows your instructions step by step. Unlike the snowball rolling down the hill, which might take an unpredictable path, building the staircase (or using a ‚Äúfor loop‚Äù) is more controlled and precise.\n\n\n\n\nWhile Loop in python\nLets to compute that number of grains using the while loop.\n\n# Initialize variables\nsquare_number = 1  # The number of the current square\ngrains_on_square = 1  # The number of grains on the current square\ntotal_grains = 0  # The total number of grains so far\n\n# Loop through each square on the chessboard\nwhile square_number &lt;= 64:\n    # Add the number of grains on the current square to the total\n    total_grains += grains_on_square\n    # Double the number of grains on the current square for the next square\n    grains_on_square *= 2\n    # Move to the next square\n    square_number += 1\n\nprint(f\"The total number of grains on the chessboard is: \\n {total_grains:,}\")\n\nThe total number of grains on the chessboard is: \n 18,446,744,073,709,551,615\n\n\nSo, it‚Äôs a big number. Here are some interesting facts about this number:\n\n\n\n\n\n\nNote\n\n\n\n\nThis number in words is eighteen quintillion, four hundred forty-six quadrillion, seven hundred forty-four trillion, seventy-three billion, seven hundred nine million, five hundred fifty-one thousand, six hundred fifteen, or approximately 18 quintillion, or 18 billion billion.\nIn seconds, it is equal to 293,274,701,009 years, 3 weeks, 3 days, 15 hours, 30 minutes, 7 seconds.\nIf each grain of rice were 0.05 grams in weight, then the total weight of rice would be 922337203685.48 tons.\n\n\n\n\n# Total number of grains as computed earlier\ntotal_grains = 18446744073709551615\n\n# Weight of a single grain in grams\nweight_per_grain = 0.05\n\n# Compute the total weight in grams\ntotal_weight_grams = total_grains * weight_per_grain\n\n# Convert to kilograms\ntotal_weight_kilograms = total_weight_grams / 1000\n\n# Convert to metric tons\ntotal_weight_metric_tons = total_weight_kilograms / 1000\n\nprint(f\"The total weight of the rice in grams is {total_weight_grams:.2f} grams\")\nprint(f\"The total weight of the rice in kilograms is {total_weight_kilograms:.2f} kilograms\")\nprint(f\"The total weight of the rice in metric tons is {total_weight_metric_tons:.2f} metric tons\")\n\nThe total weight of the rice in grams is 922337203685477632.00 grams\nThe total weight of the rice in kilograms is 922337203685477.62 kilograms\nThe total weight of the rice in metric tons is 922337203685.48 metric tons\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nFind more facts here:\n‚Äú18446744073709551615 - Facts‚Äù\n\n\nLet‚Äôs finish this post by comparing the while loop with the for loop.\n\n\n\n\n\n\n\n\nAspect\nWhile Loop\nFor Loop\n\n\n\n\nCondition vs Sequence\nUses a condition that can be any logical expression, continues iterating as long as the condition is true.\nIterates over a sequence of values, executing the code block once for each value.\n\n\nControl Over Iteration\nYou have full control over how many times the loop iterates by manipulating the condition.\nThe number of iterations is defined by the length of the sequence.\n\n\nPotential for Infinite Loop\nThere‚Äôs a risk of creating an infinite loop if the condition never becomes false.\nGenerally not at risk for infinite loops, as it iterates over a finite sequence.\n\n\nUse Cases\nOften used when you don‚Äôt know how many times you‚Äôll need to iterate.\nUsed when you want to iterate a known number of times or over a specific sequence.\n\n\n\n\nConclusion\nThe chessboard problem illustrates how quickly numbers can grow when they double with each step. By using a while loop in Python, we‚Äôve computed this enormous figure, providing insight into the power of exponential growth.\nThis example shows how Python can be used to solve problems that would be otherwise challenging to compute manually. Feel free to explore the code further, perhaps by changing the number of squares or the initial number of grains. What other intriguing patterns might you discover?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-05-10-recipes/recipes.html",
    "href": "posts/2023-05-10-recipes/recipes.html",
    "title": "An Introduction to the Recipes Package for Data Preprocessing",
    "section": "",
    "text": "I liked this presentation done by Max Kuhn.\n\n\nHere are some notes:\nThe recipes package provides a framework for preprocessing data prior to modeling or visualization. With its pipeable sequence of steps and syntax similar to dplyr, the package simplifies a wide range of preprocessing tasks, from data normalization and missing data imputation, to categorical variable encoding and data transformation.\nIt‚Äôs important to remember when using the recipes package, the type of model you‚Äôre fitting can determine the necessary preprocessing steps for your data.\nIn addition to model-driven preprocessing steps, the recipes package also provides functions for feature engineering. This involves representing your data in ways most effective for your particular problem. For instance, you might create interaction terms, polynomial terms, or spline terms to capture non-linear relationships between predictors and the outcome.\nHere are some useful preprocessing steps:\n\nData normalization: The step_normalize() function normalizes your data by centering and scaling the variables. This is useful when working with models that require predictors to be on the same scale, such as k-nearest neighbors or neural networks.\nMissing data imputation: The step_impute_*() functions impute missing data using various methods, like mean imputation, median imputation, or k-nearest neighbors imputation.\nCategorical variable encoding: The step_dummy() function creates dummy variables for categorical predictors. This is handy when working with models that can‚Äôt handle categorical predictors directly, like linear regression or logistic regression.\nData transformation: The step_*() functions transform your data in various ways, such as applying the logarithm, square root, or Box-Cox transformation to a variable. This is useful when working with data that isn‚Äôt normally distributed or when trying to improve the linearity of the relationship between predictors and the outcome.\nFeature engineering: The step_*() functions are also used for feature engineering, such as creating interaction terms, polynomial terms, or spline terms. This is beneficial when trying to capture non-linear relationships between predictors and the outcome.\n\nLink to the package documentation\nThings to think:\nA point of confusion might be whether preprocessing is considered part of data cleaning or data transformation for modeling. It appears that there‚Äôs an overlap between data cleaning and data transformation, and it can sometimes be difficult to distinguish between these stages. It would be helpful to clarify the difference between these concepts and data preprocessing.\nWhen I try to imagine where recipes fits into these models, it‚Äôs not completely clear to me.\n\n\n\nThe data science process. From R for Data Science\n\n\n\n\n\nModeling Process. From Tidymodels book\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-12-03-most/most.html",
    "href": "posts/2023-12-03-most/most.html",
    "title": "Balancing Efficiency and Flexibility: The Challenges of Over-Optimization in the Multiphase Optimization Strategy (MOST)",
    "section": "",
    "text": "The Multiphase Optimization Strategy (MOST) is an approach to intervention development in prevention science. MOST is structured around three phases: the Screening Phase, where intervention components are initially selected or eliminated based on efficacy; the Refining Phase, which is dedicated to ‚Äòcalibrating‚Äô these elements to determine their optimal levels and combinations; and the Confirming Phase, during which the refined intervention is subjected to rigorous evaluation via a conventional randomized controlled trial. Although this strategy is designed to optimize intervention development, it faces a significant challenge: the possibility that excessive optimization could be counterproductive.\nAlthough it may seem counterintuitive, pursuing efficiency can sometimes lead to inferior outcomes. This paradox is named Goodhart‚Äôs Law, which suggests that once a measure becomes a target, it can end up distorting the very outcome it was intended to assess. This happens when individuals or groups start to ‚Äògame the system‚Äô to meet these targets, thereby neglecting the real improvements these measures are designed to track.\nRegarding MOST, the issue does not always stem from intentional misbehavior or manipulation; rather, it arises from an overemphasis on efficiency that can result in an excessive focus on optimization, which may neglect components that benefit achieving the outcomes of the intervention. To illustrate how an excessive focus on optimization can lead to worse outcomes, consider students who are overly prepared to excel at standardized tests, which may cause them to neglect to develop a wider range of skills that are crucial for overall life success. Similarly, overly incentivizing researchers with bonuses can encourage fraudulent activities and undermine the integrity of scientific research.\nTherefore, within MOST, over-selecting components may lead to overfitting in interventions, which is counterproductive given the need for interventions suitable for diverse populations. Moreover, there is a risk that prioritizing intervention components based solely on their individual ‚Äòeffectiveness‚Äô can overlook the principle that the total impact may not always equal the sum of its parts. These two aspects represent fundamental challenges inherent in the MOST methodology.\nTo address this challenge, intervention developers must take into consideration an appropriate balance between the efficiency of components and the variability of their effects under diverse conditions. In addressing this, it is crucial to acknowledge that samples are less diverse than populations, that other factors such as implementation components also contribute to program effectiveness, and, overall, that interventions may not be universally beneficial across all population groups.\nIn conclusion, while MOST is a promising framework for intervention development, it is vital to recognize that it is not perfect. The applicability of the MOST methodology is not universal across all interventions, and the advantages of efficiency must be carefully weighed against the risks of overfitting.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html",
    "href": "posts/2023-01-10-posit-education/Posit.html",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "",
    "text": "I recently watched a presentation by Garrett Grolemund, the Director of Learning at Rstudio, on the POSIT Enterprise Community Meetup on YouTube. I wanted to share with you some of the interesting thoughts about teaching R that he mentioned."
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html#context",
    "href": "posts/2023-01-10-posit-education/Posit.html#context",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "Context",
    "text": "Context\n\nGarret believes that the success of the training ultimately depends on how it‚Äôs delivered.\nTeachers out there really aren‚Äôt that good and online courses are not so great.\nPeople who become experts in R by years of practice. However, they may not have had the opportunity to devote those years to being a good teacher.\nThere is theories in psychology(such as cognitive load theory and multimedia learning theory) that can be applied to make training more successful.\nResearch shows that students only retain about 56% of the content from a lecture\nThe retention rate decreases over time.\nStudies on procedural instruction show similar results, with only 60% of students able to reproduce the procedure immediately after training.\nSix months later, the number drops to 40%, and a year later it drops to 30%\nTraining outcomes are not always great and this is a ‚Äúdirty secret‚Äù of the training industry.\nTrainers are not always trained to be trainers, which is a challenge in the field.\nEducation is not simply about transferring information from an educator to a student, and the student then becoming an expert."
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html#insights",
    "href": "posts/2023-01-10-posit-education/Posit.html#insights",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "Insights",
    "text": "Insights\n\nPractice\n\nThe more the task is practiced, the more the brain will conserve the neural networks and retain the ability to perform the task. This is not likely to occur in a workshop that lasts only half a day or two days.\nTo build a robust neural network, it is important to sleep.\nThe ability to perform a new skill in six months from now dependent on the amount of practice they receive after initial instruction, rather than the instruction itself.\n\n\n‚ÄúWell, the revelation we had at our studio is that data science is a skill, and if you want to learn to do good data science with code or otherwise, you have to practice it and learn it as a skill.‚Äù\n\n\n\nMentors, Mates and Accountability\n\nAs a student, it is possible to practice a skill incorrectly without even being aware of it. To practice effectively and efficiently, it is crucial to receive feedback and guidance to make sure the proper techniques are being utilized.\nIndividuals do not acquire knowledge in a vacuum, they are driven by social influences and the identity they construct by participating in a community that is studying data science.\nThe lacking factor in online courses is motivation, which can be obtained through interaction with a mentor or peers. Talking to them can provide the necessary inspiration.\nHaving a mentor or being part of a group provides accountability, as one is expected to show up with something to demonstrate to them.\n\n\n‚ÄúSo, as you go through the course, not only do you have an expert who has your back who‚Äôs coaching you, you also have fellow travelers who you could discuss things with. You could work through problems together and, you might not even realize it, but you can hold each other accountable and motivated as you go through the process.‚Äù\n\n\n\nRecomendations\n\nHow Learning Happens by Paul Kirschner and Carl. link to amazon. But also consult with your librarian friend\nVisit Posit Academy. You can learn in detail about the POSIT Academy model in minute 24."
  },
  {
    "objectID": "posts/2022-11-27-trustworthy/trustworthy.html",
    "href": "posts/2022-11-27-trustworthy/trustworthy.html",
    "title": "Trustworthy",
    "section": "",
    "text": "Francisco Cardozo, Pablo Montero-Zamora\nConfidence in students‚Äô survey responses is commonly questioned when performing drug use research. For example, one‚Äôs can wonder how do you know students are telling the truth about their alcohol consumption? We recognize this as a legitimate question that can be even more complex when considering two types of students: 1) those who say they have not used alcohol but used it (i.e., deniers), and 2) those who say they have used it but never use it (i.e., braggers). It is crucial to understand how this reporting bias affects the validity of our measurements and research findings. Therefore, we propose the following app to know how the proportions of deniers, braggers, and drug use prevalence can influence confidence levels in self-reported measures collected in adolescents.\n\nTrue prevalence: number of students using a drug divided by the population.\n\nDeniers: students who say they have not used a drug but used it.\nBraggers: students who say they have used a drug but never used it.\n\nGiven the information above, we can estimate the probability of a student‚Äôs drug use behavior if they respond Yes or No to a drug use question.\nTo model this probability, we can use the Bayes theorem:\n\nP(A|B) = Drug use given they say yes in the questionnaire.\nP(B|A) = Say yes given that they have used drugs.\nP(A) = Drug use prevalence.\nP(B) = Say yes in the questionnaire."
  },
  {
    "objectID": "posts/2023-07-22-collect_metrics/collect_metrics.html",
    "href": "posts/2023-07-22-collect_metrics/collect_metrics.html",
    "title": "Computing Sensitivity and Specificity Using collect_metrics",
    "section": "",
    "text": "(in progress)\nIn the last post, we discussed computing sensitivity and specificity using the sensitivity() and specificity() functions from the yardstick package. In this post, we‚Äôll demonstrate how to compute these metrics using the collect_metrics function from the tidymodels package."
  },
  {
    "objectID": "posts/2023-07-22-collect_metrics/collect_metrics.html#summary",
    "href": "posts/2023-07-22-collect_metrics/collect_metrics.html#summary",
    "title": "Computing Sensitivity and Specificity Using collect_metrics",
    "section": "Summary",
    "text": "Summary\nIn this blog post, we discussed the use of the tidymodels package to compute the sensitivity and specificity of a model - two important measures for evaluating model performance. We started with the recreation of a confusion matrix, then used the tbl_cross() function to perform some data wrangling.\nNext, we leveraged fit_resamples() from the tidymodels package to estimate our model multiple times, using cross-validation with 10 repeated folds to create 100 datasets in total.\nWe set up a logistic regression model, fit it to our data, and estimated the model multiple times using the created folds. The saved predictions allowed us to compute sensitivity and specificity for each fold.\nWe defined a custom set of metrics - roc_auc, sensitivity, and specificity - using the metric_set() function from the yardstick package. Finally, we used collect_metrics() to compute the average of these metrics for all the folds, and visualized the distribution of the sensitivity and specificity values.\nFuture posts will delve into calculating confidence intervals and evaluating the variability of these values across resamples."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Welcome to my webpage. My main goal is to develop evidence-based interventions for enhancing well-being. Using scientific methods, I generate evidence to assess these interventions‚Äô effectiveness. I translate this data into actionable advice for schools, families, and policymakers. I‚Äôm committed to making a real impact on people‚Äôs lives through research."
  },
  {
    "objectID": "index.html#web-projects",
    "href": "index.html#web-projects",
    "title": "About me",
    "section": "Web Projects",
    "text": "Web Projects\n\nBusiness That Care Toolkit\nThrustworthy"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "About me",
    "section": "R Packages",
    "text": "R Packages\n\nDocumentData\nChessOlympiad"
  },
  {
    "objectID": "index.html#communities",
    "href": "index.html#communities",
    "title": "About me",
    "section": "Communities",
    "text": "Communities\n\nSoftware Carpentry\nrOpenSci\nSociety for Prevention Research\nInternational Network for Social Network Analysis"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "About me",
    "section": "Contact",
    "text": "Contact\n\nüìß foc9@miami.edu"
  }
]