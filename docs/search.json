[
  {
    "objectID": "posts/2023-05-10-recipes/recipes.html",
    "href": "posts/2023-05-10-recipes/recipes.html",
    "title": "An Introduction to the Recipes Package for Data Preprocessing",
    "section": "",
    "text": "I liked this presentation done by Max Kuhn.\n\n\nHere are some notes:\nThe recipes package provides a framework for preprocessing data prior to modeling or visualization. With its pipeable sequence of steps and syntax similar to dplyr, the package simplifies a wide range of preprocessing tasks, from data normalization and missing data imputation, to categorical variable encoding and data transformation.\nIt’s important to remember when using the recipes package, the type of model you’re fitting can determine the necessary preprocessing steps for your data.\nIn addition to model-driven preprocessing steps, the recipes package also provides functions for feature engineering. This involves representing your data in ways most effective for your particular problem. For instance, you might create interaction terms, polynomial terms, or spline terms to capture non-linear relationships between predictors and the outcome.\nHere are some useful preprocessing steps:\n\nData normalization: The step_normalize() function normalizes your data by centering and scaling the variables. This is useful when working with models that require predictors to be on the same scale, such as k-nearest neighbors or neural networks.\nMissing data imputation: The step_impute_*() functions impute missing data using various methods, like mean imputation, median imputation, or k-nearest neighbors imputation.\nCategorical variable encoding: The step_dummy() function creates dummy variables for categorical predictors. This is handy when working with models that can’t handle categorical predictors directly, like linear regression or logistic regression.\nData transformation: The step_*() functions transform your data in various ways, such as applying the logarithm, square root, or Box-Cox transformation to a variable. This is useful when working with data that isn’t normally distributed or when trying to improve the linearity of the relationship between predictors and the outcome.\nFeature engineering: The step_*() functions are also used for feature engineering, such as creating interaction terms, polynomial terms, or spline terms. This is beneficial when trying to capture non-linear relationships between predictors and the outcome.\n\nLink to the package documentation\nThings to think:\nA point of confusion might be whether preprocessing is considered part of data cleaning or data transformation for modeling. It appears that there’s an overlap between data cleaning and data transformation, and it can sometimes be difficult to distinguish between these stages. It would be helpful to clarify the difference between these concepts and data preprocessing.\nWhen I try to imagine where recipes fits into these models, it’s not completely clear to me.\n\n\n\nThe data science process. From R for Data Science\n\n\n\n\n\nModeling Process. From Tidymodels book\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html",
    "href": "posts/2023-01-10-posit-education/Posit.html",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "",
    "text": "I recently watched a presentation by Garrett Grolemund, the Director of Learning at Rstudio, on the POSIT Enterprise Community Meetup on YouTube. I wanted to share with you some of the interesting thoughts about teaching R that he mentioned."
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html#context",
    "href": "posts/2023-01-10-posit-education/Posit.html#context",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "Context",
    "text": "Context\n\nGarret believes that the success of the training ultimately depends on how it’s delivered.\nTeachers out there really aren’t that good and online courses are not so great.\nPeople who become experts in R by years of practice. However, they may not have had the opportunity to devote those years to being a good teacher.\nThere is theories in psychology(such as cognitive load theory and multimedia learning theory) that can be applied to make training more successful.\nResearch shows that students only retain about 56% of the content from a lecture\nThe retention rate decreases over time.\nStudies on procedural instruction show similar results, with only 60% of students able to reproduce the procedure immediately after training.\nSix months later, the number drops to 40%, and a year later it drops to 30%\nTraining outcomes are not always great and this is a “dirty secret” of the training industry.\nTrainers are not always trained to be trainers, which is a challenge in the field.\nEducation is not simply about transferring information from an educator to a student, and the student then becoming an expert."
  },
  {
    "objectID": "posts/2023-01-10-posit-education/Posit.html#insights",
    "href": "posts/2023-01-10-posit-education/Posit.html#insights",
    "title": "Maximizing the Impact of Your R Teaching: Insights from Garrett Grolemund",
    "section": "Insights",
    "text": "Insights\n\nPractice\n\nThe more the task is practiced, the more the brain will conserve the neural networks and retain the ability to perform the task. This is not likely to occur in a workshop that lasts only half a day or two days.\nTo build a robust neural network, it is important to sleep.\nThe ability to perform a new skill in six months from now dependent on the amount of practice they receive after initial instruction, rather than the instruction itself.\n\n\n“Well, the revelation we had at our studio is that data science is a skill, and if you want to learn to do good data science with code or otherwise, you have to practice it and learn it as a skill.”\n\n\n\nMentors, Mates and Accountability\n\nAs a student, it is possible to practice a skill incorrectly without even being aware of it. To practice effectively and efficiently, it is crucial to receive feedback and guidance to make sure the proper techniques are being utilized.\nIndividuals do not acquire knowledge in a vacuum, they are driven by social influences and the identity they construct by participating in a community that is studying data science.\nThe lacking factor in online courses is motivation, which can be obtained through interaction with a mentor or peers. Talking to them can provide the necessary inspiration.\nHaving a mentor or being part of a group provides accountability, as one is expected to show up with something to demonstrate to them.\n\n\n“So, as you go through the course, not only do you have an expert who has your back who’s coaching you, you also have fellow travelers who you could discuss things with. You could work through problems together and, you might not even realize it, but you can hold each other accountable and motivated as you go through the process.”\n\n\n\nRecomendations\n\nHow Learning Happens by Paul Kirschner and Carl. link to amazon. But also consult with your librarian friend\nVisit Posit Academy. You can learn in detail about the POSIT Academy model in minute 24."
  },
  {
    "objectID": "posts/2022-12-01-ChessOlympiad22/index.html",
    "href": "posts/2022-12-01-ChessOlympiad22/index.html",
    "title": "Chess Olympiad 2022",
    "section": "",
    "text": "library(ChessOlympiad22)\n\n\nDoes ELO predict the wining move?\nThe ELO rating system is a method for calculating the relative skill levels of chess players. The rating is based on the results of games played, and is used to compare the strength of one player to another. The higher a player’s ELO rating, the more skilled they are considered to be.\nIn this document, the ELO difference between two players is used to evaluate the probability of one player winning the match.\n\nLibraries\n\n\nlibrary(rpart.plot)\nlibrary(tidymodels)\n\n\n\nLoad the players data\n\ndata(\"players\")\n\n\n\nDistribution of ELO\nFirst, let’s visualize the ELO distribution of the players.\nIt is important to filter out players with an ELO rating of zero from the analysis, as these players have not yet been rated and do not have a known skill level.\n\nplayers %&gt;% \n  filter(rtg!=0) %&gt;% \n  ggplot(aes(rtg)) + \n  geom_histogram(bins = 50, fill=\"gray20\") +\n  scale_x_continuous(breaks = seq(1000,2900,100))+\n  theme_minimal() +\n  coord_flip() +\n  theme(axis.text = element_text(\"\")) +\n  labs(x=\"Elo Rating System (player strength)\",\n       y=\"Number of players\",\n       title = \"Elo Rating System of players in Chess Olympiad, 2022. \n       Chennai, India \",\n       caption = \"Magnus Carlsen has the highest Elo rating in the tournoment:2864\")\n\n\n\n\nIt may be interesting to plot the ELO ratings of players by federation, as the Chess Olympiad is played by national teams. By examining the ELO ratings of players within each federation, we can get a sense of the overall strength of the teams participating in the event. This analysis could potentially provide insight into the results of the Chess Olympiad and help predict the outcomes of matches.\n\nplayers %&gt;% \n  filter(rtg&gt;2600) %&gt;% \n  ggplot(aes(reorder(fed, rtg),rtg)) + \n  theme_minimal() +\n  geom_boxplot() +\n  coord_flip() +\n  labs(x=\"\", y=\"ELO\")\n\n\n\n\nThe United States have the strongest team in the tournament.\n\n\nLoad results data\n\ndata(\"results\")\n\n\n\nDifferences in ELO by round\nThe Chess Olympiad was a Swiss-style tournament, which means that players are paired against opponents with similar scores in each round. In the first round, the highest-ranked player is matched against the player in the middle of the ELO rating distribution. The second highest-ranked player is then matched against the player just below the player in the middle of the ELO rating distribution, and so on. This implies that ELO differences are maximized in the first round. Then, the pairings for each subsequent round are determined based on the results of the previous rounds, with the goal of ensuring that players face opponents with similar scores.\nA visual representation of the differences in ELO by round are presented in the following graph.\n\n\nresults %&gt;% \n  filter(elo_difference&gt;=-1000, \n         elo_difference&lt;=1000, \n         !is.na(elo_white), !is.na(elo_black), elo_white!=0,elo_black!=0) %&gt;% \n  ggplot(aes(as.numeric(elo_white),elo_difference, \n             fill=factor(result_white), \n             color=factor(result_white))) + \n  geom_point(shape = 21, alpha=0.85,\n             size = 3, stroke = 0.5) +\n  theme_minimal() +\n  scale_fill_manual(values=c(\"Lost\"=\"black\", \"Draw\"=\"gray50\",\"Won\"=\"white\")) +\n  scale_color_manual(values=c(\"Lost\"=\"black\", \"Draw\"=\"black\",\"Won\"=\"black\")) +\n  labs(fill=\"Result\", color=\"Result\", \n       x=\"Player Elo\", \n       y=\"Elo difference\",\n       caption = \"Difference greater than zero indicates stronger player\n       44th Chess Olympiad. Chennai, 2022 Open\") +\n  facet_wrap(~ round)\n\n\n\n\n\n\n\nModel the winning chances for players with the white pieces based on ELO difference.\nNow, we can identify the best split of the differences in ELO ratings to classify the results of chess games. I will use tidymodels to estimate a CART model.\n\nSet the engine\n\ncart_spec &lt;-\n   decision_tree() %&gt;% \n   set_engine(\"rpart\") %&gt;%\n   set_mode(\"classification\")\n\n\n\nCreate a recipie\nI will add two steps in the recipe. One to filter the data set by round, and the other to convert results in a factor variable. I also will limit my analysis to players with more than 1600 in ELO.\n\nresults &lt;- results %&gt;% \n  filter(as.numeric(elo_white)&gt;1600) %&gt;% \n  filter(as.numeric(elo_black)&gt;1600) \n\nrecipe &lt;- recipe(\n  result_white ~ elo_difference + round_number, data = results) %&gt;% \n    step_filter(round_number==round)\n  \n\n\n\nCreate a workflow\n\nwrkfl &lt;- workflow() %&gt;% \n  add_model(cart_spec) %&gt;% \n  add_recipe(recipe)\n\n\n\nEstimate the model\nI will estimate the model for the round 1.\n\n\nround &lt;- 1\n\ncart_fit &lt;- wrkfl %&gt;% \n  fit(data=results) %&gt;% \n  extract_fit_parsnip()\n\n\n\nDraw an tree to understand the results\nFinally, I will create a tree showing the splits\n\n\ncart_fit &lt;- repair_call(cart_fit, data = round)\n\ncart_tree_fit &lt;- cart_fit$fit\n\n\nrpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)\n\n\n\nrpart.rules(cart_tree_fit, cover = TRUE)\n#&gt; Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\n#&gt; To silence this warning:\n#&gt;     Call rpart.rules with roundint=FALSE,\n#&gt;     or rebuild the rpart model with model=TRUE.\n#&gt;   ..y  Dra Los Won                               cover\n#&gt;  Lost [.09 .91 .00] when elo_difference &lt;  -12     51%\n#&gt;   Won [.04 .01 .95] when elo_difference &gt;= -12     49%\n\nAccording to the model, a difference of 12 in the ELO is sufficient to accurately predict the winner in 95% of cases and the loser in 91% of cases.\n\n\n\nModel the last round\nLet’s test the model for the final round, where the matches happened between the strongest opponents.\nI will add a the tree_depth parameter to my model. The depth of the tree refers to the number of levels the tree has.\n\ncart_spec &lt;-\n   decision_tree(tree_depth = 4) %&gt;% \n   set_engine(\"rpart\") %&gt;%\n   set_mode(\"classification\")\n\nwrkfl &lt;- workflow() %&gt;% \n  add_model(cart_spec) %&gt;% \n  add_recipe(recipe)\n\n\nround &lt;- 11\n\ncart_fit &lt;- wrkfl %&gt;% \n  fit(data=results)\n\ncart_fit &lt;- wrkfl %&gt;% \n  fit(data=results) %&gt;% \n  extract_fit_parsnip()\n\ncart_fit &lt;- repair_call(cart_fit, data = round)\n\ncart_tree_fit &lt;- cart_fit$fit\n\nrpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)\n\n\n\nrpart.rules(cart_tree_fit, cover = TRUE)\n#&gt; Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\n#&gt; To silence this warning:\n#&gt;     Call rpart.rules with roundint=FALSE,\n#&gt;     or rebuild the rpart model with model=TRUE.\n#&gt;   ..y  Dra Los Won                                       cover\n#&gt;  Draw [.45 .30 .25] when elo_difference is -176 to  85     51%\n#&gt;  Draw [.55 .09 .36] when elo_difference &gt;=         333      4%\n#&gt;  Draw [.69 .15 .15] when elo_difference is   95 to 115      4%\n#&gt;  Lost [.29 .61 .10] when elo_difference &lt;  -176            20%\n#&gt;   Won [.10 .30 .60] when elo_difference is   85 to  95      3%\n#&gt;   Won [.17 .20 .63] when elo_difference is  115 to 333     18%\n\nThe colors of the leaves in the decision tree suggest that ELO differences continue to be an important factor in the final round of the chess tournament. The model appears to be more accurate at predicting the outcomes of games with large ELO differences. For example, if you have more than 333 points in ELO, the model predicts 63% wining chances for you. However, if the ELO difference is less than 333 (but more than 115), the model predicts 55% of draw.\nWhen the ELO difference is less than 115, the model’s predictions become more interesting. If a player has 176 points less than their opponent, the model is more likely to classify them as a loser (61%). However, if the ELO difference is less than 85 points, the player still has a good chance of winning the game. This could be seen as an indicator of the performance of some players with lower ELO ratings who are having a strong tournament. On the other hand, if the ELO difference is greater than 85 points, most of the chances are for a draw.\nThis was a quick example of how to use ChessOlympiad package.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/thrusworthy.html",
    "href": "posts/thrusworthy.html",
    "title": "Trustworthy",
    "section": "",
    "text": "Francisco Cardozo, Pablo Montero-Zamora\nConfidence in students’ survey responses is commonly questioned when performing drug use research. For example, one’s can wonder how do you know students are telling the truth about their alcohol consumption? We recognize this as a legitimate question that can be even more complex when considering two types of students: 1) those who say they have not used alcohol but used it (i.e., deniers), and 2) those who say they have used it but never use it (i.e., braggers). It is crucial to understand how this reporting bias affects the validity of our measurements and research findings. Therefore, we propose the following app to know how the proportions of deniers, braggers, and drug use prevalence can influence confidence levels in self-reported measures collected in adolescents.\n\nTrue prevalence: number of students using a drug divided by the population.\n\nDeniers: students who say they have not used a drug but used it.\nBraggers: students who say they have used a drug but never used it.\n\nGiven the information above, we can estimate the probability of a student’s drug use behavior if they respond Yes or No to a drug use question.\nTo model this probability, we can use the Bayes theorem:\n\nP(A|B) = Drug use given they say yes in the questionnaire.\nP(B|A) = Say yes given that they have used drugs.\nP(A) = Drug use prevalence.\nP(B) = Say yes in the questionnaire."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francisco Cardozo",
    "section": "",
    "text": "Thank you for visiting my webpage. I hope you find it informative. If you have any further questions or would like to learn more about my work, please do not hesitate to contact me."
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Francisco Cardozo",
    "section": "",
    "text": "Thank you for visiting my webpage. I hope you find it informative. If you have any further questions or would like to learn more about my work, please do not hesitate to contact me."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Francisco Cardozo",
    "section": "About me",
    "text": "About me\nMy primary goal is to evaluate the real-life impact of interventions. I strive to combine my expertise in measurement and research design to obtain rigorous and meaningful results. I am committed to translating complex research findings into useful insights that can inform decision-making. I believe in the power of research to effect change and I am dedicated to making that change tangible and understandable for everyone.\n\n📰 CV\n🧑 Teaching\n📝 Publications\n GitHub"
  },
  {
    "objectID": "index.html#web-projects",
    "href": "index.html#web-projects",
    "title": "Francisco Cardozo",
    "section": "Web Projects",
    "text": "Web Projects\n\nBussiness that Care Toolkit\nThrusworthy"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "Francisco Cardozo",
    "section": "R Packages",
    "text": "R Packages\n\nDocumentData\nChessOlympiad"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\nReading Time\n\n\n\n\n\n\nMay 10, 2023\n\n\nAn Introduction to the Recipes Package for Data Preprocessing\n\n\nTidymodels\n\n\n3 min\n\n\n\n\nFeb 10, 2023\n\n\nMaximizing the Impact of Your R Teaching: Insights from Garrett Grolemund\n\n\nTeaching\n\n\n3 min\n\n\n\n\nDec 1, 2022\n\n\nChess Olympiad 2022\n\n\nR\n\n\n6 min\n\n\n\n\nNov 27, 2022\n\n\nTrustworthy\n\n\nAlcohol\n\n\n2 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]